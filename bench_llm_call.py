#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ§ª BENCH LLM CALL - TEST LATENCE MINIMALE
Test de latence pour identifier si le problÃ¨me vient de l'infrastructure ou du modÃ¨le
"""

import time
import requests
import json
import psutil
from datetime import datetime

def bench_llm_call():
    """Benchmark d'un appel LLM minimal"""
    
    print("ğŸ§ª BENCH LLM CALL - TEST LATENCE MINIMALE")
    print("=" * 60)
    print(f"â° DÃ©but: {datetime.now().strftime('%H:%M:%S')}")
    print()
    
    # Configuration
    URL = "http://localhost:11434/api/generate"
    MODEL = "qwen3:8b"
    
    # Prompt ultra-simple
    PROMPT = '''Generate 5 filmable keywords for: "family playing in park". 
Return JSON: {"keywords":["k1","k2","k3","k4","k5"]}'''
    
    payload = {
        "model": MODEL,
        "prompt": PROMPT,
        "temperature": 0.2,
        "max_tokens": 200,
        "stream": False
    }
    
    print(f"ğŸ¯ ModÃ¨le: {MODEL}")
    print(f"ğŸ“ Prompt: {len(PROMPT)} caractÃ¨res")
    print(f"â±ï¸ Timeout: 120s")
    print()
    
    # Monitoring systÃ¨me avant
    print("ğŸ“Š MONITORING SYSTÃˆME - AVANT")
    print("-" * 40)
    mem_before = psutil.virtual_memory()
    cpu_percent = psutil.cpu_percent(interval=1)
    
    print(f"ğŸ’¾ RAM disponible: {mem_before.available / 1e9:.2f} GB")
    print(f"ğŸ’¾ RAM utilisÃ©e: {mem_before.used / 1e9:.2f} GB")
    print(f"ğŸ”„ CPU: {cpu_percent}%")
    print()
    
    # Test LLM
    print("ğŸš€ TEST LLM EN COURS...")
    print("-" * 40)
    
    try:
        t0 = time.time()
        r = requests.post(URL, json=payload, timeout=120)
        t1 = time.time()
        
        elapsed = t1 - t0
        status = r.status_code
        
        print(f"âœ… Statut: {status}")
        print(f"â±ï¸ Temps total: {elapsed:.2f}s")
        print(f"ğŸ“Š Latence: {elapsed*1000:.0f}ms")
        
        if r.status_code == 200:
            try:
                response_data = r.json()
                response_text = response_data.get('response', '')
                print(f"ğŸ“ RÃ©ponse: {len(response_text)} caractÃ¨res")
                print(f"ğŸ” DÃ©but rÃ©ponse: {response_text[:200]}...")
                
                # Test parsing JSON
                try:
                    json.loads(response_text)
                    print("âœ… JSON valide dÃ©tectÃ©")
                except:
                    print("âš ï¸ JSON invalide dans la rÃ©ponse")
                    
            except Exception as e:
                print(f"âŒ Erreur parsing rÃ©ponse: {e}")
                print(f"ğŸ“ RÃ©ponse brute: {r.text[:200]}...")
        else:
            print(f"âŒ Erreur HTTP: {r.text}")
            
    except requests.exceptions.Timeout:
        print("â±ï¸ TIMEOUT aprÃ¨s 120s")
        elapsed = 120
        status = "TIMEOUT"
    except Exception as e:
        print(f"âŒ Erreur: {str(e)}")
        elapsed = 0
        status = "ERROR"
    
    # Monitoring systÃ¨me aprÃ¨s
    print()
    print("ğŸ“Š MONITORING SYSTÃˆME - APRÃˆS")
    print("-" * 40)
    mem_after = psutil.virtual_memory()
    cpu_percent_after = psutil.cpu_percent(interval=1)
    
    mem_delta = mem_before.available - mem_after.available
    print(f"ğŸ’¾ RAM delta: {mem_delta / 1e6:.1f} MB")
    print(f"ğŸ’¾ RAM disponible: {mem_after.available / 1e9:.2f} GB")
    print(f"ğŸ”„ CPU: {cpu_percent_after}%")
    
    # Analyse des rÃ©sultats
    print()
    print("ğŸ” ANALYSE DES RÃ‰SULTATS")
    print("=" * 60)
    
    if elapsed < 5:
        print("âœ… INFRA OK - Latence normale (<5s)")
        print("ğŸ¯ ProblÃ¨me probable: Prompt trop complexe")
    elif elapsed < 10:
        print("âš ï¸ INFRA LENTE - Latence Ã©levÃ©e (5-10s)")
        print("ğŸ¯ ProblÃ¨me probable: ModÃ¨le ou configuration Ollama")
    elif elapsed < 30:
        print("âŒ INFRA PROBLÃ‰MATIQUE - Latence trÃ¨s Ã©levÃ©e (10-30s)")
        print("ğŸ¯ ProblÃ¨me probable: ModÃ¨le quantisÃ© mal ou RAM insuffisante")
    else:
        print("ğŸš¨ INFRA CRITIQUE - Latence excessive (>30s)")
        print("ğŸ¯ ProblÃ¨me probable: Swapping, modÃ¨le corrompu, ou configuration critique")
    
    print()
    print("ğŸ“‹ RECOMMANDATIONS")
    print("-" * 40)
    
    if elapsed < 5:
        print("1. âœ… Infra OK - Tester prompt complexe maintenant")
        print("2. ğŸ¯ Simplifier le prompt de gÃ©nÃ©ration de mots-clÃ©s")
        print("3. ğŸ”§ ImplÃ©menter fallback heuristique")
    elif elapsed < 30:
        print("1. âš ï¸ VÃ©rifier configuration Ollama")
        print("2. ğŸ” Tester modÃ¨le quantisÃ© (qwen3:4b)")
        print("3. ğŸ’¾ VÃ©rifier utilisation RAM/swap")
    else:
        print("1. ğŸš¨ VÃ©rifier immÃ©diatement l'Ã©tat du systÃ¨me")
        print("2. ğŸ”„ RedÃ©marrer Ollama")
        print("3. ğŸ“¦ RÃ©installer le modÃ¨le")
    
    return elapsed, status

if __name__ == "__main__":
    elapsed, status = bench_llm_call()
    
    print()
    print("=" * 60)
    print(f"ğŸ BENCH TERMINÃ‰ - Temps: {elapsed:.2f}s, Statut: {status}")
    
    input("\nAppuyez sur EntrÃ©e pour continuer...") 
#!/usr/bin/env python3
"""
Pipeline de S√©lection B-roll G√©n√©rique
Module r√©utilisable pour n'importe quel clip vid√©o/domaine
"""

import json
import logging
from pathlib import Path
from typing import List, Dict, Set, Optional, Tuple, Any
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
import math
from collections import defaultdict
import re
import requests

# üöÄ NOUVEAU: Cache global pour √©viter le rechargement des mod√®les
_BROLL_MODEL_CACHE = {}

def get_cached_sentence_transformer(model_name: str):
    """R√©cup√®re un mod√®le SentenceTransformer depuis le cache ou le charge"""
    if model_name not in _BROLL_MODEL_CACHE:
        print(f"    üîÑ Chargement initial du mod√®le B-roll: {model_name}")
        try:
            from sentence_transformers import SentenceTransformer
            _BROLL_MODEL_CACHE[model_name] = SentenceTransformer(model_name)
            print(f"    ‚úÖ Mod√®le B-roll {model_name} charg√© et mis en cache")
        except Exception as e:
            print(f"    ‚ùå Erreur chargement mod√®le B-roll {model_name}: {e}")
            return None
    else:
        print(f"    ‚ôªÔ∏è Mod√®le B-roll {model_name} r√©cup√©r√© du cache")
    
    return _BROLL_MODEL_CACHE[model_name]

# Fallback imports pour √©viter les erreurs
try:
    from sentence_transformers import SentenceTransformer
    SENTENCE_TRANSFORMERS_AVAILABLE = True
except ImportError:
    SENTENCE_TRANSFORMERS_AVAILABLE = False
    print("‚ö†Ô∏è SentenceTransformers non disponible - fallback vers scoring lexical")

try:
    import nltk
    from nltk.corpus import wordnet
    from nltk.stem import WordNetLemmatizer
    NLTK_AVAILABLE = True
except ImportError:
    NLTK_AVAILABLE = False
    print("‚ö†Ô∏è NLTK non disponible - fallback vers normalisation basique")

logger = logging.getLogger(__name__)

@dataclass
class Asset:
    """Repr√©sentation d'un asset B-roll"""
    id: str
    file_path: str
    tags: List[str]
    title: str
    description: str
    source: str
    fetched_at: datetime
    duration: float
    resolution: str
    precomputed_embedding: Optional[List[float]] = None
    
    def to_dict(self) -> Dict[str, Any]:
        """Convertit en dictionnaire pour JSON"""
        data = asdict(self)
        data['fetched_at'] = self.fetched_at.isoformat()
        return data

@dataclass
class ScoringFeatures:
    """Features de scoring pour un asset"""
    token_overlap: float = 0.0
    embedding_similarity: float = 0.0
    domain_match: float = 0.0
    freshness: float = 0.0
    quality_score: float = 0.0
    diversity_penalty: float = 0.0
    
    def to_dict(self) -> Dict[str, float]:
        return asdict(self)

@dataclass
class BrollCandidate:
    """Candidat B-roll avec scoring"""
    asset: Asset
    score: float
    features: ScoringFeatures
    excluded_reason: Optional[str] = None
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'asset_id': self.asset.id,
            'file_path': self.asset.file_path,
            'score': self.score,
            'features': self.features.to_dict(),
            'excluded_reason': self.excluded_reason,
            'asset': self.asset.to_dict()  # Ajouter asset complet pour √©viter erreur s√©rialisation
        }

class BrollSelector:
    """S√©lecteur B-roll g√©n√©rique avec scoring mixte et fallback hi√©rarchique"""
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """Initialise le s√©lecteur avec configuration"""
        self.config = self._get_default_config()
        if config:
            self.config.update(config)

        # Initialiser le logger avant toute utilisation
        self.logger = logging.getLogger(__name__)

        # üöÄ NOUVEAU: Mode DIRECT - utilise directement les r√©sultats API
        self.direct_api_mode = self.config.get('direct_api_mode', False)  # HYBRIDE INTELLIGENT PAR D√âFAUT

        if self.direct_api_mode:
            self.logger.info("üéØ MODE DIRECT API ACTIV√â - T√©l√©chargement automatique depuis Pexels/Pixabay")
            self.logger.info("    üì° FETCH par API = T√©l√©chargement automatique de vid√©os B-roll depuis Internet")
            self.logger.info("    Sources: Pexels (videos), Pixabay (videos)")
            self.logger.info("    üîÑ Process: Mots-cl√©s -> Recherche API -> T√©l√©chargement -> Insertion dans vid√©o")
        else:
            self.logger.info("üîç MODE S√âLECTION ACTIV√â - Re-scoring des r√©sultats API")
        
        # Initialiser les mod√®les si disponibles
        self.embedding_model = None
        self.lemmatizer = None
        
        if SENTENCE_TRANSFORMERS_AVAILABLE:
            try:
                # üöÄ OPTIMISATION: Utiliser le cache pour √©viter le rechargement
                self.embedding_model = get_cached_sentence_transformer('all-MiniLM-L6-v2')
                if self.embedding_model is not None:
                    self.logger.info("‚úÖ Mod√®le d'embeddings charg√©")
                else:
                    self.logger.warning("‚ö†Ô∏è √âchec chargement mod√®le d'embeddings")
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è Erreur chargement embeddings: {e}")
        
        if NLTK_AVAILABLE:
            try:
                self.lemmatizer = WordNetLemmatizer()
                # T√©l√©charger WordNet si n√©cessaire
                try:
                    wordnet.ensure_loaded()
                except:
                    pass
                self.logger.info("‚úÖ Lemmatiseur NLTK charg√©")
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è Erreur chargement NLTK: {e}")
    
    def _get_default_config(self) -> Dict[str, Any]:
        """Configuration par d√©faut pour le s√©lecteur B-roll"""
        return {
            # Performance et seuils
            'thresholds': {
                'min_score': 0.3,
                'min_delay_seconds': 1.5,  # R√©duit de 2.0s √† 1.5s
                'quality_threshold': 0.5
            },
            'desired_broll_count': 5,  # R√©duit de 7 √† 5
            'max_candidates': 50,
            
            # Poids pour le scoring mixte
            'scoring_weights': {
                'embedding': 0.4,
                'token': 0.2,
                'domain': 0.15,
                'freshness': 0.1,
                'quality': 0.1,
                'diversity': 0.05
            },
            
            # üöÄ NOUVEAU: Configuration du mode direct
            'direct_api_mode': False,  # Utiliser directement les r√©sultats API
            'direct_api_limit': 5,    # Nombre de B-rolls √† prendre directement
            'smart_crop_mode': True,  # Recadrage intelligent pour 9:16
            
            # Fallback et diversit√©
            'enable_fallback': True,
            'fallback_tiers': ['high_quality', 'medium_quality', 'any_available'],
            'diversity_penalty_factor': 0.1
        }
    
    def normalize_keywords(self, keywords: List[str]) -> Set[str]:
        """Normalise et nettoie les mots-cl√©s"""
        if not keywords:
            return set()
        
        normalized = set()
        stopwords = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'}
        
        for keyword in keywords:
            if not keyword or not isinstance(keyword, str):
                continue
            
            # Nettoyage basique
            clean = keyword.lower().strip()
            clean = re.sub(r'[^\w\s]', '', clean)
            
            # Supprimer les stopwords et mots trop courts
            if clean and len(clean) > 2 and clean not in stopwords:
                # Lemmatisation si disponible
                if self.lemmatizer:
                    try:
                        clean = self.lemmatizer.lemmatize(clean)
                    except:
                        pass
                
                normalized.add(clean)
        
        self.logger.info(f"üîë Mots-cl√©s normalis√©s: {len(normalized)} -> {list(normalized)[:10]}")
        return normalized
    
    def expand_keywords(self, keywords: List[str], domain: Optional[str] = None, max_expansions: int = 15) -> List[str]:
        """√âtend les mots-cl√©s avec synonymes et termes proches"""
        if not keywords:
            return []
        
        expanded = set(keywords)
        
        # Expansion via WordNet si disponible
        if NLTK_AVAILABLE and self.lemmatizer:
            for keyword in keywords[:5]:  # Limiter pour √©viter l'explosion
                try:
                    # Chercher des synonymes
                    synsets = wordnet.synsets(keyword)
                    for synset in synsets[:3]:  # Top 3 synsets
                        for lemma in synset.lemmas()[:2]:  # Top 2 lemmas
                            expanded.add(lemma.name().lower())
                except Exception as e:
                    self.logger.debug(f"‚ö†Ô∏è Erreur expansion WordNet pour '{keyword}': {e}")
        
        # Expansion par domaine si sp√©cifi√©
        if domain:
            domain_expansions = self._get_domain_expansions(domain)
            expanded.update(domain_expansions)
        
        # Limiter le nombre d'expansions
        result = list(expanded)[:max_expansions]
        self.logger.info(f"üîç Mots-cl√©s √©tendus: {len(keywords)} -> {len(result)}")
        
        return result
    
    def _get_domain_expansions(self, domain: str) -> List[str]:
        """Retourne des expansions sp√©cifiques au domaine"""
        domain_keywords = {
            'health': ['medical', 'wellness', 'fitness', 'care', 'treatment', 'doctor', 'hospital', 'medicine'],
            'finance': ['money', 'business', 'investment', 'banking', 'economy', 'trading', 'wealth'],
            'education': ['learning', 'school', 'university', 'study', 'knowledge', 'teaching', 'student'],
            'technology': ['digital', 'innovation', 'software', 'computer', 'ai', 'data', 'tech'],
            'food': ['cooking', 'restaurant', 'cuisine', 'ingredients', 'meal', 'chef', 'kitchen']
        }
        
        return domain_keywords.get(domain.lower(), [])
    
    def fetch_assets(self, keywords: List[str], limit: int = 200) -> List[Asset]:
        """R√©cup√®re les assets disponibles depuis les dossiers B-roll r√©els"""
        assets = []
        
        print(f"üîç DEBUG: fetch_assets appel√© avec {len(keywords)} mots-cl√©s")
        print(f"üîç DEBUG: Mots-cl√©s: {keywords[:5]}")
        
        # Chercher dans les dossiers B-roll r√©els
        # D'abord dans AI-B-roll/broll_library
        broll_dirs = list(Path("AI-B-roll/broll_library").glob("clip_reframed_*"))
        # üöÄ CORRECTION: PRIORISER le dossier le plus r√©cent
        broll_dirs = sorted(broll_dirs, key=lambda p: p.name, reverse=True)  # Plus r√©cent en premier
        print(f"üîç DEBUG: Dossiers clip_reframed_* trouv√©s: {len(broll_dirs)}")
        if broll_dirs:
            print(f"üéØ DEBUG: Dossier prioritaire (plus r√©cent): {broll_dirs[0].name}")
        
        # Si pas trouv√©, chercher dans output/clips
        if not broll_dirs:
            print("üîç DEBUG: Aucun dossier clip_reframed_* trouv√©, recherche dans output/clips")
            output_dirs = list(Path("output/clips").glob("*"))
            print(f"üîç DEBUG: Dossiers output/clips trouv√©s: {len(output_dirs)}")
            for output_dir in output_dirs:
                if output_dir.is_dir():
                    print(f"üîç DEBUG: Exploration de {output_dir}")
                    # Chercher des fichiers B-roll dans les sous-dossiers
                    for subdir in output_dir.iterdir():
                        if subdir.is_dir() and "broll" in subdir.name.lower():
                            print(f"üîç DEBUG: Dossier B-roll trouv√©: {subdir}")
                            broll_dirs.append(subdir)
        
        # Si toujours pas trouv√©, chercher dans le dossier racine
        if not broll_dirs:
            print("üîç DEBUG: Aucun dossier B-roll trouv√©, recherche dans le dossier racine")
            # Chercher seulement des dossiers, pas des fichiers
            root_dirs = [d for d in Path(".").glob("*broll*") if d.is_dir()]
            print(f"üîç DEBUG: Dossiers *broll* trouv√©s: {len(root_dirs)}")
            broll_dirs.extend(root_dirs)
        
        # Si toujours pas trouv√©, chercher dans test_clip
        if not broll_dirs:
            print("üîç DEBUG: Aucun dossier B-roll trouv√©, recherche dans test_clip")
            test_dir = Path("AI-B-roll/broll_library/test_clip")
            if test_dir.exists():
                print(f"üîç DEBUG: Dossier test_clip trouv√©: {test_dir}")
                broll_dirs.append(test_dir)
        
        # üöÄ NOUVEAU: Si pas de dossiers B-roll, T√âL√âCHARGER depuis les APIs
        if not broll_dirs:
            print("üîç DEBUG: Aucun dossier B-roll trouv√©, T√âL√âCHARGEMENT depuis APIs...")
            return self._fetch_from_apis(keywords, limit)
        
        print(f"üîç DEBUG: Total dossiers B-roll trouv√©s: {len(broll_dirs)}")
        
        # üöÄ CORRECTION: Utiliser SEULEMENT le dossier le plus r√©cent s'il a des assets
        prioritized_dirs = []
        if broll_dirs:
            latest_dir = broll_dirs[0]  # Le plus r√©cent gr√¢ce au tri
            latest_fetched = latest_dir / "fetched"
            if latest_fetched.exists() and len(list(latest_fetched.rglob("*.mp4"))) > 0:
                print(f"üéØ DEBUG: Utilisation exclusive du dossier r√©cent: {latest_dir.name}")
                prioritized_dirs = [latest_dir]  # SEULEMENT le plus r√©cent
            else:
                print(f"‚ö†Ô∏è DEBUG: Dossier r√©cent vide, utilisation de tous les dossiers")
                prioritized_dirs = broll_dirs  # Fallback vers tous si r√©cent vide
        else:
            prioritized_dirs = broll_dirs
            
        for broll_dir in prioritized_dirs:
            if not broll_dir.exists():
                print(f"üîç DEBUG: Dossier {broll_dir} n'existe pas")
                continue
                
            print(f"üîç DEBUG: Exploration du dossier: {broll_dir}")
            
            # Chercher des fichiers vid√©o dans ce dossier
            video_files = []
            
            # Chercher dans fetched/ si existe
            fetched_dir = broll_dir / "fetched"
            if fetched_dir.exists():
                print(f"üîç DEBUG: Dossier fetched trouv√©: {fetched_dir}")
                for provider_dir in fetched_dir.iterdir():
                    if provider_dir.is_dir():
                        print(f"üîç DEBUG: Provider trouv√©: {provider_dir}")
                        for theme_dir in provider_dir.iterdir():
                            if theme_dir.is_dir():
                                print(f"üîç DEBUG: Theme trouv√©: {theme_dir}")
                                for asset_file in theme_dir.glob("*.mp4"):
                                    print(f"üîç DEBUG: Fichier vid√©o trouv√©: {asset_file}")
                                    video_files.append(asset_file)
            else:
                print(f"üîç DEBUG: Dossier fetched non trouv√© dans {broll_dir}")
            
            # Si pas de fetched/, chercher directement
            if not video_files:
                print(f"üîç DEBUG: Aucun fichier dans fetched/, recherche directe")
                for asset_file in broll_dir.rglob("*.mp4"):
                    print(f"üîç DEBUG: Fichier vid√©o trouv√© (recherche directe): {asset_file}")
                    video_files.append(asset_file)
            
            print(f"üîç DEBUG: Fichiers vid√©o trouv√©s dans {broll_dir.name}: {len(video_files)}")
            
            for asset_file in video_files[:10]:  # Limiter par dossier
                try:
                    print(f"üîç DEBUG: Cr√©ation asset pour {asset_file}")
                    # Cr√©er un asset
                    asset = Asset(
                        id=f"asset_{len(assets)}",
                        file_path=str(asset_file),
                        tags=self._extract_tags_from_path(asset_file),
                        title=asset_file.stem,
                        description=f"Asset from {broll_dir.name}",
                        source="local",
                        fetched_at=datetime.now() - timedelta(days=len(assets) % 30),
                        duration=2.0 + (len(assets) % 3),
                        resolution="1920x1080"
                    )
                    assets.append(asset)
                    print(f"üîç DEBUG: Asset cr√©√© avec succ√®s: {asset.id}")
                    
                    if len(assets) >= limit:
                        break
                        
                except Exception as e:
                    print(f"üîç DEBUG: Erreur cr√©ation asset {asset_file}: {e}")
        
        print(f"üîç DEBUG: Total assets r√©cup√©r√©s: {len(assets)}")
        
        # üöÄ AM√âLIORATION: T√©l√©charger aussi si mots-cl√©s sp√©cifiques et peu d'assets
        specialized_keywords = [kw for kw in keywords if '_' in kw or any(term in kw.lower() for term in ['brain', 'neural', 'adrenaline', 'chemical', 'medical'])]
        
        # üöÄ CORRECTION CRITIQUE: V√©rifier si le dossier ACTUEL est vide
        # Compter seulement les assets du dossier le plus r√©cent
        latest_clip_assets = 0
        if broll_dirs:
            latest_clip_dir = max(broll_dirs, key=lambda p: p.name)
            latest_fetched = latest_clip_dir / "fetched"
            if latest_fetched.exists():
                latest_clip_assets = len(list(latest_fetched.rglob("*.mp4")))
                print(f"üîç DEBUG: Assets dans dossier actuel {latest_clip_dir.name}: {latest_clip_assets}")
        
        should_download = (
            len(assets) == 0 or  # Aucun asset global
            latest_clip_assets == 0 or  # Dossier actuel vide
            (len(specialized_keywords) > 0 and latest_clip_assets < 5)  # Peu d'assets sp√©cifiques au dossier actuel
        )
        
        if should_download:
            if len(assets) == 0:
                print("üîç DEBUG: Aucun asset trouv√© dans le cache local")
            else:
                print(f"üîç DEBUG: {len(specialized_keywords)} mots-cl√©s sp√©cialis√©s d√©tect√©s, t√©l√©chargement compl√©mentaire")
                print(f"    üéØ Mots-cl√©s sp√©cialis√©s: {specialized_keywords[:3]}")
            
            print("üì• Lancement t√©l√©chargement depuis APIs...")
            api_assets = self._fetch_from_apis(keywords, limit)
            assets.extend(api_assets)
            print(f"üì• Total apr√®s t√©l√©chargement: {len(assets)} assets")
        
        self.logger.info(f"üì• Assets r√©cup√©r√©s: {len(assets)}")
        return assets
    
    def _fetch_from_apis(self, keywords: List[str], limit: int = 200) -> List[Asset]:
        """T√©l√©charge des B-rolls depuis les APIs externes"""
        try:
            import requests
            import os
            from datetime import datetime
            
            assets = []
            
            # Configuration des cl√©s API
            pexels_key = os.getenv('PEXELS_API_KEY') or 'pwhBa9K7fa9IQJCmfCy0NfHFWy8QyqoCkGnWLK3NC2SbDTtUeuhxpDoD'
            pixabay_key = os.getenv('PIXABAY_API_KEY') or '51724939-ee09a81ccfce0f5623df46a69'
            
            if not pexels_key and not pixabay_key:
                print("‚ùå Pas de cl√© API (Pexels/Pixabay) pour le t√©l√©chargement")
                return self._create_fallback_assets(keywords)
            
            # üöÄ CORRECTION: T√©l√©charger dans le dossier clip le plus r√©cent
            # Trouver le dossier clip le plus r√©cent (celui qui vient d'√™tre cr√©√©)
            broll_dirs = list(Path("AI-B-roll/broll_library").glob("clip_reframed_*"))
            if broll_dirs:
                # Prendre le plus r√©cent (tri par nom qui contient timestamp)
                latest_clip_dir = max(broll_dirs, key=lambda p: p.name)
                fetch_dir = latest_clip_dir / "fetched"
                print(f"üéØ T√©l√©chargement dans: {latest_clip_dir.name}/fetched/")
            else:
                # Fallback vers dossier g√©n√©rique si pas de clip trouv√©
                fetch_dir = Path("AI-B-roll/broll_library/fetched")
                print("‚ö†Ô∏è Aucun dossier clip trouv√©, utilisation dossier g√©n√©rique")
            
            fetch_dir.mkdir(parents=True, exist_ok=True)
            
            print(f"üì• T√©l√©chargement B-rolls depuis APIs pour {len(keywords)} mots-cl√©s...")
            
            # Essayer tous les providers (Pexels/Pixabay uniquement)
            providers = []
            if pexels_key:
                providers.append(('pexels', pexels_key))
            if pixabay_key:
                providers.append(('pixabay', pixabay_key))
            if not providers:
                print('WARNING: no API provider (Pexels/Pixabay)')

            # üöÄ NOUVEAU: Simplifier les mots-cl√©s pour APIs externes
            simplified_keywords = []
            for keyword in keywords[:5]:  # Plus de mots-cl√©s pour augmenter les chances
                # Simplifier les mots-cl√©s LLM pour les APIs
                simplified = self._simplify_keyword_for_api(keyword)
                if simplified and simplified not in simplified_keywords:
                    simplified_keywords.append(simplified)
            
            # Limiter et afficher
            simplified_keywords = simplified_keywords[:3]
            print(f"üîç Mots-cl√©s simplifi√©s pour APIs: {simplified_keywords}")
            
            # üöÄ OPTIMISATION VALID√âE: T√©l√©chargement parall√®le des APIs
            from concurrent.futures import ThreadPoolExecutor, as_completed
            import threading
            
            def fetch_from_provider(keyword, provider, api_key, fetch_dir):
                """Download an asset for the given provider (Pexels/Pixabay)."""
                try:
                    if provider == 'pexels':
                        return self._fetch_from_pexels(keyword, api_key, fetch_dir / 'pexels')
                    if provider == 'pixabay':
                        return self._fetch_from_pixabay(keyword, api_key, fetch_dir / 'pixabay')
                    return []
                except Exception as e:
                    print(f"WARNING: provider {provider} failed for '{keyword}': {e}")
                    return []

            fetch_tasks = []
            for keyword in simplified_keywords:
                for provider, api_key in providers:
                    if len(assets) >= limit:
                        break
                    fetch_tasks.append((keyword, provider, api_key, fetch_dir))
            
            print(f"üöÄ T√©l√©chargement parall√®le: {len(fetch_tasks)} t√¢ches sur {len(providers)} APIs")
            
            # Ex√©cution parall√®le avec maximum 4 threads (optimisation r√©seau)
            with ThreadPoolExecutor(max_workers=min(4, len(fetch_tasks))) as executor:
                # Soumettre toutes les t√¢ches
                future_to_task = {
                    executor.submit(fetch_from_provider, keyword, provider, api_key, fetch_dir): (keyword, provider)
                    for keyword, provider, api_key, fetch_dir in fetch_tasks
                }
                
                # R√©cup√©rer les r√©sultats au fur et √† mesure
                for future in as_completed(future_to_task):
                    keyword, provider = future_to_task[future]
                    try:
                        provider_assets = future.result(timeout=30)  # Timeout 30s par provider
                        if provider_assets:
                            assets.extend(provider_assets)
                            print(f"   ‚úÖ {provider}: {len(provider_assets)} assets pour '{keyword}'")
                        
                        # Arr√™ter si limite atteinte
                        if len(assets) >= limit:
                            print(f"   üéØ Limite atteinte: {len(assets)} assets")
                            break
                            
                    except Exception as e:
                        print(f"   ‚ùå {provider} √©chou√© pour '{keyword}': {e}")
            
            print(f"‚ö° T√©l√©chargement parall√®le termin√©: {len(assets)} assets obtenus")
                

            
            print(f"‚úÖ {len(assets)} B-rolls t√©l√©charg√©s depuis les APIs")
            
            # Si pas d'assets t√©l√©charg√©s, fallback
            if not assets:
                print("üîÑ Aucun t√©l√©chargement r√©ussi, utilisation fallback")
                return self._create_fallback_assets(keywords)
            
            return assets
            
        except Exception as e:
            print(f"‚ùå Erreur t√©l√©chargement APIs: {e}")
            return self._create_fallback_assets(keywords)
    
    def _simplify_keyword_for_api(self, keyword: str) -> str:
        """Simplifie un mot-cl√© LLM pour les APIs externes en pr√©servant la sp√©cificit√©"""
        # Convertir underscore en espace pour les APIs
        simplified = keyword.replace('_', ' ')
        
        # üöÄ AM√âLIORATION: Pr√©server la sp√©cificit√© des mots-cl√©s LLM
        concept_mapping = {
            # üß† Cerveau & Neuroscience - PR√âSERVER LA SP√âCIFICIT√â
            'brain neural networks': 'brain neurons neural network',
            'brain adrenaline buffer': 'brain neurotransmitter adrenaline',
            'brain neural connections': 'brain synapses neural',
            'brain internal reward': 'brain dopamine reward system',
            'neural networks': 'brain neural network',
            
            # üë§ Actions humaines - GARDER LE CONTEXTE
            'person thinking concept': 'person thinking meditation',
            'person celebrating achievement': 'person celebrating success',
            'person achieving goal': 'person achievement success',
            'person celebrating success': 'person celebration victory',
            'person reflecting concept': 'person contemplating thinking',
            'person achieving objective': 'person goal achievement',
            'person celebrating win': 'person victory celebration',
            
            # üíº Business - ENRICHIR AU LIEU DE SIMPLIFIER
            'business handshake deal': 'business handshake partnership',
            'entrepreneur presenting idea': 'entrepreneur presentation business',
            'team brainstorming session': 'team meeting brainstorming',
            'data visualization reward': 'data visualization charts',
            
            # Anciens mappings (maintenir compatibilit√©)
            'process direction visual': 'business process',
            'internal motivation concept': 'motivation psychology',
            'brain focus concept': 'brain thinking',
            'energy drive concept': 'energy motivation',
            'path outcome visual': 'path direction success',
            'adrenaline buffer concept': 'adrenaline stress hormone',
            'reward mechanism visual': 'reward success achievement',
            'goal achievement visual': 'goal achievement success',
            'cognitive control visual': 'brain thinking cognitive'
        }
        
        # Chercher correspondance exacte
        simplified_lower = simplified.lower()
        for complex_term, enhanced_term in concept_mapping.items():
            if complex_term in simplified_lower:
                print(f"    üéØ Mapping sp√©cialis√©: {keyword} ‚Üí {enhanced_term}")
                return enhanced_term
        
        # üöÄ AM√âLIORATION: Traitement intelligent des mots-cl√©s structur√©s
        words = simplified.lower().split()
        if len(words) >= 2:
            # Identifier et enrichir les domaines sp√©cialis√©s
            domain_enrichment = {
                'brain': ['neuroscience', 'cognitive'],
                'person': ['human', 'individual'],
                'business': ['professional', 'corporate'],
                'data': ['analytics', 'visualization']
            }
            
            # Construire une requ√™te enrichie
            enhanced_terms = []
            for word in words:
                if word in domain_enrichment:
                    enhanced_terms.append(word)
                    enhanced_terms.extend(domain_enrichment[word][:1])  # Ajouter 1 terme de domaine
                elif len(word) > 3 and word not in ['concept', 'visual', 'mechanism']:
                    enhanced_terms.append(word)
            
            if enhanced_terms:
                result = ' '.join(enhanced_terms[:4])  # Max 4 mots pour l'API
                print(f"    üß† Enrichissement intelligent: {keyword} ‚Üí {result}")
                return result
        
        # Fallback am√©lior√© : garder la sp√©cificit√©
        if len(simplified) > 2:
            return simplified
        else:
            return 'professional business'
    
    def _fetch_from_pexels(self, keyword: str, api_key: str, fetch_dir: Path) -> List[Asset]:
        """T√©l√©charge des B-rolls depuis Pexels"""
        assets = []
        try:
            print(f"üîç Recherche Pexels: '{keyword}'")
            
            # Appel API Pexels
            headers = {"Authorization": api_key}
            response = requests.get(
                f"https://api.pexels.com/videos/search?query={keyword}&per_page=2",
                headers=headers,
                timeout=15
            )
            
            if response.status_code != 200:
                print(f"‚ö†Ô∏è Erreur API Pexels pour '{keyword}': {response.status_code}")
                return assets
            
            data = response.json()
            videos = data.get('videos', [])
            
            for i, video in enumerate(videos):
                try:
                    video_files = video.get('video_files', [])
                    if not video_files:
                        continue
                    
                    # Choisir la qualit√© medium ou HD
                    suitable_files = [vf for vf in video_files if vf.get('quality') in ['hd', 'medium']]
                    if not suitable_files:
                        suitable_files = video_files[:1]
                    
                    video_file = suitable_files[0]
                    download_url = video_file['link']
                    
                    # Nom du fichier
                    filename = f"{keyword}_{video['id']}_{i}.mp4"
                    file_path = fetch_dir / filename
                    
                    # Cr√©er le dossier
                    fetch_dir.mkdir(parents=True, exist_ok=True)
                    
                    print(f"üì• T√©l√©chargement Pexels: {filename}")
                    
                    # T√©l√©charger
                    download_response = requests.get(download_url, stream=True, timeout=30)
                    download_response.raise_for_status()
                    
                    with open(file_path, 'wb') as f:
                        downloaded_size = 0
                        max_size = 10 * 1024 * 1024  # 10MB max par fichier
                        
                        for chunk in download_response.iter_content(chunk_size=8192):
                            if chunk:
                                f.write(chunk)
                                downloaded_size += len(chunk)
                                if downloaded_size > max_size:
                                    break
                    
                    # üöÄ VALIDATION: V√©rifier l'int√©grit√© du fichier t√©l√©charg√©
                    if file_path.exists() and file_path.stat().st_size > 1000:
                        # Validation basique de l'int√©grit√© vid√©o
                        is_valid = True
                        try:
                            if filename.endswith(('.mp4', '.mov', '.avi', '.mkv')):
                                # Test d'ouverture rapide avec MoviePy
                                from moviepy import VideoFileClip
                                with VideoFileClip(str(file_path)) as test_clip:
                                    # V√©rifier que la dur√©e est coh√©rente
                                    if test_clip.duration <= 0 or test_clip.duration > 300:  # Max 5 minutes
                                        is_valid = False
                        except Exception:
                            is_valid = False
                            
                        if is_valid:
                            asset = Asset(
                                id=f"pexels_{video['id']}",
                                file_path=str(file_path),
                                tags=[keyword, 'pexels', 'video'] + keyword.split('_'),
                                title=f"Pexels {keyword} {video['id']}",
                                description=f"B-roll t√©l√©charg√© depuis Pexels pour {keyword}",
                                source="pexels_api",
                                fetched_at=datetime.now(),
                                duration=float(video.get('duration', 3.0)),
                                resolution=f"{video_file.get('width', 1920)}x{video_file.get('height', 1080)}"
                            )
                            assets.append(asset)
                            print(f"‚úÖ T√©l√©charg√© Pexels: {filename} ({file_path.stat().st_size / 1024 / 1024:.1f}MB)")
                        else:
                            print(f"‚ö†Ô∏è Fichier Pexels corrompu ignor√©: {filename}")
                            try:
                                file_path.unlink()  # Supprimer le fichier corrompu
                            except:
                                pass
                    
                    if len(assets) >= 2:  # Limiter √† 2 par mot-cl√© par provider
                        break
                        
                except Exception as e:
                    print(f"‚ö†Ô∏è Erreur t√©l√©chargement Pexels {i}: {e}")
                    continue
                    
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur recherche Pexels '{keyword}': {e}")
        
        return assets
    
    def _fetch_from_pixabay(self, keyword: str, api_key: str, fetch_dir: Path) -> List[Asset]:
        """T√©l√©charge des B-rolls depuis Pixabay avec format officiel"""
        assets = []
        try:
            print(f"üîç Recherche Pixabay: '{keyword}'")
            
            # URL officielle exacte de la documentation Pixabay
            # Pixabay accepte per_page entre 3-200, pas 2
            url = f"https://pixabay.com/api/videos/?key={api_key}&q={keyword}&per_page=3"
            
            response = requests.get(url, timeout=15)
            
            if response.status_code != 200:
                print(f"‚ö†Ô∏è Erreur API Pixabay pour '{keyword}': {response.status_code}")
                print(f"   R√©ponse: {response.text[:100]}")
                return assets
            
            data = response.json()
            videos = data.get('hits', [])
            
            print(f"üìπ Pixabay trouv√©: {len(videos)} vid√©os pour '{keyword}'")
            
            for i, video in enumerate(videos):
                try:
                    video_files = video.get('videos', {})
                    if not video_files:
                        continue
                    
                    # Choisir la meilleure qualit√© disponible
                    quality_order = ['medium', 'small', 'tiny']  # medium = 1280x720 g√©n√©ralement
                    selected_quality = None
                    
                    for quality in quality_order:
                        if quality in video_files and video_files[quality].get('url'):
                            selected_quality = quality
                            break
                    
                    if not selected_quality:
                        print(f"‚ö†Ô∏è Aucune qualit√© disponible pour Pixabay video {video['id']}")
                        continue
                    
                    video_info = video_files[selected_quality]
                    download_url = video_info['url']
                    
                    # Nom du fichier
                    filename = f"{keyword}_{video['id']}_{i}.mp4"
                    file_path = fetch_dir / filename
                    
                    # Cr√©er le dossier
                    fetch_dir.mkdir(parents=True, exist_ok=True)
                    
                    print(f"üì• T√©l√©chargement Pixabay: {filename} ({selected_quality})")
                    
                    # T√©l√©charger
                    download_response = requests.get(download_url, stream=True, timeout=30)
                    download_response.raise_for_status()
                    
                    with open(file_path, 'wb') as f:
                        downloaded_size = 0
                        max_size = 15 * 1024 * 1024  # 15MB max pour Pixabay
                        
                        for chunk in download_response.iter_content(chunk_size=8192):
                            if chunk:
                                f.write(chunk)
                                downloaded_size += len(chunk)
                                if downloaded_size > max_size:
                                    break
                    
                    # üöÄ VALIDATION: V√©rifier l'int√©grit√© du fichier t√©l√©charg√©
                    if file_path.exists() and file_path.stat().st_size > 1000:
                        # Validation basique de l'int√©grit√© vid√©o
                        is_valid = True
                        try:
                            if filename.endswith(('.mp4', '.mov', '.avi', '.mkv')):
                                # Test d'ouverture rapide avec MoviePy
                                from moviepy import VideoFileClip
                                with VideoFileClip(str(file_path)) as test_clip:
                                    # V√©rifier que la dur√©e est coh√©rente
                                    if test_clip.duration <= 0 or test_clip.duration > 300:  # Max 5 minutes
                                        is_valid = False
                        except Exception:
                            is_valid = False
                            
                        if is_valid:
                            asset = Asset(
                                id=f"pixabay_{video['id']}",
                                file_path=str(file_path),
                                tags=[keyword, 'pixabay', 'video'] + video.get('tags', '').split(', '),
                                title=f"Pixabay {keyword} {video['id']}",
                                description=f"B-roll t√©l√©charg√© depuis Pixabay pour {keyword}",
                                source="pixabay_api",
                                fetched_at=datetime.now(),
                                duration=float(video.get('duration', 3.0)),
                                resolution=f"{video_info.get('width', 1280)}x{video_info.get('height', 720)}"
                            )
                            assets.append(asset)
                            print(f"‚úÖ T√©l√©charg√© Pixabay: {filename} ({file_path.stat().st_size / 1024 / 1024:.1f}MB)")
                        else:
                            print(f"‚ö†Ô∏è Fichier Pixabay corrompu ignor√©: {filename}")
                            try:
                                file_path.unlink()  # Supprimer le fichier corrompu
                            except:
                                pass
                    
                    if len(assets) >= 2:  # Limiter √† 2 par mot-cl√© par provider
                        break
                        
                except Exception as e:
                    print(f"‚ö†Ô∏è Erreur t√©l√©chargement Pixabay {i}: {e}")
                    continue
                    
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur recherche Pixabay '{keyword}': {e}")
        
        return assets
    
    def _fetch_from_unsplash(self, keyword: str, access_key: str, app_id: str, fetch_dir: Path) -> List[Asset]:
        """T√©l√©charge des images depuis Unsplash (photos haute qualit√©)"""
        assets = []
        try:
            print(f"üîç Recherche Unsplash: '{keyword}'")
            
            # API Unsplash pour les photos
            headers = {
                "Authorization": f"Client-ID {access_key}",
                "Accept-Version": "v1"
            }
            
            # Recherche de photos avec le mot-cl√©
            response = requests.get(
                f"https://api.unsplash.com/search/photos?query={keyword}&per_page=3&orientation=landscape",
                headers=headers,
                timeout=15
            )
            
            if response.status_code != 200:
                print(f"‚ö†Ô∏è Erreur API Unsplash pour '{keyword}': {response.status_code}")
                return assets
            
            data = response.json()
            photos = data.get('results', [])
            
            print(f"üì∏ Unsplash trouv√©: {len(photos)} photos pour '{keyword}'")
            
            for i, photo in enumerate(photos):
                try:
                    # Choisir la qualit√© regular (1080p) ou full (haute r√©solution)
                    urls = photo.get('urls', {})
                    download_url = urls.get('regular') or urls.get('full') or urls.get('small')
                    
                    if not download_url:
                        continue
                    
                    # Nom du fichier (image)
                    filename = f"{keyword}_{photo['id']}_{i}.jpg"
                    file_path = fetch_dir / filename
                    
                    # Cr√©er le dossier
                    fetch_dir.mkdir(parents=True, exist_ok=True)
                    
                    print(f"üì• T√©l√©chargement Unsplash: {filename}")
                    
                    # T√©l√©charger l'image
                    download_response = requests.get(download_url, stream=True, timeout=30)
                    download_response.raise_for_status()
                    
                    with open(file_path, 'wb') as f:
                        downloaded_size = 0
                        max_size = 5 * 1024 * 1024  # 5MB max pour les images
                        
                        for chunk in download_response.iter_content(chunk_size=8192):
                            if chunk:
                                f.write(chunk)
                                downloaded_size += len(chunk)
                                if downloaded_size > max_size:
                                    break
                    
                    # Cr√©er l'asset
                    if file_path.exists() and file_path.stat().st_size > 1000:
                        # Extraire les tags depuis la description/alt_description
                        photo_tags = [keyword, 'unsplash', 'photo']
                        if photo.get('alt_description'):
                            photo_tags.extend(photo['alt_description'].lower().split()[:5])
                        if photo.get('description'):
                            photo_tags.extend(photo['description'].lower().split()[:3])
                        
                        asset = Asset(
                            id=f"unsplash_{photo['id']}",
                            file_path=str(file_path),
                            tags=photo_tags,
                            title=f"Unsplash {keyword} {photo['id']}",
                            description=photo.get('alt_description') or photo.get('description') or f"Photo Unsplash pour {keyword}",
                            source="unsplash_api",
                            fetched_at=datetime.now(),
                            duration=3.0,  # Image statique, dur√©e par d√©faut pour Ken Burns
                            resolution=f"{photo.get('width', 1920)}x{photo.get('height', 1080)}"
                        )
                        assets.append(asset)
                        print(f"‚úÖ T√©l√©charg√© Unsplash: {filename} ({file_path.stat().st_size / 1024:.1f}KB)")
                    
                    if len(assets) >= 3:  # Limiter √† 3 par mot-cl√© pour Unsplash
                        break
                        
                except Exception as e:
                    print(f"‚ö†Ô∏è Erreur t√©l√©chargement Unsplash {i}: {e}")
                    continue
                    
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur recherche Unsplash '{keyword}': {e}")
        
        return assets
    
    def _fetch_from_archive_org(self, keyword: str, fetch_dir: Path) -> List[Asset]:
        """T√©l√©charge des vid√©os depuis Archive.org (gratuit, domaine public)"""
        assets = []
        try:
            print(f"üîç Recherche Archive.org: '{keyword}'")
            
            # API de recherche Archive.org
            # Recherche dans la collection de vid√©os open source
            search_query = f"collection:opensource_movies AND ({keyword})"
            
            response = requests.get(
                f"https://archive.org/advancedsearch.php",
                params={
                    'q': search_query,
                    'fl[]': ['identifier', 'title', 'description', 'downloads'],
                    'rows': 5,
                    'page': 1,
                    'output': 'json'
                },
                timeout=20
            )
            
            if response.status_code != 200:
                print(f"‚ö†Ô∏è Erreur API Archive.org pour '{keyword}': {response.status_code}")
                return assets
            
            data = response.json()
            items = data.get('response', {}).get('docs', [])
            
            print(f"üìπ Archive.org trouv√©: {len(items)} items pour '{keyword}'")
            
            for i, item in enumerate(items):
                try:
                    identifier = item.get('identifier', '')
                    if not identifier:
                        continue
                    
                    # Obtenir les d√©tails de l'item pour trouver des fichiers MP4
                    details_response = requests.get(
                        f"https://archive.org/metadata/{identifier}",
                        timeout=15
                    )
                    
                    if details_response.status_code != 200:
                        continue
                    
                    details = details_response.json()
                    files = details.get('files', [])
                    
                    # Chercher des fichiers MP4 de taille raisonnable
                    video_files = [
                        f for f in files 
                        if f.get('format', '').lower() in ['mpeg4', 'mp4'] 
                        and f.get('name', '').endswith('.mp4')
                        and int(f.get('size', '0')) < 50 * 1024 * 1024  # Moins de 50MB
                        and int(f.get('size', '0')) > 1 * 1024 * 1024   # Plus de 1MB
                    ]
                    
                    if not video_files:
                        continue
                    
                    # Prendre le premier fichier vid√©o valide
                    video_file = video_files[0]
                    filename_original = video_file['name']
                    
                    # Construire l'URL de t√©l√©chargement
                    download_url = f"https://archive.org/download/{identifier}/{filename_original}"
                    
                    # Nom du fichier local
                    filename = f"{keyword}_{identifier}_{i}.mp4"
                    file_path = fetch_dir / filename
                    
                    # Cr√©er le dossier
                    fetch_dir.mkdir(parents=True, exist_ok=True)
                    
                    print(f"üì• T√©l√©chargement Archive.org: {filename}")
                    
                    # T√©l√©charger (avec limite de taille)
                    download_response = requests.get(download_url, stream=True, timeout=45)
                    download_response.raise_for_status()
                    
                    with open(file_path, 'wb') as f:
                        downloaded_size = 0
                        max_size = 20 * 1024 * 1024  # 20MB max pour Archive.org
                        
                        for chunk in download_response.iter_content(chunk_size=8192):
                            if chunk:
                                f.write(chunk)
                                downloaded_size += len(chunk)
                                if downloaded_size > max_size:
                                    print(f"   ‚ö†Ô∏è T√©l√©chargement arr√™t√© √† 20MB")
                                    break
                    
                    # Cr√©er l'asset
                    if file_path.exists() and file_path.stat().st_size > 100000:  # Au moins 100KB
                        # Extraire des tags depuis le titre et la description
                        archive_tags = [keyword, 'archive', 'video', 'creative_commons']
                        title = item.get('title', '')
                        if title:
                            archive_tags.extend(title.lower().split()[:5])
                        
                        asset = Asset(
                            id=f"archive_{identifier}",
                            file_path=str(file_path),
                            tags=archive_tags,
                            title=title or f"Archive.org {keyword} {identifier}",
                            description=item.get('description', f"Vid√©o Archive.org pour {keyword}"),
                            source="archive_org",
                            fetched_at=datetime.now(),
                            duration=float(video_file.get('length', '10.0') or '10.0'),
                            resolution="unknown"
                        )
                        assets.append(asset)
                        print(f"‚úÖ T√©l√©charg√© Archive.org: {filename} ({file_path.stat().st_size / 1024 / 1024:.1f}MB)")
                    
                    if len(assets) >= 2:  # Limiter √† 2 par mot-cl√© pour Archive.org
                        break
                        
                except Exception as e:
                    print(f"‚ö†Ô∏è Erreur t√©l√©chargement Archive.org {i}: {e}")
                    continue
                    
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur recherche Archive.org '{keyword}': {e}")
        
        return assets
    
    def _create_fallback_assets(self, keywords: List[str]) -> List[Asset]:
        """Cr√©e des assets de fallback si le t√©l√©chargement √©choue"""
        assets = []
        
        for i, keyword in enumerate(keywords[:3]):
            asset = Asset(
                id=f"fallback_{keyword}_{i}",
                file_path=f"fallback_{keyword}.mp4",
                tags=[keyword, 'fallback'] + keywords[:3],
                title=f"Fallback {keyword.title()}",
                description=f"Asset de fallback pour {keyword}",
                source="fallback",
                fetched_at=datetime.now(),
                duration=2.0 + (i * 0.5),
                resolution="1920x1080"
            )
            assets.append(asset)
        
        return assets
    
    def _extract_tags_from_path(self, file_path: Path) -> List[str]:
        """Extrait des tags depuis le chemin du fichier"""
        tags = []
        
        # Extraire des tags depuis le nom du fichier
        filename = file_path.stem.lower()
        tags.extend(filename.split('_'))
        
        # Extraire des tags depuis les dossiers parents
        for parent in file_path.parents:
            if parent.name and parent.name != ".":
                tags.extend(parent.name.lower().split('_'))
        
        # Nettoyer et filtrer les tags
        clean_tags = []
        for tag in tags:
            if tag and len(tag) > 2 and tag not in ['clip', 'reframed', 'fetched', 'broll', 'library']:
                clean_tags.append(tag)
        
        return clean_tags[:10]  # Limiter √† 10 tags
    
    def score_asset(self, asset: Asset, query_keywords: Set[str], domain: Optional[str] = None) -> ScoringFeatures:
        """Calcule le score complet d'un asset"""
        features = ScoringFeatures()
        
        # 1. Token overlap (Jaccard)
        asset_tokens = set()
        if asset.tags:
            asset_tokens.update(asset.tags)
        if asset.title:
            asset_tokens.update(asset.title.lower().split())
        if asset.description:
            asset_tokens.update(asset.description.lower().split())
        
        if asset_tokens and query_keywords:
            intersection = len(asset_tokens & query_keywords)
            union = len(asset_tokens | query_keywords)
            features.token_overlap = intersection / union if union > 0 else 0.0
        
        # 2. Embedding similarity (si disponible)
        if self.embedding_model:
            try:
                # Cr√©er un texte de recherche depuis les mots-cl√©s
                query_text = " ".join(query_keywords)
                asset_text = " ".join([asset.title, asset.description] + asset.tags)
                
                # Calculer les embeddings
                query_embedding = self.embedding_model.encode([query_text])
                asset_embedding = self.embedding_model.encode([asset_text])
                
                # Calculer la similarit√© cosinus
                import numpy as np
                similarity = np.dot(query_embedding[0], asset_embedding[0]) / (
                    np.linalg.norm(query_embedding[0]) * np.linalg.norm(asset_embedding[0])
                )
                features.embedding_similarity = max(0.0, min(1.0, similarity))
            except Exception as e:
                self.logger.debug(f"‚ö†Ô∏è Erreur embedding similarity: {e}")
                # Fallback bas√© sur les tags pour les assets Pexels
                if asset.source == "pexels_api":
                    features.embedding_similarity = 0.7  # Score √©lev√© pour Pexels
                else:
                    features.embedding_similarity = 0.3
        
        # 3. Domain match
        if domain and asset.tags:
            domain_tokens = set(self._get_domain_expansions(domain))
            domain_overlap = len(set(asset.tags) & domain_tokens)
            features.domain_match = min(1.0, domain_overlap / max(len(domain_tokens), 1))
        
        # 4. Freshness
        if asset.fetched_at:
            days_diff = (datetime.now() - asset.fetched_at).days
            half_life = self.config['freshness_half_life_days']
            features.freshness = 1.0 / (1.0 + days_diff / half_life)
        
        # 5. Quality score
        if asset.source == "pexels_api":
            features.quality_score = 0.9  # Score √©lev√© pour Pexels (qualit√© garantie)
        elif "1920x1080" in asset.resolution or "hd" in asset.resolution.lower():
            features.quality_score = 0.8  # HD quality
        else:
            features.quality_score = 0.6  # Standard quality
        
        # 6. Diversity penalty (sera calcul√© plus tard)
        features.diversity_penalty = 0.0
        
        return features
    
    def calculate_final_score(self, features: ScoringFeatures) -> float:
        """Calcule le score final pond√©r√©"""
        weights = self.config['weights']
        
        score = (
            weights['embedding'] * features.embedding_similarity +
            weights['token'] * features.token_overlap +
            weights['domain'] * features.domain_match +
            weights['freshness'] * features.freshness +
            weights['quality'] * features.quality_score -
            weights['diversity'] * features.diversity_penalty
        )
        
        return max(0.0, min(1.0, score))
    
    def _should_use_direct_mode(self, keywords: List[str], domain: Optional[str] = None) -> bool:
        """
        D√©cide intelligemment si utiliser le mode direct ou la s√©lection
        
        UTILISE MODE DIRECT pour:
        - Mots-cl√©s sp√©cifiques et visuels concrets
        - Domaines o√π les APIs excellent (sant√©, business, tech)
        
        UTILISE S√âLECTION pour:
        - Concepts abstraits
        - Mots-cl√©s g√©n√©riques
        - Besoin de coh√©rence narrative
        """
        
        # üéØ CRIT√àRES POUR MODE DIRECT (High confidence)
        concrete_indicators = {
            'professional_actions': ['talking', 'presenting', 'meeting', 'consultation', 'interview'],
            'specific_professions': ['doctor', 'therapist', 'teacher', 'engineer', 'lawyer'],
            'clear_objects': ['handshake', 'computer', 'stethoscope', 'whiteboard', 'documents'],
            'defined_settings': ['office', 'hospital', 'classroom', 'laboratory', 'clinic']
        }
        
        # üö® CRIT√àRES CONTRE MODE DIRECT (Requires smart selection)
        abstract_indicators = {
            'emotions': ['happiness', 'success', 'motivation', 'growth', 'inspiration'],
            'concepts': ['achievement', 'progress', 'innovation', 'excellence', 'quality'],
            'vague_terms': ['content', 'media', 'general', 'various', 'different']
        }
        
        # Analyser les mots-cl√©s
        keyword_text = ' '.join(keywords).lower()
        
        # Score de concr√©tude
        concrete_score = 0
        abstract_score = 0
        
        for category, terms in concrete_indicators.items():
            for term in terms:
                if term in keyword_text:
                    concrete_score += 2
        
        for category, terms in abstract_indicators.items():
            for term in terms:
                if term in keyword_text:
                    abstract_score += 1
        
        # Bonus pour mots-cl√©s structur√©s (person_doing_something)
        structured_keywords = [kw for kw in keywords if '_' in kw and len(kw.split('_')) >= 2]
        if structured_keywords:
            concrete_score += len(structured_keywords) * 1.5
        
        # Bonus pour domaines o√π APIs excellent
        api_friendly_domains = ['healthcare', 'business', 'technology', 'education']
        if domain and domain.lower() in api_friendly_domains:
            concrete_score += 3
        
        # D√©cision
        use_direct = concrete_score > abstract_score and concrete_score >= 4
        
        print(f"ü§ñ D√âCISION INTELLIGENTE:")
        print(f"   Concret: {concrete_score:.1f} | Abstrait: {abstract_score:.1f}")
        print(f"   Mode: {'DIRECT API' if use_direct else 'S√âLECTION INTELLIGENTE'}")
        print(f"   Raison: {'APIs excellent pour ce contenu' if use_direct else 'Besoin de curation contextuelle'}")
        
        return use_direct

    def select_brolls(self, keywords: List[str], domain: Optional[str] = None, 
                      min_delay: float = 4.0, desired_count: int = 3) -> Dict[str, Any]:
        """
        S√©lection intelligente : d√©cide automatiquement entre direct et s√©lection
        """
        try:
            print(f"üé¨ S√©lection B-roll: {len(keywords)} mots-cl√©s, domaine: {domain or 'g√©n√©ral'}")
            
            # üß† D√âCISION INTELLIGENTE bas√©e sur le contenu
            if self.direct_api_mode:
                # Forcer le mode direct si explicitement demand√©
                use_direct = True
                print("üîí MODE DIRECT FORC√â par configuration")
            else:
                # D√©cision intelligente automatique
                use_direct = self._should_use_direct_mode(keywords, domain)
            
            if use_direct:
                return self._select_brolls_direct_api(keywords, domain, min_delay, desired_count)
            else:
                return self._select_brolls_smart_selection(keywords, domain, min_delay, desired_count)
            
        except Exception as e:
            print(f"‚ùå Erreur s√©lection B-roll: {e}")
            return self._create_empty_report()

    def _select_brolls_smart_selection(self, keywords: List[str], domain: Optional[str], 
                                      min_delay: float, desired_count: int) -> Dict[str, Any]:
        """Mode S√âLECTION INTELLIGENTE : curation contextuelle pour concepts abstraits"""
        print("üß† MODE S√âLECTION INTELLIGENTE : Curation contextuelle pour votre contenu")
            
        # R√©cup√©rer plus de candidats pour avoir le choix
        api_limit = self.config.get('direct_api_limit', 5) * 3  # 3x plus de candidats
        candidate_assets = self._fetch_from_apis(keywords, limit=api_limit)
            
        if not candidate_assets:
            print("‚ö†Ô∏è Aucun asset trouv√© via APIs - Fallback vers librairie locale")
            return self._create_fallback_report(desired_count)
            
        # Appliquer scoring intelligent
        normalized_keywords = self.normalize_keywords(keywords)
        scored_candidates = []
        
        for asset in candidate_assets:
            features = self.score_asset(asset, normalized_keywords, domain)
            final_score = self.calculate_final_score(features)
            
            candidate = BrollCandidate(
                asset=asset,
                score=final_score,
                features=features
            )
            scored_candidates.append(candidate)
            
        # Trier et s√©lectionner les meilleurs
        scored_candidates.sort(key=lambda x: x.score, reverse=True)
            
        # Seuil adaptatif
        min_score = self._calculate_adaptive_threshold(scored_candidates)
        selected = [c for c in scored_candidates if c.score >= min_score]
            
        # Assurer diversit√©
        final_selection = self.ensure_diversity(selected, desired_count)
            
        print(f"‚úÖ S√âLECTION INTELLIGENTE : {len(final_selection)} B-rolls cur√©s")
        print(f"   üìä Candidats √©valu√©s: {len(candidate_assets)} ‚Üí S√©lectionn√©s: {len(final_selection)}")
        print(f"   üéØ Seuil qualit√©: {min_score:.2f}")
        
        return {
            'selected': [c.to_dict() for c in final_selection],
            'total_candidates': len(candidate_assets),
            'selection_method': 'smart_selection',
            'keywords_used': keywords,
            'domain': domain,
            'fallback_used': False,
            'diagnostics': {
                'num_selected': len(final_selection),
                'num_candidates': len(candidate_assets),
                'top_score': final_selection[0].score if final_selection else 0.0,
                'min_score': min_score,
                'selection_method': 'smart_selection'
            }
        }

    def _select_brolls_direct_api(self, keywords: List[str], domain: Optional[str], 
                                 min_delay: float, desired_count: int) -> Dict[str, Any]:
        """Mode DIRECT : utilise directement les meilleurs r√©sultats API"""
        print("üöÄ MODE DIRECT API : Utilisation directe des r√©sultats Pexels/Pixabay")
        
        # R√©cup√©rer directement depuis les APIs
        api_limit = self.config.get('direct_api_limit', 5)
        direct_assets = self._fetch_from_apis(keywords, limit=api_limit)
        
        if not direct_assets:
            print("‚ö†Ô∏è Aucun asset trouv√© via APIs - Fallback vers librairie locale")
            return self._create_fallback_report(desired_count)
        
        # Prendre directement les X premiers (pas de re-scoring complexe)
        selected_count = min(desired_count, len(direct_assets))
        selected_assets = direct_assets[:selected_count]
        
        # Cr√©er des candidats simples
        selected_candidates = []
        for i, asset in enumerate(selected_assets):
            candidate = BrollCandidate(
                asset=asset,
                score=1.0 - (i * 0.1),  # Score d√©croissant simple
                features=ScoringFeatures(
                    token_overlap=1.0,
                    embedding_similarity=0.9,
                    domain_match=0.8,
                    freshness=1.0,
                    quality_score=0.9
                )
            )
            selected_candidates.append(candidate)
        
        print(f"‚úÖ MODE DIRECT : {len(selected_candidates)} B-rolls s√©lectionn√©s directement")
        for i, candidate in enumerate(selected_candidates):
            print(f"   {i+1}. {Path(candidate.asset.file_path).name} (source: {candidate.asset.source})")
        
        return {
            'selected': [c.to_dict() for c in selected_candidates],
            'total_candidates': len(direct_assets),
            'selection_method': 'direct_api',
            'keywords_used': keywords,
            'domain': domain,
            'fallback_used': False,
            'diagnostics': {
                'num_selected': len(selected_candidates),
                'num_candidates': len(direct_assets),
                'top_score': selected_candidates[0].score if selected_candidates else 0.0,
                'min_score': 1.0,
                'selection_method': 'direct_api'
            }
        }

    def _select_brolls_classic(self, keywords: List[str], domain: Optional[str], 
                              min_delay: float, desired_count: int) -> Dict[str, Any]:
        """Mode classique avec re-s√©lection et scoring complexe"""
        print("üîç MODE CLASSIQUE : Re-scoring des r√©sultats avec s√©lection intelligente")
        
        # Ancien comportement (votre code existant)
        normalized_keywords = self.normalize_keywords(keywords)
        expanded_keywords = self.expand_keywords(list(normalized_keywords), domain)
        
        # Fetch from multiple sources
        candidates_assets = self.fetch_assets(expanded_keywords, limit=self.config['max_candidates'])
        
        if not candidates_assets:
            return self._create_empty_report()
        
        # Score all candidates
        candidates = []
        for asset in candidates_assets:
            features = self.score_asset(asset, normalized_keywords, domain)
            final_score = self.calculate_final_score(features)
            
            candidate = BrollCandidate(
                asset=asset,
                score=final_score,
                features=features
            )
            candidates.append(candidate)
        
        # Rest of classic selection logic...
        # (votre code existant pour le scoring complexe)
        return self._create_selection_report([], candidates, keywords, domain, 0.3)
    
    # üöÄ NOUVEAU: Fonction de compatibilit√© pour le pipeline existant
    def find_broll_matches(self, keywords: List[str], domain: Optional[str] = None, 
                          max_results: int = 10) -> List[Dict[str, Any]]:
        """
        Fonction de compatibilit√© pour le pipeline existant.
        Retourne les correspondances B-roll dans le format attendu.
        
        Args:
            keywords: Mots-cl√©s de recherche
            domain: Domaine d√©tect√©
            max_results: Nombre maximum de r√©sultats
        
        Returns:
            Liste des correspondances au format pipeline
        """
        try:
            # Utiliser la logique de s√©lection principale
            selection_report = self.select_brolls(
                keywords=keywords,
                domain=domain,
                desired_count=max_results
            )
            
            # Convertir au format attendu par le pipeline
            matches = []
            for candidate in selection_report.get('selected', []):
                match = {
                    'asset_id': candidate.asset.id,
                    'file_path': candidate.asset.file_path,
                    'score': candidate.score,
                    'tags': candidate.asset.tags,
                    'title': candidate.asset.title,
                    'description': candidate.asset.description,
                    'source': candidate.asset.source,
                    'duration': candidate.asset.duration,
                    'resolution': candidate.asset.resolution
                }
                matches.append(match)
            
            return matches
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur find_broll_matches: {e}")
            return []
    
    def _apply_fallback_hierarchy(self, candidates: List[BrollCandidate], 
                                 selected: List[BrollCandidate], desired_count: int,
                                 min_delay: float) -> Tuple[bool, Optional[str], List[BrollCandidate]]:
        """Applique le fallback hi√©rarchique"""
        self.logger.info("üÜò Activation du fallback hi√©rarchique")
        
        # Tier A: Domain-broad (expansion forte)
        tier_a_candidates = self._get_tier_a_candidates(candidates, selected, min_delay)
        if len(selected) + len(tier_a_candidates) >= desired_count:
            selected.extend(tier_a_candidates[:desired_count - len(selected)])
            return True, "A", selected
        
        # Tier B: Contextual semi-relevant
        tier_b_candidates = self._get_tier_b_candidates(candidates, selected, min_delay)
        if len(selected) + len(tier_b_candidates) >= desired_count:
            selected.extend(tier_b_candidates[:desired_count - len(selected)])
            return True, "B", selected
        
        # Tier C: Neutral scenic
        tier_c_candidates = self._get_tier_c_candidates(candidates, selected, min_delay)
        selected.extend(tier_c_candidates[:desired_count - len(selected)])
        
        return True, "C", selected
    
    def _get_tier_a_candidates(self, candidates: List[BrollCandidate], 
                              selected: List[BrollCandidate], min_delay: float) -> List[BrollCandidate]:
        """Tier A: Domain-broad (expansion forte)"""
        # Filtrer les candidats d√©j√† s√©lectionn√©s et respectant le d√©lai
        available = [c for c in candidates if c not in selected]
        return self._filter_by_timing(available, min_delay)
    
    def _get_tier_b_candidates(self, candidates: List[BrollCandidate], 
                              selected: List[BrollCandidate], min_delay: float) -> List[BrollCandidate]:
        """Tier B: Contextual semi-relevant (actions, √©motions, gestes)"""
        # Chercher des assets avec des tags g√©n√©riques mais s√ªrs
        safe_tags = {'people', 'family', 'walking', 'talking', 'working', 'thinking'}
        
        available = []
        for c in candidates:
            if c not in selected:
                asset_tags = set(c.asset.tags)
                if asset_tags & safe_tags:
                    available.append(c)
        
        return self._filter_by_timing(available, min_delay)
    
    def _get_tier_c_candidates(self, candidates: List[BrollCandidate], 
                              selected: List[BrollCandidate], min_delay: float) -> List[BrollCandidate]:
        """Tier C: Neutral scenic (paysages, textures)"""
        # √âviter les termes fortement hors-sujet
        neutral_tags = {'landscape', 'texture', 'abstract', 'nature', 'city'}
        
        available = []
        for c in candidates:
            if c not in selected:
                asset_tags = set(c.asset.tags)
                if asset_tags & neutral_tags:
                    available.append(c)
        
        return self._filter_by_timing(available, min_delay)
    
    def _filter_by_timing(self, candidates: List[BrollCandidate], min_delay: float) -> List[BrollCandidate]:
        """Filtre les candidats par timing"""
        filtered = []
        last_end_time = 0.0
        
        for candidate in candidates:
            if last_end_time == 0.0 or (candidate.asset.duration + last_end_time) >= min_delay:
                filtered.append(candidate)
                last_end_time = candidate.asset.duration + last_end_time
        
        return filtered
    
    def _create_report(self, keywords: List[str], domain: Optional[str], 
                       candidates: List[BrollCandidate], selected: List[BrollCandidate],
                       fallback_used: bool, fallback_tier: Optional[str],
                       top_score: float, min_score: float) -> Dict[str, Any]:
        """Cr√©e le rapport JSON d√©taill√©"""
        return {
            'video_id': f"video_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            'timestamp': datetime.now().isoformat(),
            'keywords': keywords,
            'domain': domain,
            'planned_candidates': [c.to_dict() for c in candidates],
            'selected': [c.to_dict() for c in selected],
            'fallback_used': fallback_used,
            'fallback_tier': fallback_tier,
            'diagnostics': {
                'top_score': top_score,
                'min_score': min_score,
                'num_candidates': len(candidates),
                'num_selected': len(selected),
                'selection_ratio': len(selected) / len(candidates) if candidates else 0.0
            }
        }
    
    def _create_empty_report(self) -> Dict[str, Any]:
        """Cr√©e un rapport vide en cas d'erreur"""
        return {
            'video_id': f"video_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            'timestamp': datetime.now().isoformat(),
            'error': 'No assets available',
            'planned_candidates': [],
            'selected': [],
            'fallback_used': False,
            'fallback_tier': None,
            'diagnostics': {
                'top_score': 0.0,
                'min_score': 0.0,
                'num_candidates': 0,
                'num_selected': 0,
                'selection_ratio': 0.0
            }
        }

    def _create_fallback_report(self, desired_count: int) -> Dict[str, Any]:
        """Cr√©e un rapport de fallback quand aucun asset n'est trouv√©"""
        return {
            'selected': [],
            'excluded': [],
            'fallback_used': True,
            'fallback_tier': 'C',
            'diagnostics': {
                'num_candidates': 0,
                'num_selected': 0,
                'top_score': 0.0,
                'min_score': 0.0,
                'selection_ratio': 0.0
            },
            'keywords': [],
            'domain': None,
            'timestamp': datetime.now().isoformat()
        }
    
    def _create_selection_report(self, selected: List[BrollCandidate], candidates: List[BrollCandidate],
                                keywords: List[str], domain: Optional[str], min_score: float) -> Dict[str, Any]:
        """Cr√©e le rapport de s√©lection complet"""
        return {
            'selected': selected,
            'excluded': [c for c in candidates if c not in selected],
            'fallback_used': False,
            'fallback_tier': None,
            'diagnostics': {
                'num_candidates': len(candidates),
                'num_selected': len(selected),
                'top_score': candidates[0].score if candidates else 0.0,
                'min_score': min_score,
                'selection_ratio': len(selected) / len(candidates) if candidates else 0.0
            },
            'keywords': keywords,
            'domain': domain,
            'timestamp': datetime.now().isoformat()
        }
    
    def _calculate_adaptive_threshold(self, candidates: List[BrollCandidate]) -> float:
        """Calcule le seuil adaptatif bas√© sur les scores"""
        if not candidates:
            return 0.0
        
        top_score = candidates[0].score
        global_min = self.config['thresholds']['global_min']
        relative_factor = self.config['thresholds']['relative_factor']
        
        return max(global_min, top_score * relative_factor)
    
    def ensure_diversity(self, selected: List[BrollCandidate], desired_count: int) -> List[BrollCandidate]:
        """Assure la diversit√© des sources et du contenu"""
        if len(selected) <= desired_count:
            return selected
        
        # Prioriser la diversit√© des sources
        diverse_selection = []
        used_sources = set()
        
        for candidate in selected:
            if len(diverse_selection) >= desired_count:
                break
            
            if candidate.asset.source not in used_sources:
                diverse_selection.append(candidate)
                used_sources.add(candidate.asset.source)
        
        # Compl√©ter avec les meilleurs scores si n√©cessaire
        while len(diverse_selection) < desired_count and len(selected) > len(diverse_selection):
            for candidate in selected:
                if candidate not in diverse_selection:
                    diverse_selection.append(candidate)
                    break
        
        return diverse_selection[:desired_count]

# Instance globale paresseuse pour compatibilit√©
_broll_selector_instance: Optional[BrollSelector] = None


def get_broll_selector(config: Optional[Dict[str, Any]] = None, *, force_reload: bool = False) -> BrollSelector:
    """Retourne une instance partag√©e du :class:`BrollSelector`.

    Cette fonction instancie le s√©lecteur uniquement lors de la premi√®re
    utilisation, √©vitant ainsi les effets de bord (logs, t√©l√©chargements,
    initialisations co√ªteuses) pendant l'import du module.

    Args:
        config: Configuration optionnelle √† fusionner lors de la cr√©ation ou √†
            appliquer dynamiquement si l'instance existe d√©j√†.
        force_reload: Si ``True``, remplace l'instance existante par une
            nouvelle en utilisant la configuration fournie.
    """

    global _broll_selector_instance

    if force_reload or _broll_selector_instance is None:
        _broll_selector_instance = BrollSelector(config)
    elif config:
        # Mettre √† jour dynamiquement la configuration existante
        _broll_selector_instance.config.update(config)

    return _broll_selector_instance


class _BrollSelectorProxy:
    """Proxy l√©ger conservant la compatibilit√© avec l'ancienne API module."""

    def __call__(self, config: Optional[Dict[str, Any]] = None, *, force_reload: bool = False) -> BrollSelector:
        return get_broll_selector(config, force_reload=force_reload)

    def __getattr__(self, item: str) -> Any:
        return getattr(get_broll_selector(), item)

    def __repr__(self) -> str:
        instance = _broll_selector_instance
        if instance is None:
            return "<BrollSelector lazy proxy (uninitialized)>"
        return repr(instance)


broll_selector = _BrollSelectorProxy()

# üöÄ FONCTION DE COMPATIBILIT√â MANQUANTE
def find_broll_matches(keywords: List[str], max_count: int = 10, 
                       min_duration: float = 2.0, max_duration: float = 15.0,
                       **kwargs) -> List[Dict[str, Any]]:
    """
    Fonction de compatibilit√© pour l'ancien syst√®me
    Utilise le nouveau BrollSelector pour maintenir la compatibilit√©
    """
    try:
        # Utiliser l'instance globale du BrollSelector
        selector = get_broll_selector()
        
        # Normaliser et √©tendre les mots-cl√©s
        normalized_keywords = selector.normalize_keywords(keywords)
        expanded_keywords = selector.expand_keywords(list(normalized_keywords))

        # S√©lectionner les B-rolls
        result = selector.select_brolls(
            keywords=expanded_keywords,
            desired_count=max_count
        )

        # Convertir en format compatible
        matches = []
        for candidate in result.get('selected', []):
            asset = candidate.get('asset', {}) if isinstance(candidate, dict) else {}
            matches.append({
                'file_path': asset.get('file_path'),
                'duration': asset.get('duration'),
                'score': candidate.get('score') if isinstance(candidate, dict) else None,
                'tags': asset.get('tags'),
                'source': asset.get('source')
            })

        return matches
        
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur dans find_broll_matches: {e}")
        return []

# üöÄ FONCTION CONTEXTUELLE CORRIG√âE
def get_contextual_broll_score(keywords: List[str], asset_tokens: List[str], asset_tags: List[str]) -> float:
    """
    Calcule un score contextuel intelligent pour la s√©lection B-roll
    Compatibilit√© avec l'ancien syst√®me - CORRIG√â pour retourner des scores r√©els
    """
    try:
        score = 0.0
        
        # Mapping contextuel simplifi√© pour compatibilit√©
        CONTEXTUAL_MAPPING = {
            'technology': {
                'keywords': ['ai', 'artificial', 'intelligence', 'tech', 'digital', 'smartphone', 'computer', 'software', 'app', 'online', 'automation'],
                'priority': 8.0,
                'broll_themes': ['technology', 'digital', 'computer', 'smartphone', 'tech', 'software']
            },
            'health': {
                'keywords': ['health', 'medical', 'doctor', 'hospital', 'care', 'wellness', 'fitness', 'medicine', 'treatment'],
                'priority': 7.0,
                'broll_themes': ['health', 'medical', 'hospital', 'doctor', 'wellness', 'fitness']
            },
            'business': {
                'keywords': ['business', 'money', 'finance', 'work', 'office', 'corporate', 'company', 'startup', 'entrepreneur', 'profit', 'revenue'],
                'priority': 6.0,
                'broll_themes': ['business', 'office', 'money', 'work', 'corporate', 'finance']
            },
            'lifestyle': {
                'keywords': ['family', 'home', 'food', 'travel', 'fitness', 'leisure', 'cooking', 'restaurant', 'vacation'],
                'priority': 5.0,
                'broll_themes': ['family', 'home', 'food', 'travel', 'lifestyle', 'cooking']
            },
            'education': {
                'keywords': ['learn', 'study', 'education', 'school', 'university', 'knowledge', 'teaching', 'student', 'course'],
                'priority': 5.5,
                'broll_themes': ['education', 'school', 'study', 'learning', 'university']
            }
        }
        
        # üö® CORRECTION: Normaliser les tokens et tags pour comparaison
        asset_tokens_lower = [token.lower().strip() for token in asset_tokens if token]
        asset_tags_lower = [tag.lower().strip() for tag in asset_tags if tag]
        keywords_lower = [kw.lower().strip() for kw in keywords if kw]
        
        # Analyser le contexte des mots-cl√©s
        context_matches = []
        for keyword in keywords_lower:
            # V√©rifier le mapping contextuel
            for context, mapping in CONTEXTUAL_MAPPING.items():
                if keyword in mapping['keywords']:
                    context_matches.append(context)
                    # Score de base selon la priorit√© du contexte
                    score += mapping['priority']
                    
                    # üö® CORRECTION: Bonus pour les th√®mes B-roll correspondants
                    asset_text_combined = ' '.join(asset_tokens_lower + asset_tags_lower)
                    theme_matches = 0
                    for theme in mapping['broll_themes']:
                        if theme in asset_text_combined:
                            theme_matches += 1
                            score += 5.0  # Bonus majeur pour correspondance parfaite
                    
                    # üö® CORRECTION: Bonus pour les tags correspondants directs
                    tag_matches = 0
                    for tag in asset_tags_lower:
                        if any(kw in tag for kw in mapping['keywords']):
                            tag_matches += 1
                            score += 3.0  # Bonus pour correspondance de tags
                    
                    # üö® NOUVEAU: Bonus pour correspondance directe mot-cl√©
                    if keyword in asset_text_combined:
                        score += 10.0  # Bonus tr√®s √©lev√© pour correspondance exacte
                    
                    break
        
        # üö® NOUVEAU: Bonus de diversit√© contextuelle
        unique_contexts = len(set(context_matches))
        if unique_contexts > 1:
            score += unique_contexts * 2.0  # Bonus pour diversit√©
        
        # üö® NOUVEAU: Fallback scoring pour mots-cl√©s non mapp√©s
        if score == 0.0:
            # Score basique bas√© sur correspondances lexicales
            for keyword in keywords_lower:
                # Correspondance exacte dans tokens/tags
                if keyword in asset_tokens_lower or keyword in asset_tags_lower:
                    score += 2.0
                # Correspondance partielle
                elif any(keyword in token for token in asset_tokens_lower + asset_tags_lower):
                    score += 1.0
        
        # üö® NOUVEAU: Bonus pour mots-cl√©s sp√©cifiques avec underscores
        for keyword in keywords_lower:
            if '_' in keyword:  # Mots-cl√©s format "person_talking_to_therapist"
                # Ces mots-cl√©s sont tr√®s sp√©cifiques, bonus majeur
                score += 15.0
                
                # D√©composer et chercher les parties
                parts = keyword.split('_')
                for part in parts:
                    if part in asset_text_combined:
                        score += 5.0  # Bonus pour chaque partie trouv√©e
        
        # üß† NOUVEAU: Bonus pour concepts directs importants (cerveau, science, etc.)
        concept_terms = ['brain', 'neurons', 'neural', 'science', 'medical', 'technology', 'business', 'education', 'adrenaline', 'chemical', 'hormone', 'neurotransmitter']
        for keyword in keywords_lower:
            for concept in concept_terms:
                if concept in keyword and concept in asset_text_combined:
                    score += 20.0  # Bonus tr√®s √©lev√© pour concepts sp√©cialis√©s
                    print(f"    üéØ Bonus concept sp√©cialis√©: {concept} ‚Üí +20.0")
                    break  # Un seul bonus par mot-cl√©
        
        # üî¨ NOUVEAU: Super bonus pour mots-cl√©s tr√®s sp√©cifiques
        specialized_terms = ['brain_scan', 'neural_networks', 'adrenaline_concept', 'chemical_reaction', 'medical_research']
        for keyword in keywords_lower:
            for specialized in specialized_terms:
                if specialized in keyword:
                    score += 25.0  # Super bonus pour termes tr√®s sp√©cialis√©s
                    print(f"    üöÄ Super bonus sp√©cialis√©: {specialized} ‚Üí +25.0")
                    break
        
        # üö® CORRECTION: S'assurer qu'on retourne un score > 0 si pertinent
        final_score = max(0.0, score)
        
        # Debug logging pour diagnostiquer
        if final_score > 0:
            print(f"    üéØ Score contextuel: {final_score:.1f} | Mots-cl√©s: {keywords_lower[:3]} | Contextes: {set(context_matches)}")
        
        return final_score
        
    except Exception as e:
        print(f"‚ùå Erreur calcul score contextuel: {e}")
        return 1.0  # üö® CORRECTION: Retour fallback > 0 au lieu de 0.0 

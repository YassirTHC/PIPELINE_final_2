#!/usr/bin/env python3
"""
ğŸ” VÃ‰RIFICATEUR POST-INSTALLATION LLM
Script de vÃ©rification aprÃ¨s migration vers llama3.2:8b
"""

import subprocess
import json
import time
import psutil
import os
from pathlib import Path

def verifier_installation_llm():
    """VÃ©rifier l'installation et la configuration du nouveau LLM"""
    print("ğŸ” VÃ‰RIFICATEUR POST-INSTALLATION LLM")
    print("=" * 60)
    
    try:
        # 1. VÃ©rifier qu'Ollama est en cours d'exÃ©cution
        print("\nğŸ“Š Ã‰TAPE 1: VÃ©rification d'Ollama")
        print("-" * 40)
        
        try:
            result = subprocess.run(["ollama", "list"], capture_output=True, text=True, timeout=10)
            if result.returncode == 0:
                print("âœ… Ollama est accessible")
                print(f"ğŸ“‹ ModÃ¨les disponibles:\n{result.stdout}")
            else:
                print(f"âŒ Erreur Ollama: {result.stderr}")
                return False
        except Exception as e:
            print(f"âŒ Ollama non accessible: {e}")
            return False
        
        # 2. VÃ©rifier que llama3.2:8b est installÃ©
        print("\nğŸ“Š Ã‰TAPE 2: VÃ©rification du modÃ¨le llama3.2:8b")
        print("-" * 40)
        
        if "llama3.2:8b" in result.stdout:
            print("âœ… ModÃ¨le llama3.2:8b dÃ©tectÃ©")
        else:
            print("âŒ ModÃ¨le llama3.2:8b NON dÃ©tectÃ©")
            print("ğŸš€ Installation en cours...")
            try:
                install_result = subprocess.run(["ollama", "pull", "llama3.2:8b"], 
                                             capture_output=True, text=True, timeout=300)
                if install_result.returncode == 0:
                    print("âœ… Installation rÃ©ussie")
                else:
                    print(f"âŒ Ã‰chec installation: {install_result.stderr}")
                    return False
            except Exception as e:
                print(f"âŒ Erreur installation: {e}")
                return False
        
        # 3. VÃ©rifier la configuration centralisÃ©e
        print("\nğŸ“Š Ã‰TAPE 3: VÃ©rification de la configuration centralisÃ©e")
        print("-" * 40)
        
        config_path = Path("config/llm_config.yaml")
        if config_path.exists():
            print("âœ… Fichier de configuration LLM trouvÃ©")
            try:
                import yaml
                with open(config_path, "r", encoding="utf-8") as f:
                    config = yaml.safe_load(f)
                print(f"ğŸ“ ModÃ¨le configurÃ©: {config['llm']['model']}")
                print(f"ğŸ“ Fallback: {config['llm']['fallback_model']}")
                print(f"ğŸ“ Validation JSON: {config['llm']['enforce_json_output']}")
            except Exception as e:
                print(f"âš ï¸ Erreur lecture config: {e}")
        else:
            print("âŒ Fichier de configuration LLM non trouvÃ©")
            return False
        
        # 4. VÃ©rifier la mÃ©moire disponible
        print("\nğŸ“Š Ã‰TAPE 4: VÃ©rification de la mÃ©moire")
        print("-" * 40)
        
        memory = psutil.virtual_memory()
        print(f"ğŸ’¾ RAM totale: {memory.total / (1024**3):.1f} GB")
        print(f"ğŸ’¾ RAM disponible: {memory.available / (1024**3):.1f} GB")
        print(f"ğŸ’¾ RAM utilisÃ©e: {memory.percent:.1f}%")
        
        if memory.available / (1024**3) < 8:
            print("âš ï¸ ATTENTION: Moins de 8GB RAM disponible")
            print("   Le modÃ¨le llama3.2:8b peut Ãªtre lent ou instable")
        else:
            print("âœ… RAM suffisante pour llama3.2:8b")
        
        # 5. Test de communication avec le modÃ¨le
        print("\nğŸ“Š Ã‰TAPE 5: Test de communication avec le modÃ¨le")
        print("-" * 40)
        
        try:
            test_prompt = '{"test": "simple"}'
            test_payload = {
                "model": "llama3.2:8b",
                "prompt": f"Output this exact JSON: {test_prompt}",
                "temperature": 0.1,
                "stream": False
            }
            
            import requests
            start_time = time.time()
            response = requests.post("http://localhost:11434/api/generate", 
                                  json=test_payload, timeout=30)
            end_time = time.time()
            
            if response.status_code == 200:
                data = response.json()
                response_text = data.get("response", "")
                response_time = end_time - start_time
                
                print(f"âœ… Communication rÃ©ussie en {response_time:.1f}s")
                print(f"ğŸ“Š Taille rÃ©ponse: {len(response_text)} caractÃ¨res")
                
                # VÃ©rifier si la rÃ©ponse contient le JSON de test
                if test_prompt in response_text:
                    print("âœ… RÃ©ponse JSON correcte")
                else:
                    print(f"âš ï¸ RÃ©ponse JSON diffÃ©rente: {response_text[:100]}...")
                    
            else:
                print(f"âŒ Erreur HTTP: {response.status_code}")
                return False
                
        except Exception as e:
            print(f"âŒ Erreur test communication: {e}")
            return False
        
        # 6. VÃ©rifier les fichiers de test mis Ã  jour
        print("\nğŸ“Š Ã‰TAPE 6: VÃ©rification des fichiers de test")
        print("-" * 40)
        
        test_files = [
            "test_prompt_optimise.py",
            "test_pipeline_direct_136.py", 
            "test_pipeline_complet.py",
            "test_prompt_avec_video.py"
        ]
        
        for test_file in test_files:
            if Path(test_file).exists():
                with open(test_file, "r", encoding="utf-8") as f:
                    content = f.read()
                if "llama3.2:8b" in content:
                    print(f"âœ… {test_file} - RÃ©fÃ©rence LLM mise Ã  jour")
                else:
                    print(f"âŒ {test_file} - RÃ©fÃ©rence LLM non mise Ã  jour")
            else:
                print(f"âš ï¸ {test_file} - Fichier non trouvÃ©")
        
        # 7. RÃ©sumÃ© final
        print("\nğŸ“Š RÃ‰SUMÃ‰ FINAL")
        print("=" * 40)
        print("âœ… Ollama accessible et fonctionnel")
        print("âœ… ModÃ¨le llama3.2:8b installÃ©")
        print("âœ… Configuration centralisÃ©e active")
        print("âœ… Communication avec le modÃ¨le rÃ©ussie")
        print("âœ… Fichiers de test mis Ã  jour")
        
        if memory.available / (1024**3) >= 8:
            print("âœ… RAM suffisante pour les performances optimales")
        else:
            print("âš ï¸ RAM limitÃ©e - performances peuvent Ãªtre dÃ©gradÃ©es")
        
        print("\nğŸ‰ MIGRATION LLM TERMINÃ‰E AVEC SUCCÃˆS !")
        print("ğŸš€ Le pipeline est prÃªt Ã  utiliser llama3.2:8b")
        
        return True
        
    except Exception as e:
        print(f"âŒ Erreur lors de la vÃ©rification: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    verifier_installation_llm() 
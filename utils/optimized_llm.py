#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
üöÄ SYST√àME LLM MINIMALISTE - PROMPTS G√âN√âRIQUES + SP√âCIALISATION PIPELINE
Bas√© sur l'analyse brillante de l'utilisateur : prompts simples + sp√©cialisation intelligente
"""

import os
import requests
import json
import time
import logging
from typing import Dict, List, Optional, Sequence, Tuple, Any
from pathlib import Path

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def _parse_int_env(name: str, default: int, *, minimum: int = 0) -> int:
    value = os.getenv(name)
    try:
        parsed = int(value) if value is not None else default
    except (TypeError, ValueError):
        parsed = default
    return max(minimum, parsed)


def _parse_float_env(
    name: str,
    default: float,
    *,
    minimum: float = 0.0,
    maximum: Optional[float] = None,
) -> float:
    value = os.getenv(name)
    try:
        parsed = float(value) if value is not None else default
    except (TypeError, ValueError):
        parsed = default
    if maximum is not None:
        parsed = min(maximum, parsed)
    return max(minimum, parsed)


def _parse_stop_tokens_env(name: str, default: Sequence[str]) -> List[str]:
    value = os.getenv(name)
    if not value:
        return list(default)
    try:
        parsed = json.loads(value)
        if isinstance(parsed, (list, tuple)):
            return [str(token) for token in parsed if str(token)]
    except (TypeError, ValueError, json.JSONDecodeError):
        pass
    tokens = [token.strip() for token in value.split("|") if token.strip()]
    if tokens:
        return tokens
    return list(default)


_DEFAULT_STOP_TOKENS: Tuple[str, ...] = ("```", "\n\n\n", "END_OF_CONTEXT", "</json>")

class OptimizedLLM:
    """Syst√®me LLM avec prompts minimalistes et sp√©cialisation via pipeline"""
    
    def __init__(self, base_url: str = "http://localhost:11434", model: str = "gemma3:4b"):
        self.base_url = base_url.rstrip("/")
        self.model = model
        self.timeout = 60  # Timeout plus court pour d√©tecter rapidement les blocages
        self.num_predict = _parse_int_env("PIPELINE_LLM_NUM_PREDICT", 256, minimum=1)
        self.temperature = _parse_float_env("PIPELINE_LLM_TEMP", 0.1, minimum=0.0)
        self.top_p = _parse_float_env("PIPELINE_LLM_TOP_P", 0.9, minimum=0.0, maximum=1.0)
        self.repeat_penalty = _parse_float_env("PIPELINE_LLM_REPEAT_PENALTY", 1.1, minimum=0.0)
        self.stop: List[str] = _parse_stop_tokens_env("PIPELINE_LLM_STOP_TOKENS", _DEFAULT_STOP_TOKENS)

    def configure_generation(
        self,
        *,
        num_predict: Optional[int] = None,
        temperature: Optional[float] = None,
        top_p: Optional[float] = None,
        repeat_penalty: Optional[float] = None,
        stop: Optional[Sequence[str]] = None,
    ) -> None:
        if num_predict is not None:
            try:
                self.num_predict = max(1, int(num_predict))
            except (TypeError, ValueError):
                pass
        if temperature is not None:
            try:
                self.temperature = float(temperature)
            except (TypeError, ValueError):
                pass
        if top_p is not None:
            try:
                self.top_p = float(top_p)
            except (TypeError, ValueError):
                pass
        if repeat_penalty is not None:
            try:
                self.repeat_penalty = float(repeat_penalty)
            except (TypeError, ValueError):
                pass
        if stop is not None:
            try:
                self.stop = [str(token) for token in stop if str(token)]
            except Exception:
                pass

    def _call_llm(
        self,
        prompt: str,
        temperature: float = 0.1,
        max_tokens: int = 100,
        *,
        timeout: Optional[int] = None,
    ) -> Tuple[bool, str, Optional[str]]:
        """Appel LLM simple avec gestion d'erreur"""
        try:
            options = {
                "num_predict": max_tokens if max_tokens is not None else self.num_predict,
                "temperature": temperature if temperature is not None else self.temperature,
                "top_p": self.top_p,
                "repeat_penalty": self.repeat_penalty,
                "stop": self.stop,
            }
            options = {key: value for key, value in options.items() if value is not None}

            payload = {
                "model": self.model,
                "prompt": prompt,
                "stream": False,
                "options": options,
            }

            start_time = time.time()
            response = requests.post(
                f"{self.base_url}/api/generate",
                json=payload,
                timeout=timeout or self.timeout,
            )
            end_time = time.time()

            if response.status_code == 200:
                result = response.json()
                response_text = result.get('response', '').strip()
                duration = end_time - start_time

                logger.info(f"‚úÖ LLM r√©ussi en {duration:.1f}s - {len(response_text)} caract√®res")
                return True, response_text, None
            else:
                logger.error(f"‚ùå Erreur HTTP: {response.status_code}")
                return False, "", "http_error"

        except requests.exceptions.Timeout:
            effective_timeout = timeout or self.timeout
            logger.error(f"‚è±Ô∏è Timeout apr√®s {effective_timeout}s")
            return False, "", "timeout"
        except Exception as e:
            logger.error(f"‚ùå Erreur LLM: {str(e)}")
            return False, "", "exception"
    
    def _extract_json(self, text: str) -> Optional[Dict[str, Any]]:
        """Extraction robuste du JSON depuis la r√©ponse LLM"""
        try:
            # Nettoyer et extraire le JSON
            text = text.strip()
            start_idx = text.find("{")
            end_idx = text.rfind("}")
            
            if start_idx != -1 and end_idx != -1 and end_idx > start_idx:
                json_str = text[start_idx:end_idx + 1]
                return json.loads(json_str)
            else:
                logger.warning("‚ö†Ô∏è Aucun JSON trouv√© dans la r√©ponse")
                return None
                
        except json.JSONDecodeError as e:
            logger.error(f"‚ùå Erreur parsing JSON: {e}")
            return None
    
    def complete(
        self,
        prompt: str,
        *,
        temperature: float = 0.1,
        max_tokens: int = 800,
        timeout: Optional[int] = None,
    ) -> str:
        success, response, err = self._call_llm(
            prompt,
            temperature=temperature,
            max_tokens=max_tokens,
            timeout=timeout,
        )
        if not success:
            raise RuntimeError(f"LLM completion failed: {err or 'unknown'}")
        return response

    def complete_json(
        self,
        prompt: str,
        *,
        temperature: float = 0.1,
        max_tokens: int = 800,
        timeout: Optional[int] = None,
    ) -> Dict[str, Any]:
        response = self.complete(
            prompt,
            temperature=temperature,
            max_tokens=max_tokens,
            timeout=timeout,
        )
        payload = self._extract_json(response)
        if payload is None:
            raise ValueError('LLM response did not contain JSON payload')
        return payload


    def generate_keywords(self, transcript: str, max_keywords: int = 15) -> Tuple[bool, List[str]]:
        """G√©n√©ration de mots-cl√©s avec prompt minimaliste g√©n√©rique"""
        
        # üéØ PROMPT MINIMALISTE (votre approche parfaite)
        prompt = f"""Extract {max_keywords} to {max_keywords + 5} relevant single-word keywords from the transcript.
Do not invent unrelated terms.
Output JSON only: {{"keywords":["word1","word2", "..."]}}

Transcript: {transcript}

JSON:"""
        
        logger.info(f"üéØ G√©n√©ration mots-cl√©s avec prompt minimaliste ({len(prompt)} caract√®res)")

        success, response, _ = self._call_llm(prompt)
        if not success:
            return False, []
        
        # Extraction et validation
        json_data = self._extract_json(response)
        if not json_data:
            return False, []
        
        keywords = json_data.get("keywords", [])
        if not keywords or not isinstance(keywords, list):
            logger.warning("‚ö†Ô∏è Aucun mot-cl√© valide trouv√©")
            return False, []
        
        # Nettoyage et validation
        clean_keywords = []
        seen = set()
        generic = {"what","over","your","look","when","learn","about","thing","things","people","really","going","want","need","make","take","time","back","good","best","more","most","very","that","this","those","these"}
        for kw in keywords:
            if isinstance(kw, str) and kw.strip():
                clean_kw = kw.strip().lower()
                if len(clean_kw) <= 2:
                    continue
                if clean_kw in generic:
                    continue
                if not clean_kw.isalpha():
                    continue
                if clean_kw in seen:
                    continue
                seen.add(clean_kw)
                clean_keywords.append(clean_kw)
        
        # Prioritize specificity by length and uniqueness
        clean_keywords.sort(key=lambda k: (-len(k), k))
        
        logger.info(f"‚úÖ {len(clean_keywords)} mots-cl√©s g√©n√©r√©s avec succ√®s")
        return True, clean_keywords[:max_keywords]
    
    def generate_title_hashtags(self, transcript: str) -> Tuple[bool, Dict[str, Any]]:
        """G√©n√©ration titre + hashtags avec prompt minimaliste"""
        
        # üéØ PROMPT MINIMALISTE pour titre + hashtags
        prompt = f"""Generate a title and hashtags from this transcript.
Output JSON only: {{"title": "Title here", "hashtags": ["#tag1", "#tag2", "..."]}}

Transcript: {transcript}

JSON:"""
        
        logger.info(f"üéØ G√©n√©ration titre + hashtags avec prompt minimaliste ({len(prompt)} caract√®res)")
        
        success, response, _ = self._call_llm(prompt)
        if not success:
            return False, {}
        
        # Extraction et validation
        json_data = self._extract_json(response)
        if not json_data:
            return False, {}
        
        title = json_data.get("title", "").strip()
        hashtags = json_data.get("hashtags", [])
        
        if not title:
            logger.warning("‚ö†Ô∏è Aucun titre valide trouv√©")
            return False, {}
        
        # Nettoyage des hashtags
        clean_hashtags = []
        for tag in hashtags:
            if isinstance(tag, str) and tag.strip():
                clean_tag = tag.strip()
                if not clean_tag.startswith("#"):
                    clean_tag = f"#{clean_tag}"
                clean_hashtags.append(clean_tag)
        
        result = {
            "title": title,
            "hashtags": clean_hashtags
        }
        
        logger.info(f"‚úÖ Titre et {len(clean_hashtags)} hashtags g√©n√©r√©s avec succ√®s")
        return True, result
    
    def generate_complete_metadata(self, transcript: str) -> Tuple[bool, Dict[str, Any]]:
        """G√©n√©ration compl√®te : titre, description, hashtags, mots-cl√©s"""
        
        # üéØ PROMPT MINIMALISTE pour m√©tadonn√©es compl√®tes
        prompt = f"""Generate title, description, hashtags, and keywords from this transcript.
Output JSON only: {{"title": "Title", "description": "Description", "hashtags": ["#tag1"], "keywords": ["word1"]}}

Transcript: {transcript}

JSON:"""
        
        logger.info(f"üéØ G√©n√©ration m√©tadonn√©es compl√®tes avec prompt minimaliste ({len(prompt)} caract√®res)")
        
        success, response, _ = self._call_llm(prompt)
        if not success:
            return False, {}
        
        # Extraction et validation
        json_data = self._extract_json(response)
        if not json_data:
            return False, {}
        
        # Extraction des champs
        title = json_data.get("title", "").strip()
        description = json_data.get("description", "").strip()
        hashtags = json_data.get("hashtags", [])
        keywords = json_data.get("keywords", [])
        
        # Validation des champs obligatoires
        if not title:
            logger.warning("‚ö†Ô∏è Aucun titre valide trouv√©")
            return False, {}
        
        # Nettoyage des hashtags
        clean_hashtags = []
        for tag in hashtags:
            if isinstance(tag, str) and tag.strip():
                clean_tag = tag.strip()
                if not clean_tag.startswith("#"):
                    clean_tag = f"#{clean_tag}"
                clean_hashtags.append(clean_tag)
        
        # Nettoyage des mots-cl√©s
        clean_keywords = []
        for kw in keywords:
            if isinstance(kw, str) and kw.strip():
                clean_kw = kw.strip().lower()
                if len(clean_kw) > 2:
                    clean_keywords.append(clean_kw)
        
        result = {
            "title": title,
            "description": description,
            "hashtags": clean_hashtags,
            "keywords": clean_keywords
        }
        
        logger.info(f"‚úÖ M√©tadonn√©es compl√®tes g√©n√©r√©es : titre, description, {len(clean_hashtags)} hashtags, {len(clean_keywords)} mots-cl√©s")
        return True, result
    
    def generate_broll_keywords_and_queries(self, transcript: str, max_keywords: int = 15) -> Tuple[bool, Dict[str, Any]]:
        """
        üéØ NOUVEAU: G√©n√©ration sp√©cialis√©e pour B-roll
        Produit explicitement broll_keywords + search_queries
        """
        
        # üéØ PROMPT OPTIMIS√â pour B-roll hybride (actions + concepts)
        trimmed = transcript[:1500]
        prompt = f"""Tu es planificatrice B-roll pour un format vertical (TikTok/Shorts, 9:16). √Ä partir du transcript ci-dessous, produis des id√©es de vid√©os libres de droits.

Exigences :
- Analyse le th√®me, l‚Äô√©motion et le rythme : pense en fen√™tres de 3 √† 6 secondes.
- Garde uniquement des id√©es filmables (actions humaines pr√©cises, d√©tails d‚Äôobjet, d√©cors identifiables).
- √âvite les termes creux : people, thing, nice, background, start, generic.
- 60 %% d‚Äôactions humaines (sujet_action_contexte avec underscores) / 40 %% de concepts visuels directs (ex. "brain_scan_monitor").
- Donne pour chaque id√©e une requ√™te courte (2 √† 4 mots) optimis√©e pour les APIs vid√©o.
- Produis aussi un mapping segmentaire facultatif pour faciliter la synchro.

R√©ponds uniquement en JSON :
{{
  "detected_domain": "...",
  "context": "r√©sum√© en 12 mots max",
  "broll_keywords": ["..."],
  "search_queries": ["..."],
  "segment_briefs": [
    {{"segment_index": 0, "suggested_window_s": 4, "keywords": ["action_pr√©cise", "d√©tail_visuel"]}}
  ]
}}

Transcript (tronqu√©) : {trimmed}
JSON:"""

        logger.info(f"üéØ G√©n√©ration B-roll avec prompt minimaliste ({len(prompt)} caract√®res)")

        success, response, error_kind = self._call_llm(prompt, max_tokens=350)
        if not success and error_kind == "timeout":
            shorter = trimmed[:600]
            retry_prompt = prompt.replace(trimmed, shorter)
            logger.info("‚è±Ô∏è Retentative LLM B-roll avec transcript raccourci")
            success, response, error_kind = self._call_llm(retry_prompt, max_tokens=200, timeout=40)
        if not success:
            return False, {}
        
        # Extraction et validation
        json_data = self._extract_json(response)
        if not json_data:
            return False, {}
        
        # Extraction des champs enrichis
        domain = json_data.get("domain", "").strip()
        context = json_data.get("context", "").strip() 
        broll_keywords = json_data.get("broll_keywords", [])
        search_queries = json_data.get("search_queries", [])
        
        # Validation des champs
        if not broll_keywords or not search_queries:
            logger.warning("‚ö†Ô∏è Champs B-roll manquants dans la r√©ponse")
            return False, {}
        
        # Nettoyage des mots-cl√©s B-roll
        clean_broll_keywords = []
        for kw in broll_keywords:
            if isinstance(kw, str) and kw.strip():
                clean_kw = kw.strip().lower()
                if len(clean_kw) > 2:
                    clean_broll_keywords.append(clean_kw)
        
        # Nettoyage des requ√™tes de recherche
        clean_search_queries = []
        for query in search_queries:
            if isinstance(query, str) and query.strip():
                clean_query = query.strip()
                if len(clean_query) <= 30:  # Augment√© pour phrases plus descriptives
                    clean_search_queries.append(clean_query)
        
        result = {
            "domain": domain,
            "context": context,
            "broll_keywords": clean_broll_keywords[:max_keywords],
            "search_queries": clean_search_queries[:max_keywords]
        }
        
        logger.info(f"‚úÖ B-roll g√©n√©r√© : {len(clean_broll_keywords)} mots-cl√©s, {len(clean_search_queries)} requ√™tes")
        return True, result
    
    def generate_metadata_with_broll(self, transcript: str) -> Tuple[bool, Dict[str, Any]]:
        """
        üéØ NOUVEAU: G√©n√©ration compl√®te avec m√©tadonn√©es + B-roll
        Combine toutes les informations n√©cessaires
        """
        
        # üéØ PROMPT VIRAL pour m√©tadonn√©es + B-roll
        prompt = f"""Tu es copywriter growth pour vid√©os verticales (TikTok/Shorts).

Objectif : g√©n√©rer un TITRE + DESCRIPTION qui stoppent le scroll et maximisent la r√©tention.

Contraintes :
- Titre : 60 √† 70 caract√®res, commence par un hook (verbe d‚Äôaction, question ou chiffre) et annonce le b√©n√©fice principal.
- Description : 3 phrases max. Phrase 1 = b√©n√©fice concret; Phrase 2 = preuve/tip actionnable; Phrase 3 = CTA soft (ex. "Sauvegarde ce clip"). Total ‚â§ 220 caract√®res.
- Ajoute 4 √† 6 hashtags pertinents (mix niche + large, sans doublon).
- Fournis 6 mots-cl√©s SEO en snake_case et 3 requ√™tes B-roll optimis√©es pour des banques vid√©o.
- Ton positif, pas de clickbait vide, pas de MAJUSCULES abusives.

R√©ponds uniquement en JSON :
{{
    "title": "...",
    "description": "...",
    "hashtags": ["#..."],
    "keywords": ["mot_clef"],
    "broll_keywords": ["visual_word"],
    "search_queries": ["requ√™te vid√©o"]
}}

Transcript : {transcript}
JSON:"""
        
        logger.info(f"üéØ G√©n√©ration compl√®te avec B-roll ({len(prompt)} caract√®res)")
        
        success, response, _ = self._call_llm(prompt)
        if not success:
            return False, {}
        
        # Extraction et validation
        json_data = self._extract_json(response)
        if not json_data:
            return False, {}
        
        # Extraction de tous les champs
        title = json_data.get("title", "").strip()
        description = json_data.get("description", "").strip()
        hashtags = json_data.get("hashtags", [])
        keywords = json_data.get("keywords", [])
        broll_keywords = json_data.get("broll_keywords", [])
        search_queries = json_data.get("search_queries", [])
        
        # Validation des champs obligatoires
        if not title:
            logger.warning("‚ö†Ô∏è Aucun titre valide trouv√©")
            return False, {}
        
        # Nettoyage des hashtags
        clean_hashtags = []
        for tag in hashtags:
            if isinstance(tag, str) and tag.strip():
                clean_tag = tag.strip()
                if not clean_tag.startswith("#"):
                    clean_tag = f"#{clean_tag}"
                clean_hashtags.append(clean_tag)
        
        # Nettoyage des mots-cl√©s
        clean_keywords = []
        for kw in keywords:
            if isinstance(kw, str) and kw.strip():
                clean_kw = kw.strip().lower()
                if len(clean_kw) > 2:
                    clean_keywords.append(clean_kw)
        
        # Nettoyage des mots-cl√©s B-roll
        clean_broll_keywords = []
        for kw in broll_keywords:
            if isinstance(kw, str) and kw.strip():
                clean_kw = kw.strip().lower()
                if len(clean_kw) > 2:
                    clean_broll_keywords.append(clean_kw)
        
        # Nettoyage des requ√™tes de recherche
        clean_search_queries = []
        for query in search_queries:
            if isinstance(query, str) and query.strip():
                clean_query = query.strip()
                if len(clean_query) <= 25:
                    clean_search_queries.append(clean_query)
        
        result = {
            "title": title,
            "description": description,
            "hashtags": clean_hashtags,
            "keywords": clean_keywords,
            "broll_keywords": clean_broll_keywords,
            "search_queries": clean_search_queries
        }
        
        logger.info(f"‚úÖ M√©tadonn√©es compl√®tes avec B-roll : titre, description, {len(clean_hashtags)} hashtags, {len(clean_keywords)} mots-cl√©s, {len(clean_broll_keywords)} B-roll, {len(clean_search_queries)} requ√™tes")
        return True, result

# === FONCTIONS UTILITAIRES POUR L'INT√âGRATION ===

def create_optimized_llm(base_url: str = None, model: str = None) -> OptimizedLLM:
    """Factory pour cr√©er une instance LLM optimis√©e"""
    
    # D√©tection automatique de l'URL et du mod√®le
    if not base_url:
        # Essayer Ollama en premier
        try:
            response = requests.get("http://localhost:11434/api/tags", timeout=5)
            if response.status_code == 200:
                base_url = "http://localhost:11434"
                logger.info("‚úÖ Ollama d√©tect√© sur localhost:11434")
            else:
                base_url = "http://localhost:1234"  # LM Studio par d√©faut
                logger.info("‚ö†Ô∏è Ollama non disponible, utilisation LM Studio par d√©faut")
        except:
            base_url = "http://localhost:1234"
            logger.info("‚ö†Ô∏è Aucun LLM local d√©tect√©, utilisation LM Studio par d√©faut")
    
    if not model:
        # Mod√®le par d√©faut selon la disponibilit√©
        if "11434" in base_url:  # Ollama
            model = "gemma3:4b"  # Mod√®le recommand√©
        else:  # LM Studio
            model = "default"
    
    return OptimizedLLM(base_url, model)

def generate_keywords_for_pipeline(transcript: str, max_keywords: int = 15) -> Tuple[bool, List[str]]:
    """Fonction utilitaire pour int√©gration directe dans le pipeline"""
    llm = create_optimized_llm()
    return llm.generate_keywords(transcript, max_keywords)

def generate_metadata_for_pipeline(transcript: str) -> Tuple[bool, Dict[str, Any]]:
    """Fonction utilitaire pour int√©gration directe dans le pipeline"""
    llm = create_optimized_llm()
    return llm.generate_complete_metadata(transcript)

def generate_broll_for_pipeline(transcript: str, max_keywords: int = 15) -> Tuple[bool, Dict[str, Any]]:
    """üéØ NOUVEAU: Fonction utilitaire pour B-roll"""
    llm = create_optimized_llm()
    return llm.generate_broll_keywords_and_queries(transcript, max_keywords)

def generate_complete_with_broll(transcript: str) -> Tuple[bool, Dict[str, Any]]:
    """üéØ NOUVEAU: Fonction utilitaire pour m√©tadonn√©es compl√®tes avec B-roll"""
    llm = create_optimized_llm()
    return llm.generate_metadata_with_broll(transcript)

# === TEST RAPIDE ===
if __name__ == "__main__":
    print("üß† Test du syst√®me LLM optimis√©...")
    
    # Test avec un transcript simple
    test_transcript = "EMDR therapy utilizes bilateral stimulation to process traumatic memories. The therapist guides the patient through eye movements while recalling distressing events."
    
    llm = create_optimized_llm()
    
    # Test mots-cl√©s
    print("\nüéØ Test g√©n√©ration mots-cl√©s...")
    success, keywords = llm.generate_keywords(test_transcript, 10)
    if success:
        print(f"‚úÖ Mots-cl√©s g√©n√©r√©s: {keywords}")
    else:
        print("‚ùå √âchec g√©n√©ration mots-cl√©s")
    
    # Test m√©tadonn√©es compl√®tes
    print("\nüéØ Test g√©n√©ration m√©tadonn√©es compl√®tes...")
    success, metadata = llm.generate_complete_metadata(test_transcript)
    if success:
        print(f"‚úÖ M√©tadonn√©es g√©n√©r√©es:")
        for key, value in metadata.items():
            print(f"   {key}: {value}")
    else:
        print("‚ùå √âchec g√©n√©ration m√©tadonn√©es")
    
    # üéØ NOUVEAU: Test B-roll
    print("\nüéØ Test g√©n√©ration B-roll...")
    success, broll_data = llm.generate_broll_keywords_and_queries(test_transcript, 8)
    if success:
        print(f"‚úÖ B-roll g√©n√©r√©:")
        print(f"   Mots-cl√©s: {broll_data['broll_keywords']}")
        print(f"   Requ√™tes: {broll_data['search_queries']}")
    else:
        print("‚ùå √âchec g√©n√©ration B-roll")
    
    # üéØ NOUVEAU: Test complet avec B-roll
    print("\nüéØ Test g√©n√©ration compl√®te avec B-roll...")
    success, complete_data = llm.generate_metadata_with_broll(test_transcript)
    if success:
        print(f"‚úÖ Donn√©es compl√®tes g√©n√©r√©es:")
        for key, value in complete_data.items():
            print(f"   {key}: {value}")
    else:
        print("‚ùå √âchec g√©n√©ration compl√®te") 

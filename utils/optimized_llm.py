#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸš€ SYSTÃˆME LLM MINIMALISTE - PROMPTS GÃ‰NÃ‰RIQUES + SPÃ‰CIALISATION PIPELINE
BasÃ© sur l'analyse brillante de l'utilisateur : prompts simples + spÃ©cialisation intelligente
"""

import requests
import json
import time
import logging
from typing import Dict, List, Optional, Tuple, Any
from pathlib import Path

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class OptimizedLLM:
    """SystÃ¨me LLM avec prompts minimalistes et spÃ©cialisation via pipeline"""
    
    def __init__(self, base_url: str = "http://localhost:11434", model: str = "gemma3:4b"):
        self.base_url = base_url.rstrip("/")
        self.model = model
        self.timeout = 60  # Timeout plus court pour dÃ©tecter rapidement les blocages

    def _call_llm(
        self,
        prompt: str,
        temperature: float = 0.1,
        max_tokens: int = 100,
        *,
        timeout: Optional[int] = None,
    ) -> Tuple[bool, str, Optional[str]]:
        """Appel LLM simple avec gestion d'erreur"""
        try:
            payload = {
                "model": self.model,
                "prompt": prompt,
                "temperature": temperature,
                "stream": False,
                "max_tokens": max_tokens,
            }

            start_time = time.time()
            response = requests.post(
                f"{self.base_url}/api/generate",
                json=payload,
                timeout=timeout or self.timeout,
            )
            end_time = time.time()

            if response.status_code == 200:
                result = response.json()
                response_text = result.get('response', '').strip()
                duration = end_time - start_time

                logger.info(f"âœ… LLM rÃ©ussi en {duration:.1f}s - {len(response_text)} caractÃ¨res")
                return True, response_text, None
            else:
                logger.error(f"âŒ Erreur HTTP: {response.status_code}")
                return False, "", "http_error"

        except requests.exceptions.Timeout:
            effective_timeout = timeout or self.timeout
            logger.error(f"â±ï¸ Timeout aprÃ¨s {effective_timeout}s")
            return False, "", "timeout"
        except Exception as e:
            logger.error(f"âŒ Erreur LLM: {str(e)}")
            return False, "", "exception"
    
    def _extract_json(self, text: str) -> Optional[Dict[str, Any]]:
        """Extraction robuste du JSON depuis la rÃ©ponse LLM"""
        try:
            # Nettoyer et extraire le JSON
            text = text.strip()
            start_idx = text.find("{")
            end_idx = text.rfind("}")
            
            if start_idx != -1 and end_idx != -1 and end_idx > start_idx:
                json_str = text[start_idx:end_idx + 1]
                return json.loads(json_str)
            else:
                logger.warning("âš ï¸ Aucun JSON trouvÃ© dans la rÃ©ponse")
                return None
                
        except json.JSONDecodeError as e:
            logger.error(f"âŒ Erreur parsing JSON: {e}")
            return None
    
    def complete(
        self,
        prompt: str,
        *,
        temperature: float = 0.1,
        max_tokens: int = 800,
        timeout: Optional[int] = None,
    ) -> str:
        success, response, err = self._call_llm(
            prompt,
            temperature=temperature,
            max_tokens=max_tokens,
            timeout=timeout,
        )
        if not success:
            raise RuntimeError(f"LLM completion failed: {err or 'unknown'}")
        return response

    def complete_json(
        self,
        prompt: str,
        *,
        temperature: float = 0.1,
        max_tokens: int = 800,
        timeout: Optional[int] = None,
    ) -> Dict[str, Any]:
        response = self.complete(
            prompt,
            temperature=temperature,
            max_tokens=max_tokens,
            timeout=timeout,
        )
        payload = self._extract_json(response)
        if payload is None:
            raise ValueError('LLM response did not contain JSON payload')
        return payload


    def generate_keywords(self, transcript: str, max_keywords: int = 15) -> Tuple[bool, List[str]]:
        """GÃ©nÃ©ration de mots-clÃ©s avec prompt minimaliste gÃ©nÃ©rique"""
        
        # ğŸ¯ PROMPT MINIMALISTE (votre approche parfaite)
        prompt = f"""Extract {max_keywords} to {max_keywords + 5} relevant single-word keywords from the transcript.
Do not invent unrelated terms.
Output JSON only: {{"keywords":["word1","word2", "..."]}}

Transcript: {transcript}

JSON:"""
        
        logger.info(f"ğŸ¯ GÃ©nÃ©ration mots-clÃ©s avec prompt minimaliste ({len(prompt)} caractÃ¨res)")

        success, response, _ = self._call_llm(prompt)
        if not success:
            return False, []
        
        # Extraction et validation
        json_data = self._extract_json(response)
        if not json_data:
            return False, []
        
        keywords = json_data.get("keywords", [])
        if not keywords or not isinstance(keywords, list):
            logger.warning("âš ï¸ Aucun mot-clÃ© valide trouvÃ©")
            return False, []
        
        # Nettoyage et validation
        clean_keywords = []
        seen = set()
        generic = {"what","over","your","look","when","learn","about","thing","things","people","really","going","want","need","make","take","time","back","good","best","more","most","very","that","this","those","these"}
        for kw in keywords:
            if isinstance(kw, str) and kw.strip():
                clean_kw = kw.strip().lower()
                if len(clean_kw) <= 2:
                    continue
                if clean_kw in generic:
                    continue
                if not clean_kw.isalpha():
                    continue
                if clean_kw in seen:
                    continue
                seen.add(clean_kw)
                clean_keywords.append(clean_kw)
        
        # Prioritize specificity by length and uniqueness
        clean_keywords.sort(key=lambda k: (-len(k), k))
        
        logger.info(f"âœ… {len(clean_keywords)} mots-clÃ©s gÃ©nÃ©rÃ©s avec succÃ¨s")
        return True, clean_keywords[:max_keywords]
    
    def generate_title_hashtags(self, transcript: str) -> Tuple[bool, Dict[str, Any]]:
        """GÃ©nÃ©ration titre + hashtags avec prompt minimaliste"""
        
        # ğŸ¯ PROMPT MINIMALISTE pour titre + hashtags
        prompt = f"""Generate a title and hashtags from this transcript.
Output JSON only: {{"title": "Title here", "hashtags": ["#tag1", "#tag2", "..."]}}

Transcript: {transcript}

JSON:"""
        
        logger.info(f"ğŸ¯ GÃ©nÃ©ration titre + hashtags avec prompt minimaliste ({len(prompt)} caractÃ¨res)")
        
        success, response, _ = self._call_llm(prompt)
        if not success:
            return False, {}
        
        # Extraction et validation
        json_data = self._extract_json(response)
        if not json_data:
            return False, {}
        
        title = json_data.get("title", "").strip()
        hashtags = json_data.get("hashtags", [])
        
        if not title:
            logger.warning("âš ï¸ Aucun titre valide trouvÃ©")
            return False, {}
        
        # Nettoyage des hashtags
        clean_hashtags = []
        for tag in hashtags:
            if isinstance(tag, str) and tag.strip():
                clean_tag = tag.strip()
                if not clean_tag.startswith("#"):
                    clean_tag = f"#{clean_tag}"
                clean_hashtags.append(clean_tag)
        
        result = {
            "title": title,
            "hashtags": clean_hashtags
        }
        
        logger.info(f"âœ… Titre et {len(clean_hashtags)} hashtags gÃ©nÃ©rÃ©s avec succÃ¨s")
        return True, result
    
    def generate_complete_metadata(self, transcript: str) -> Tuple[bool, Dict[str, Any]]:
        """GÃ©nÃ©ration complÃ¨te : titre, description, hashtags, mots-clÃ©s"""
        
        # ğŸ¯ PROMPT MINIMALISTE pour mÃ©tadonnÃ©es complÃ¨tes
        prompt = f"""Generate title, description, hashtags, and keywords from this transcript.
Output JSON only: {{"title": "Title", "description": "Description", "hashtags": ["#tag1"], "keywords": ["word1"]}}

Transcript: {transcript}

JSON:"""
        
        logger.info(f"ğŸ¯ GÃ©nÃ©ration mÃ©tadonnÃ©es complÃ¨tes avec prompt minimaliste ({len(prompt)} caractÃ¨res)")
        
        success, response, _ = self._call_llm(prompt)
        if not success:
            return False, {}
        
        # Extraction et validation
        json_data = self._extract_json(response)
        if not json_data:
            return False, {}
        
        # Extraction des champs
        title = json_data.get("title", "").strip()
        description = json_data.get("description", "").strip()
        hashtags = json_data.get("hashtags", [])
        keywords = json_data.get("keywords", [])
        
        # Validation des champs obligatoires
        if not title:
            logger.warning("âš ï¸ Aucun titre valide trouvÃ©")
            return False, {}
        
        # Nettoyage des hashtags
        clean_hashtags = []
        for tag in hashtags:
            if isinstance(tag, str) and tag.strip():
                clean_tag = tag.strip()
                if not clean_tag.startswith("#"):
                    clean_tag = f"#{clean_tag}"
                clean_hashtags.append(clean_tag)
        
        # Nettoyage des mots-clÃ©s
        clean_keywords = []
        for kw in keywords:
            if isinstance(kw, str) and kw.strip():
                clean_kw = kw.strip().lower()
                if len(clean_kw) > 2:
                    clean_keywords.append(clean_kw)
        
        result = {
            "title": title,
            "description": description,
            "hashtags": clean_hashtags,
            "keywords": clean_keywords
        }
        
        logger.info(f"âœ… MÃ©tadonnÃ©es complÃ¨tes gÃ©nÃ©rÃ©es : titre, description, {len(clean_hashtags)} hashtags, {len(clean_keywords)} mots-clÃ©s")
        return True, result
    
    def generate_broll_keywords_and_queries(self, transcript: str, max_keywords: int = 15) -> Tuple[bool, Dict[str, Any]]:
        """
        ğŸ¯ NOUVEAU: GÃ©nÃ©ration spÃ©cialisÃ©e pour B-roll
        Produit explicitement broll_keywords + search_queries
        """
        
        # ğŸ¯ PROMPT OPTIMISÃ‰ pour B-roll hybride (actions + concepts)
        trimmed = transcript[:1500]
        prompt = f"""Tu es planificatrice B-roll pour un format vertical (TikTok/Shorts, 9:16). Ã€ partir du transcript ci-dessous, produis des idÃ©es de vidÃ©os libres de droits.

Exigences :
- Analyse le thÃ¨me, lâ€™Ã©motion et le rythme : pense en fenÃªtres de 3 Ã  6 secondes.
- Garde uniquement des idÃ©es filmables (actions humaines prÃ©cises, dÃ©tails dâ€™objet, dÃ©cors identifiables).
- Ã‰vite les termes creux : people, thing, nice, background, start, generic.
- 60 %% dâ€™actions humaines (sujet_action_contexte avec underscores) / 40 %% de concepts visuels directs (ex. "brain_scan_monitor").
- Donne pour chaque idÃ©e une requÃªte courte (2 Ã  4 mots) optimisÃ©e pour les APIs vidÃ©o.
- Produis aussi un mapping segmentaire facultatif pour faciliter la synchro.

RÃ©ponds uniquement en JSON :
{{
  "detected_domain": "...",
  "context": "rÃ©sumÃ© en 12 mots max",
  "broll_keywords": ["..."],
  "search_queries": ["..."],
  "segment_briefs": [
    {{"segment_index": 0, "suggested_window_s": 4, "keywords": ["action_prÃ©cise", "dÃ©tail_visuel"]}}
  ]
}}

Transcript (tronquÃ©) : {trimmed}
JSON:"""

        logger.info(f"ğŸ¯ GÃ©nÃ©ration B-roll avec prompt minimaliste ({len(prompt)} caractÃ¨res)")

        success, response, error_kind = self._call_llm(prompt, max_tokens=350)
        if not success and error_kind == "timeout":
            shorter = trimmed[:600]
            retry_prompt = prompt.replace(trimmed, shorter)
            logger.info("â±ï¸ Retentative LLM B-roll avec transcript raccourci")
            success, response, error_kind = self._call_llm(retry_prompt, max_tokens=200, timeout=40)
        if not success:
            return False, {}
        
        # Extraction et validation
        json_data = self._extract_json(response)
        if not json_data:
            return False, {}
        
        # Extraction des champs enrichis
        domain = json_data.get("domain", "").strip()
        context = json_data.get("context", "").strip() 
        broll_keywords = json_data.get("broll_keywords", [])
        search_queries = json_data.get("search_queries", [])
        
        # Validation des champs
        if not broll_keywords or not search_queries:
            logger.warning("âš ï¸ Champs B-roll manquants dans la rÃ©ponse")
            return False, {}
        
        # Nettoyage des mots-clÃ©s B-roll
        clean_broll_keywords = []
        for kw in broll_keywords:
            if isinstance(kw, str) and kw.strip():
                clean_kw = kw.strip().lower()
                if len(clean_kw) > 2:
                    clean_broll_keywords.append(clean_kw)
        
        # Nettoyage des requÃªtes de recherche
        clean_search_queries = []
        for query in search_queries:
            if isinstance(query, str) and query.strip():
                clean_query = query.strip()
                if len(clean_query) <= 30:  # AugmentÃ© pour phrases plus descriptives
                    clean_search_queries.append(clean_query)
        
        result = {
            "domain": domain,
            "context": context,
            "broll_keywords": clean_broll_keywords[:max_keywords],
            "search_queries": clean_search_queries[:max_keywords]
        }
        
        logger.info(f"âœ… B-roll gÃ©nÃ©rÃ© : {len(clean_broll_keywords)} mots-clÃ©s, {len(clean_search_queries)} requÃªtes")
        return True, result
    
    def generate_metadata_with_broll(self, transcript: str) -> Tuple[bool, Dict[str, Any]]:
        """
        ğŸ¯ NOUVEAU: GÃ©nÃ©ration complÃ¨te avec mÃ©tadonnÃ©es + B-roll
        Combine toutes les informations nÃ©cessaires
        """
        
        # ğŸ¯ PROMPT VIRAL pour mÃ©tadonnÃ©es + B-roll
        prompt = f"""Tu es copywriter growth pour vidÃ©os verticales (TikTok/Shorts).

Objectif : gÃ©nÃ©rer un TITRE + DESCRIPTION qui stoppent le scroll et maximisent la rÃ©tention.

Contraintes :
- Titre : 60 Ã  70 caractÃ¨res, commence par un hook (verbe dâ€™action, question ou chiffre) et annonce le bÃ©nÃ©fice principal.
- Description : 3 phrases max. Phrase 1 = bÃ©nÃ©fice concret; Phrase 2 = preuve/tip actionnable; Phrase 3 = CTA soft (ex. "Sauvegarde ce clip"). Total â‰¤ 220 caractÃ¨res.
- Ajoute 4 Ã  6 hashtags pertinents (mix niche + large, sans doublon).
- Fournis 6 mots-clÃ©s SEO en snake_case et 3 requÃªtes B-roll optimisÃ©es pour des banques vidÃ©o.
- Ton positif, pas de clickbait vide, pas de MAJUSCULES abusives.

RÃ©ponds uniquement en JSON :
{{
    "title": "...",
    "description": "...",
    "hashtags": ["#..."],
    "keywords": ["mot_clef"],
    "broll_keywords": ["visual_word"],
    "search_queries": ["requÃªte vidÃ©o"]
}}

Transcript : {transcript}
JSON:"""
        
        logger.info(f"ğŸ¯ GÃ©nÃ©ration complÃ¨te avec B-roll ({len(prompt)} caractÃ¨res)")
        
        success, response, _ = self._call_llm(prompt)
        if not success:
            return False, {}
        
        # Extraction et validation
        json_data = self._extract_json(response)
        if not json_data:
            return False, {}
        
        # Extraction de tous les champs
        title = json_data.get("title", "").strip()
        description = json_data.get("description", "").strip()
        hashtags = json_data.get("hashtags", [])
        keywords = json_data.get("keywords", [])
        broll_keywords = json_data.get("broll_keywords", [])
        search_queries = json_data.get("search_queries", [])
        
        # Validation des champs obligatoires
        if not title:
            logger.warning("âš ï¸ Aucun titre valide trouvÃ©")
            return False, {}
        
        # Nettoyage des hashtags
        clean_hashtags = []
        for tag in hashtags:
            if isinstance(tag, str) and tag.strip():
                clean_tag = tag.strip()
                if not clean_tag.startswith("#"):
                    clean_tag = f"#{clean_tag}"
                clean_hashtags.append(clean_tag)
        
        # Nettoyage des mots-clÃ©s
        clean_keywords = []
        for kw in keywords:
            if isinstance(kw, str) and kw.strip():
                clean_kw = kw.strip().lower()
                if len(clean_kw) > 2:
                    clean_keywords.append(clean_kw)
        
        # Nettoyage des mots-clÃ©s B-roll
        clean_broll_keywords = []
        for kw in broll_keywords:
            if isinstance(kw, str) and kw.strip():
                clean_kw = kw.strip().lower()
                if len(clean_kw) > 2:
                    clean_broll_keywords.append(clean_kw)
        
        # Nettoyage des requÃªtes de recherche
        clean_search_queries = []
        for query in search_queries:
            if isinstance(query, str) and query.strip():
                clean_query = query.strip()
                if len(clean_query) <= 25:
                    clean_search_queries.append(clean_query)
        
        result = {
            "title": title,
            "description": description,
            "hashtags": clean_hashtags,
            "keywords": clean_keywords,
            "broll_keywords": clean_broll_keywords,
            "search_queries": clean_search_queries
        }
        
        logger.info(f"âœ… MÃ©tadonnÃ©es complÃ¨tes avec B-roll : titre, description, {len(clean_hashtags)} hashtags, {len(clean_keywords)} mots-clÃ©s, {len(clean_broll_keywords)} B-roll, {len(clean_search_queries)} requÃªtes")
        return True, result

# === FONCTIONS UTILITAIRES POUR L'INTÃ‰GRATION ===

def create_optimized_llm(base_url: str = None, model: str = None) -> OptimizedLLM:
    """Factory pour crÃ©er une instance LLM optimisÃ©e"""
    
    # DÃ©tection automatique de l'URL et du modÃ¨le
    if not base_url:
        # Essayer Ollama en premier
        try:
            response = requests.get("http://localhost:11434/api/tags", timeout=5)
            if response.status_code == 200:
                base_url = "http://localhost:11434"
                logger.info("âœ… Ollama dÃ©tectÃ© sur localhost:11434")
            else:
                base_url = "http://localhost:1234"  # LM Studio par dÃ©faut
                logger.info("âš ï¸ Ollama non disponible, utilisation LM Studio par dÃ©faut")
        except:
            base_url = "http://localhost:1234"
            logger.info("âš ï¸ Aucun LLM local dÃ©tectÃ©, utilisation LM Studio par dÃ©faut")
    
    if not model:
        # ModÃ¨le par dÃ©faut selon la disponibilitÃ©
        if "11434" in base_url:  # Ollama
            model = "gemma3:4b"  # ModÃ¨le recommandÃ©
        else:  # LM Studio
            model = "default"
    
    return OptimizedLLM(base_url, model)

def generate_keywords_for_pipeline(transcript: str, max_keywords: int = 15) -> Tuple[bool, List[str]]:
    """Fonction utilitaire pour intÃ©gration directe dans le pipeline"""
    llm = create_optimized_llm()
    return llm.generate_keywords(transcript, max_keywords)

def generate_metadata_for_pipeline(transcript: str) -> Tuple[bool, Dict[str, Any]]:
    """Fonction utilitaire pour intÃ©gration directe dans le pipeline"""
    llm = create_optimized_llm()
    return llm.generate_complete_metadata(transcript)

def generate_broll_for_pipeline(transcript: str, max_keywords: int = 15) -> Tuple[bool, Dict[str, Any]]:
    """ğŸ¯ NOUVEAU: Fonction utilitaire pour B-roll"""
    llm = create_optimized_llm()
    return llm.generate_broll_keywords_and_queries(transcript, max_keywords)

def generate_complete_with_broll(transcript: str) -> Tuple[bool, Dict[str, Any]]:
    """ğŸ¯ NOUVEAU: Fonction utilitaire pour mÃ©tadonnÃ©es complÃ¨tes avec B-roll"""
    llm = create_optimized_llm()
    return llm.generate_metadata_with_broll(transcript)

# === TEST RAPIDE ===
if __name__ == "__main__":
    print("ğŸ§  Test du systÃ¨me LLM optimisÃ©...")
    
    # Test avec un transcript simple
    test_transcript = "EMDR therapy utilizes bilateral stimulation to process traumatic memories. The therapist guides the patient through eye movements while recalling distressing events."
    
    llm = create_optimized_llm()
    
    # Test mots-clÃ©s
    print("\nğŸ¯ Test gÃ©nÃ©ration mots-clÃ©s...")
    success, keywords = llm.generate_keywords(test_transcript, 10)
    if success:
        print(f"âœ… Mots-clÃ©s gÃ©nÃ©rÃ©s: {keywords}")
    else:
        print("âŒ Ã‰chec gÃ©nÃ©ration mots-clÃ©s")
    
    # Test mÃ©tadonnÃ©es complÃ¨tes
    print("\nğŸ¯ Test gÃ©nÃ©ration mÃ©tadonnÃ©es complÃ¨tes...")
    success, metadata = llm.generate_complete_metadata(test_transcript)
    if success:
        print(f"âœ… MÃ©tadonnÃ©es gÃ©nÃ©rÃ©es:")
        for key, value in metadata.items():
            print(f"   {key}: {value}")
    else:
        print("âŒ Ã‰chec gÃ©nÃ©ration mÃ©tadonnÃ©es")
    
    # ğŸ¯ NOUVEAU: Test B-roll
    print("\nğŸ¯ Test gÃ©nÃ©ration B-roll...")
    success, broll_data = llm.generate_broll_keywords_and_queries(test_transcript, 8)
    if success:
        print(f"âœ… B-roll gÃ©nÃ©rÃ©:")
        print(f"   Mots-clÃ©s: {broll_data['broll_keywords']}")
        print(f"   RequÃªtes: {broll_data['search_queries']}")
    else:
        print("âŒ Ã‰chec gÃ©nÃ©ration B-roll")
    
    # ğŸ¯ NOUVEAU: Test complet avec B-roll
    print("\nğŸ¯ Test gÃ©nÃ©ration complÃ¨te avec B-roll...")
    success, complete_data = llm.generate_metadata_with_broll(test_transcript)
    if success:
        print(f"âœ… DonnÃ©es complÃ¨tes gÃ©nÃ©rÃ©es:")
        for key, value in complete_data.items():
            print(f"   {key}: {value}")
    else:
        print("âŒ Ã‰chec gÃ©nÃ©ration complÃ¨te") 

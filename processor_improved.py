# processor_improved.py - Version am√©lior√©e avec vos modifications

import os
import json
import subprocess
import logging
from pathlib import Path
from typing import List, Dict, Optional
import whisper
import requests
from moviepy import VideoFileClip, TextClip, CompositeVideoClip
import numpy as np
import cv2
import mediapipe as mp
import random
import time
from tiktok_subtitles import add_tiktok_subtitles


logger = logging.getLogger(__name__)

class Config:
    """Configuration centralis√©e du pipeline"""
    CLIPS_FOLDER = Path("./clips")
    OUTPUT_FOLDER = Path("./output") 
    TEMP_FOLDER = Path("./temp")
    
    # R√©solution cible pour les r√©seaux sociaux
    TARGET_WIDTH = 720
    TARGET_HEIGHT = 1280  # Format 9:16
    
    # Param√®tres Whisper
    WHISPER_MODEL = "tiny"
    
    # Param√®tres sous-titres am√©lior√©s
    SUBTITLE_FONT_SIZE = 70
    SUBTITLE_FONT = 'Segoe UI'  # Police compatible emoji (au lieu d'Impact)
    SUBTITLE_STROKE_WIDTH = 3

class VideoProcessorAI:
    """Processor vid√©o avec IA am√©lior√©e"""
    
    def __init__(self):
        self.whisper_model = whisper.load_model(Config.WHISPER_MODEL)
        self._setup_directories()
        
        # Initialisation MediaPipe
        self.mp_pose = mp.solutions.pose
        self.mp_face = mp.solutions.face_detection
        
        # Cache pour √©viter de recalculer les d√©tections
        self.detection_cache = {}
    
    def _setup_directories(self):
        """Cr√©e les dossiers n√©cessaires"""
        for folder in [Config.CLIPS_FOLDER, Config.OUTPUT_FOLDER, Config.TEMP_FOLDER]:
            folder.mkdir(exist_ok=True)
    
    def reframe_to_vertical(self, clip_path: Path) -> Path:
        """
        Reframe dynamique bas√© sur la d√©tection IA (MediaPipe)
        Version am√©lior√©e avec d√©tection de visage en fallback
        """
        logger.info("üéØ Reframe dynamique avec IA (MediaPipe)")
        
        video = VideoFileClip(str(clip_path))
        fps = int(video.fps)
        duration = video.duration
        
        # D√©tection des centres d'int√©r√™t
        x_centers = self._detect_focus_points(video, fps, duration)
        
        # Lissage avanc√© avec fen√™tre adaptative
        x_centers_smooth = self._smooth_trajectory(x_centers, window_size=min(8, len(x_centers)//6))
        
        # Application du reframe dynamique
        def crop_frame(get_frame, t):
            frame = get_frame(t)
            h, w, _ = frame.shape
            
            # Index temporel
            frame_idx = int(t * fps)
            frame_idx = min(frame_idx, len(x_centers_smooth) - 1)
            
            x_center = x_centers_smooth[frame_idx] * w
            
            # Calcul du crop avec ratio 9:16
            crop_width = int(Config.TARGET_WIDTH * h / Config.TARGET_HEIGHT)
            crop_width = min(crop_width, w)  # S√©curit√©
            
            # Centrage avec limites
            x1 = int(max(0, min(w - crop_width, x_center - crop_width / 2)))
            x2 = x1 + crop_width
            
            cropped = frame[:, x1:x2]
            
            # Redimensionnement avec anti-aliasing
            resized = cv2.resize(cropped, (Config.TARGET_WIDTH, Config.TARGET_HEIGHT), 
                               interpolation=cv2.INTER_LANCZOS4)
            
            return resized
        
        reframed = video.fl_image(crop_frame)
        
        output_path = Config.TEMP_FOLDER / f"reframed_{clip_path.name}"
        reframed.write_videofile(
            str(output_path), 
            codec='h264_nvenc', 
            audio_codec='aac', 
            verbose=False, 
            logger=None,
            preset=None,
            ffmpeg_params=['-rc','vbr','-cq','19','-b:v','0','-maxrate','0','-pix_fmt','yuv420p','-movflags','+faststart']
        )
        
        video.close()
        reframed.close()
        
        return output_path
    
    def _detect_focus_points(self, video: VideoFileClip, fps: int, duration: float) -> List[float]:
        """
        D√©tecte les points d'int√©r√™t (pose + visage) avec fallback intelligent
        """
        x_centers = []
        
        # Initialisation des d√©tecteurs
        with self.mp_pose.Pose(
            static_image_mode=False,
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5
        ) as pose, self.mp_face.FaceDetection(
            model_selection=0,
            min_detection_confidence=0.5
        ) as face_detection:
            
            # √âchantillonnage adaptatif (plus dense au d√©but)
            sample_times = self._get_sample_times(duration, fps)
            
            for t in sample_times:
                try:
                    frame = video.get_frame(t)
                    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    
                    x_center = self._detect_single_frame(image_rgb, pose, face_detection)
                    x_centers.append(x_center)
                    
                except Exception as e:
                    logger.warning(f"Erreur d√©tection frame √† {t:.2f}s: {e}")
                    x_centers.append(0.5)  # Fallback centre
        
        # Interpolation pour avoir tous les frames
        return self._interpolate_trajectory(x_centers, sample_times, duration, fps)
    
    def _detect_single_frame(self, image_rgb: np.ndarray, pose, face_detection) -> float:
        """D√©tection sur une frame unique avec fallback hi√©rarchique"""
        
        h, w = image_rgb.shape[:2]
        
        # 1. Tentative d√©tection pose (priorit√© haute)
        pose_results = pose.process(image_rgb)
        if pose_results.pose_landmarks:
            # Moyenne des landmarks du torse/t√™te pour plus de stabilit√©
            landmarks = pose_results.pose_landmarks.landmark
            
            key_points = [
                landmarks[self.mp_pose.PoseLandmark.NOSE],
                landmarks[self.mp_pose.PoseLandmark.LEFT_SHOULDER],
                landmarks[self.mp_pose.PoseLandmark.RIGHT_SHOULDER],
            ]
            
            valid_points = [p.x for p in key_points if p.visibility > 0.5]
            if valid_points:
                return sum(valid_points) / len(valid_points)
        
        # 2. Fallback : d√©tection visage
        face_results = face_detection.process(image_rgb)
        if face_results.detections:
            detection = face_results.detections[0]  # Plus grand visage
            bbox = detection.location_data.relative_bounding_box
            return bbox.xmin + bbox.width / 2
        
        # 3. Fallback : d√©tection de mouvement (centres de masse)
        gray = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2GRAY)
        edges = cv2.Canny(gray, 50, 150)
        
        # Calcul du centre de masse des contours
        contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        if contours:
            moments = [cv2.moments(c) for c in contours if cv2.contourArea(c) > 100]
            if moments:
                centroids_x = [m['m10']/m['m00'] for m in moments if m['m00'] > 0]
                if centroids_x:
                    return sum(centroids_x) / len(centroids_x) / w
        
        # 4. Dernier fallback : centre de l'image
        return 0.5
    
    def _get_sample_times(self, duration: float, fps: int) -> List[float]:
        """G√©n√®re des temps d'√©chantillonnage adaptatifs"""
        
        if duration <= 10:
            # Vid√©o courte : √©chantillonnage dense
            return list(np.arange(0, duration, 1/fps))
        elif duration <= 30:
            # Vid√©o moyenne : tous les 3 frames
            return list(np.arange(0, duration, 3/fps))
        else:
            # Vid√©o longue : √©chantillonnage plus espac√©
            return list(np.arange(0, duration, 5/fps))
    
    def _smooth_trajectory(self, x_centers: List[float], window_size: int = 15) -> List[float]:
        """Lissage avanc√© avec filtre de Savitzky-Golay"""
        
        if len(x_centers) < window_size:
            # Fallback : moyenne mobile simple
            kernel = np.ones(min(5, len(x_centers))) / min(5, len(x_centers))
            return np.convolve(x_centers, kernel, mode='same').tolist()
        
        try:
            from scipy.signal import savgol_filter
            # Filtre Savitzky-Golay pour un lissage plus naturel
            return savgol_filter(x_centers, window_size, 3).tolist()
        except ImportError:
            # Fallback : moyenne mobile
            kernel = np.ones(window_size) / window_size
            return np.convolve(x_centers, kernel, mode='same').tolist()
    
    def _interpolate_trajectory(self, x_centers: List[float], sample_times: List[float], 
                               duration: float, fps: int) -> List[float]:
        """Interpolation pour obtenir une trajectoire compl√®te"""
        
        if not x_centers:
            return [0.5] * int(duration * fps)
        
        target_times = np.arange(0, duration, 1/fps)
        
        if len(x_centers) == 1:
            return [x_centers[0]] * len(target_times)
        
        try:
            interpolated = np.interp(target_times, sample_times, x_centers)
            return interpolated.tolist()
        except:
            # Fallback : r√©p√©tition du dernier point connu
            return [x_centers[-1]] * len(target_times)
    
    def add_dynamic_subtitles(self, video_path: Path, subtitles: List[Dict]) -> Path:
        """
        Sous-titres stylis√©s TikTok avec IA contextuelle am√©lior√©e
        """
        logger.info("üí¨ Ajout de sous-titres TikTok stylis√©s avec IA")
        
        video = VideoFileClip(str(video_path))
        text_clips = []
        
        # Palettes de couleurs th√©matiques
        color_palettes = {
            'energy': ['#FF6B6B', '#4ECDC4', '#FFE66D'],
            'professional': ['#FFFFFF', '#F7DC6F', '#AED6F1'],
            'viral': ['#FF1744', '#00E676', '#FFD600', '#E91E63']
        }
        
        # D√©tection du th√®me g√©n√©ral
        all_text = ' '.join([s['text'].lower() for s in subtitles])
        theme = self._detect_content_theme(all_text)
        colors = color_palettes.get(theme, color_palettes['viral'])
        
        for i, subtitle in enumerate(subtitles):
            text = subtitle["text"].strip()
            
            # IA contextuelle pour emojis
            enhanced_text = self._add_contextual_emojis(text)
            
            # Couleur rotative de la palette
            color = colors[i % len(colors)]
            
            # Style dynamique bas√© sur le contenu
            font_size, stroke_width = self._get_dynamic_style(text)
            
            # Cr√©ation du clip texte
            txt_clip = TextClip(
                enhanced_text,
                fontsize=font_size,
                color=color,
                stroke_color='black',
                stroke_width=stroke_width,
                font=Config.SUBTITLE_FONT,
                method='caption',
                size=(video.w * 0.9, None)
            ).set_start(subtitle["start"]).set_end(subtitle["end"])
            
            # Position dynamique avec variation
            y_positions = [0.15, 0.75, 0.85]  # Haut, bas, tr√®s bas
            y_pos = y_positions[i % len(y_positions)]
            txt_clip = txt_clip.set_position(('center', video.h * y_pos))
            
            # Animations avanc√©es
            txt_clip = self._add_text_animations(txt_clip, i)
            
            text_clips.append(txt_clip)
        
        # Composition finale
        final_video = CompositeVideoClip([video] + text_clips)
        
        output_path = Config.TEMP_FOLDER / f"subtitled_{video_path.name}"
        final_video.write_videofile(
            str(output_path),
            codec='h264_nvenc',
            audio_codec='aac',
            verbose=False,
            logger=None,
            preset=None,
            ffmpeg_params=['-rc','vbr','-cq','19','-b:v','0','-maxrate','0','-pix_fmt','yuv420p','-movflags','+faststart']
        )
        
        # Nettoyage
        video.close()
        for clip in text_clips:
            clip.close()
        final_video.close()
        
        return output_path
    
    def _detect_content_theme(self, text: str) -> str:
        """D√©tecte le th√®me du contenu pour adapter le style"""
        
        energy_words = ['wow', 'amazing', 'incredible', 'fire', 'crazy', 'insane']
        professional_words = ['business', 'money', 'success', 'tips', 'advice']
        
        if any(word in text for word in energy_words):
            return 'energy'
        elif any(word in text for word in professional_words):
            return 'professional'
        else:
            return 'viral'
    
    def _add_contextual_emojis(self, text: str) -> str:
        """IA contextuelle pour ajouter des emojis pertinents"""
        
        text_lower = text.lower()
        
        # Dictionnaire contextuel √©tendu
        emoji_triggers = {
            'money': ' üí∞', 'cash': ' üíµ', 'rich': ' ü§ë', 'expensive': ' üíé',
            'fire': ' üî•', 'hot': ' üî•', 'amazing': ' ü§Ø', 'wow': 'ü§Ø ',
            'love': ' ‚ù§Ô∏è', 'heart': ' üíñ', 'beautiful': ' ‚ú®',
            'food': ' üçï', 'eat': ' üòã', 'delicious': ' ü§§',
            'fast': ' ‚ö°', 'quick': ' ‚ö°', 'speed': ' üöÄ',
            'think': ' ü§î', 'question': ' ‚ùì', 'why': ' ü§∑‚Äç‚ôÇÔ∏è',
            'win': ' üèÜ', 'winner': ' üéâ', 'success': ' ‚úÖ',
            'fail': ' ‚ùå', 'wrong': ' ‚ùå', 'mistake': ' ü§¶‚Äç‚ôÇÔ∏è'
        }
        
        enhanced_text = text
        
        for trigger, emoji in emoji_triggers.items():
            if trigger in text_lower:
                if emoji.startswith(' '):
                    enhanced_text += emoji
                else:
                    enhanced_text = emoji + enhanced_text
                break  # Un seul emoji par phrase pour √©viter la surcharge
        
        return enhanced_text
    
    def _get_dynamic_style(self, text: str) -> tuple:
        """Style dynamique bas√© sur le contenu"""
        
        if '!' in text or text.isupper():
            # Texte √©nergique
            return 75, 4
        elif '?' in text:
            # Question
            return 65, 3
        else:
            # Texte normal
            return Config.SUBTITLE_FONT_SIZE, Config.SUBTITLE_STROKE_WIDTH
    
    def _add_text_animations(self, txt_clip, index: int):
        """Ajoute des animations vari√©es aux textes"""
        
        animations = [
            lambda clip: clip.fadein(0.2).fadeout(0.2),  # Fade simple
            lambda clip: clip.fadein(0.1).fadeout(0.1),  # Fade rapide
            lambda clip: clip.crossfadein(0.3),          # Cross fade
        ]
        
        animation = animations[index % len(animations)]
        return animation(txt_clip)
    
    # M√©thodes existantes inchang√©es
    def process_all_clips(self, input_video_path: str):
        """Pipeline principal de traitement"""
        logger.info("üöÄ D√©but du pipeline de traitement avec IA")
        
        # √âtape 1: D√©coupage (votre IA existante)
        self.cut_viral_clips(input_video_path)
        
        # √âtape 2: Traitement de chaque clip
        clip_files = list(Config.CLIPS_FOLDER.glob("*.mp4"))
        
        for i, clip_path in enumerate(clip_files):
            logger.info(f"üé¨ Traitement du clip {i+1}/{len(clip_files)}: {clip_path.name}")
            try:
                self.process_single_clip(clip_path)
                logger.info(f"‚úÖ Clip {clip_path.name} trait√© avec succ√®s")
            except Exception as e:
                logger.error(f"‚ùå Erreur lors du traitement de {clip_path.name}: {e}")
    
    def cut_viral_clips(self, input_video_path: str):
        """Interface pour votre IA de d√©coupage existante"""
        logger.info("üìº D√©coupage des clips avec IA...")
        
        # Exemple basique - remplacez par votre IA
        video = VideoFileClip(input_video_path)
        duration = video.duration
        
        # Exemple: d√©couper en segments de 30 secondes
        segment_duration = 30
        segments = int(duration // segment_duration)
        
        for i in range(min(segments, 5)):  # Max 5 clips pour test
            start_time = i * segment_duration
            end_time = min((i + 1) * segment_duration, duration)
            
            clip = video.subclip(start_time, end_time)
            output_path = Config.CLIPS_FOLDER / f"clip_{i+1:02d}.mp4"
            clip.write_videofile(str(output_path), verbose=False, logger=None)
        
        video.close()
        logger.info(f"‚úÖ {segments} clips g√©n√©r√©s")
    
    def process_single_clip(self, clip_path: Path):
        """Traite un clip individuel avec les nouvelles fonctionnalit√©s IA"""
        
        # 1. Reframe et recadrage 9:16 avec IA
        reframed_path = self.reframe_to_vertical(clip_path)
        
        # 2. G√©n√©ration des sous-titres avec timestamps
        subtitles = self.generate_subtitles_with_timing(reframed_path)
        
        # 3. Ajout des sous-titres stylis√©s TikTok (m√©thode compatible emoji)
        final_path = add_tiktok_subtitles(str(reframed_path), subtitles)
        
        # 4. D√©placement vers le dossier output  
        output_path = Config.OUTPUT_FOLDER / f"final_{clip_path.name}"
        Path(final_path).rename(output_path)
        
        return output_path
    
    def transcribe_audio(self, video_path: Path) -> str:
        """Transcription avec Whisper"""
        logger.info("üìù Transcription audio avec Whisper")
        result = self.whisper_model.transcribe(str(video_path))
        return result["text"]
    
    def generate_subtitles_with_timing(self, video_path: Path) -> List[Dict]:
        """G√©n√®re des sous-titres avec timestamps pr√©cis"""
        logger.info("‚è±Ô∏è G√©n√©ration des sous-titres avec timing")
        
        result = self.whisper_model.transcribe(str(video_path), word_timestamps=True)
        
        subtitles = []
        for segment in result["segments"]:
            subtitle = {
                "text": segment["text"].strip(),
                "start": segment["start"],
                "end": segment["end"]
            }
            subtitles.append(subtitle)
        
        return subtitles
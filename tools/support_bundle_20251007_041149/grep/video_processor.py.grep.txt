
  video_processor.py:23:from collections import Counter
  video_processor.py:24:from dataclasses import dataclass
  video_processor.py:25:
> video_processor.py:26:from video_pipeline.broll_rules import BrollClip, enforce_broll_schedule_rules 
as _enforce_broll_schedule_rules_v2
  video_processor.py:27:from video_pipeline.config.settings import get_settings
  video_processor.py:28:import types
  video_processor.py:29:import gc
  video_processor.py:67:
  video_processor.py:68:
  video_processor.py:69:try:
> video_processor.py:70:    from pipeline_core.llm_service import generate_metadata_as_json
  video_processor.py:71:except Exception:
> video_processor.py:72:    generate_metadata_as_json = None
  video_processor.py:73:
  video_processor.py:74:try:
  video_processor.py:75:    from config import Config as _ROOT_CONFIG
> video_processor.py:76:except Exception:  # pragma: no cover - fallback when root config is 
unavailable
  video_processor.py:77:    class _ROOT_CONFIG:
> video_processor.py:78:        ENABLE_LEGACY_PIPELINE_FALLBACK = False
  video_processor.py:79:
  video_processor.py:80:# ðŸš€ NOUVEAU: Configuration des logs temps rÃ©el + suppression warnings 
non-critiques
  video_processor.py:81:import warnings
  video_processor.py:86:
  video_processor.py:87:try:
  video_processor.py:88:    if hasattr(sys.stdout, 'buffer'):
> video_processor.py:89:        _STDOUT_STREAM = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', 
errors='replace', write_through=True)
  video_processor.py:90:    else:
> video_processor.py:91:        _STDOUT_STREAM = sys.stdout
  video_processor.py:92:except Exception:
> video_processor.py:93:    _STDOUT_STREAM = sys.stdout
  video_processor.py:94:
  video_processor.py:95:logging.basicConfig(
  video_processor.py:96:    level=logging.INFO,
  video_processor.py:97:    format='[%(asctime)s] %(message)s',
  video_processor.py:98:    datefmt='%H:%M:%S',
  video_processor.py:99:    handlers=[
> video_processor.py:100:        logging.StreamHandler(_STDOUT_STREAM)
  video_processor.py:101:    ]
  video_processor.py:102:)
  video_processor.py:103:logger = logging.getLogger(__name__)
  video_processor.py:113:    _DEPENDENCY_STATUS_MESSAGES.append(message)
  video_processor.py:114:
  video_processor.py:115:
> video_processor.py:116:def emit_dependency_status(stream: Optional[TextIO] = None, *, once: bool = 
True) -> None:
  video_processor.py:117:    """Print deferred dependency status messages.
  video_processor.py:118:
  video_processor.py:119:    Parameters
  video_processor.py:120:    ----------
> video_processor.py:121:    stream:
> video_processor.py:122:        Destination stream; defaults to ``sys.stdout``.
  video_processor.py:123:    once:
  video_processor.py:124:        When ``True`` (the default), messages are emitted only the first time 
the
  video_processor.py:125:        function is called. Subsequent calls become no-ops to avoid duplicated
  video_processor.py:131:    if once and _DEPENDENCY_STATUS_EMITTED:
  video_processor.py:132:        return
  video_processor.py:133:
> video_processor.py:134:    output = stream or sys.stdout
  video_processor.py:135:    for message in _DEPENDENCY_STATUS_MESSAGES:
  video_processor.py:136:        print(message, file=output)
  video_processor.py:137:
  video_processor.py:168:            total_frames = sample_frames * 2
  video_processor.py:169:        step = max(int(total_frames // max(sample_frames, 1) or 1), 1)
  video_processor.py:170:
> video_processor.py:171:        face_model = cv2.data.haarcascades + 
'haarcascade_frontalface_default.xml'
> video_processor.py:172:        face_detector = cv2.CascadeClassifier(face_model)
  video_processor.py:173:        face_centers: List[float] = []
  video_processor.py:174:        for idx in range(sample_frames):
  video_processor.py:175:            frame_idx = idx * step
  video_processor.py:274:    return cropped, metadata
  video_processor.py:275:
  video_processor.py:276:
> video_processor.py:277:def format_broll_completion_banner(
  video_processor.py:278:    inserted_count: int,
  video_processor.py:279:    *,
  video_processor.py:280:    origin: str = "pipeline",
  video_processor.py:282:) -> Tuple[bool, str]:
  video_processor.py:283:    """Return a (success, banner) tuple describing the B-roll insertion 
result.
  video_processor.py:284:
> video_processor.py:285:    The banner preserves the historical success message so downstream log 
parsing
  video_processor.py:286:    keeps working, while providing a dedicated warning when no insertions were
  video_processor.py:287:    produced. The *origin* parameter allows callers to specialise the warning 
for
  video_processor.py:288:    pipeline_core versus the legacy pipeline.
  video_processor.py:314:PHASH_DISTANCE = 6
  video_processor.py:315:
  video_processor.py:316:
> video_processor.py:317:_GLOBAL_BROLL_EVENTS_LOGGER: Optional['JsonlLogger'] = None
  video_processor.py:318:
  video_processor.py:319:
  video_processor.py:320:@dataclass
  video_processor.py:350:    text: str
  video_processor.py:351:
  video_processor.py:352:
> video_processor.py:353:def _apply_broll_invariants_to_core_entries(
  video_processor.py:354:    entries: Sequence[CoreTimelineEntry],
  video_processor.py:355:    *,
  video_processor.py:356:    seen_updates: Optional[Sequence[Dict[str, Any]]] = None,
  video_processor.py:367:    if not entries_list:
  video_processor.py:368:        settings = get_settings()
  video_processor.py:369:        print(
> video_processor.py:370:            f"[BROLL_RULES] kept=0/0 (100.0%) 
min_start={settings.broll.min_start_s}s "
> video_processor.py:371:            f"gap={settings.broll.min_gap_s}s 
no_repeat={settings.broll.no_repeat_s}s"
  video_processor.py:372:        )
  video_processor.py:373:        return [], [] if updates_list is not None else None
  video_processor.py:374:
  video_processor.py:375:    settings = get_settings()
  video_processor.py:376:
> video_processor.py:377:    clip_records: List[Tuple[CoreTimelineEntry, BrollClip]] = []
  video_processor.py:378:    for entry in entries_list:
  video_processor.py:379:        asset_identifier = entry.url or str(entry.path)
  video_processor.py:380:        clip_records.append(
  video_processor.py:381:            (
  video_processor.py:382:                entry,
> video_processor.py:383:                BrollClip(
  video_processor.py:384:                    start_s=float(entry.start),
  video_processor.py:385:                    end_s=float(entry.end),
  video_processor.py:386:                    asset_id=str(asset_identifier),
  video_processor.py:389:            )
  video_processor.py:390:        )
  video_processor.py:391:
> video_processor.py:392:    filtered_clips = _enforce_broll_schedule_rules_v2(
  video_processor.py:393:        [clip for _, clip in clip_records],
> video_processor.py:394:        min_start_s=settings.broll.min_start_s,
> video_processor.py:395:        min_gap_s=settings.broll.min_gap_s,
> video_processor.py:396:        no_repeat_s=settings.broll.no_repeat_s,
  video_processor.py:397:    )
  video_processor.py:398:
  video_processor.py:399:    key_counts: Counter[Tuple[float, float, str, int]] = Counter(
  video_processor.py:416:    kept = len(filtered_entries)
  video_processor.py:417:    ratio = 100.0 if total == 0 else (100.0 * kept / total)
  video_processor.py:418:    print(
> video_processor.py:419:        f"[BROLL_RULES] kept={kept}/{total} ({ratio:.1f}%) "
> video_processor.py:420:        f"min_start={settings.broll.min_start_s}s 
gap={settings.broll.min_gap_s}s "
> video_processor.py:421:        f"no_repeat={settings.broll.no_repeat_s}s"
  video_processor.py:422:    )
  video_processor.py:423:
  video_processor.py:424:    if filtered_updates is not None:
  video_processor.py:426:    return filtered_entries, None
  video_processor.py:427:
  video_processor.py:428:
> video_processor.py:429:def run_with_timeout(fn, timeout_s: float, *args, **kwargs):
> video_processor.py:430:    if timeout_s <= 0:
  video_processor.py:431:        return fn(*args, **kwargs)
  video_processor.py:432:    executor = concurrent.futures.ThreadPoolExecutor(max_workers=1)
  video_processor.py:433:    future = executor.submit(fn, *args, **kwargs)
  video_processor.py:434:    try:
> video_processor.py:435:        return future.result(timeout=timeout_s)
> video_processor.py:436:    except concurrent.futures.TimeoutError:
  video_processor.py:437:        try:
  video_processor.py:438:            future.cancel()
  video_processor.py:439:        except Exception:
  video_processor.py:500:    return unique, hits
  video_processor.py:501:
  video_processor.py:502:
> video_processor.py:503:def enforce_broll_schedule_rules(plan, *, min_duration: float = 1.8, min_gap: 
float = 1.5):
  video_processor.py:504:    """Filter a naive schedule to satisfy duration and gap constraints."""
  video_processor.py:505:
  video_processor.py:506:    filtered = []
  video_processor.py:514:            start, end = end, start
  video_processor.py:515:        duration = max(0.0, end - start)
  video_processor.py:516:        reason = None
> video_processor.py:517:        gap_threshold = max(0.0, min_gap)
  video_processor.py:518:        duration_threshold = max(0.0, min_duration)
  video_processor.py:519:
  video_processor.py:520:        if filtered and (start - last_end) < gap_threshold:
  video_processor.py:531:
  video_processor.py:532:    return filtered, drops
  video_processor.py:533:
> video_processor.py:534:# --- Query normalization for external providers (Pexels/Pixabay)
  video_processor.py:535:_STOPWORDS: Set[str] = {
  video_processor.py:536:    'the','a','an','to','of','in','on','at','for','and','or','but',
  video_processor.py:537:    'first','thing','that','this','those','these','you','we','they',
  video_processor.py:545:    'internal_reward': ['self reward', 'intrinsic motivation']
  video_processor.py:546:}
  video_processor.py:547:
> video_processor.py:548:def _normalize_queries(llm_keywords: List[str], transcript_tokens: List[str], 
*, max_queries: int = 8) -> List[str]:
  video_processor.py:549:    """Produce a compact, deduplicated list of provider queries.
> video_processor.py:550:    - Prefer LLM keywords; fallback to transcript tokens if empty
  video_processor.py:551:    - Normalize underscores, strip punctuation, drop stopwords
  video_processor.py:552:    - Expand with a small synonym map; cap list size
  video_processor.py:553:    """
  video_processor.py:563:            if cleaned and cleaned not in _STOPWORDS:
  video_processor.py:564:                yield cleaned
  video_processor.py:565:
> video_processor.py:566:    llm_terms = list(llm_keywords or [])
  video_processor.py:567:    if llm_terms:
  video_processor.py:568:        llm_terms = _concretize_queries(llm_terms)
  video_processor.py:569:
  video_processor.py:594:        folder.mkdir(parents=True, exist_ok=True)
  video_processor.py:595:
  video_processor.py:596:
> video_processor.py:597:def _try_overlay_http_direct(broll_url: str, t0: float, t1: float, 
render_cfg, base_cmd: list[str]) -> bool:
  video_processor.py:598:    duration = max(0.1, float(t1 - t0))
  video_processor.py:599:    cmd = base_cmd + [
  video_processor.py:600:        '-ss', f"{max(0.0, t0):.3f}",
  video_processor.py:601:        '-to', f"{max(0.0, t0 + duration):.3f}",
> video_processor.py:602:        '-i', broll_url,
  video_processor.py:603:    ]
  video_processor.py:604:    try:
  video_processor.py:605:        proc = subprocess.run(cmd, check=True)
  video_processor.py:608:        return False
  video_processor.py:609:
  video_processor.py:610:
> video_processor.py:611:def _overlay_via_pipe(broll_url: str, t0: float, t1: float, render_cfg, 
base_cmd: list[str]) -> bool:
  video_processor.py:612:    duration = max(0.1, float(t1 - t0))
> video_processor.py:613:    extract_cmd = f'ffmpeg -hide_banner -loglevel error -ss {t0:.3f} -i 
"{broll_url}" -t {duration:.3f} -an -c:v libx264 -preset veryfast -f mpegts -'
  video_processor.py:614:    try:
  video_processor.py:615:        extractor = subprocess.Popen(shlex.split(extract_cmd), 
stdout=subprocess.PIPE)
  video_processor.py:616:    except Exception:
  video_processor.py:625:                if not chunk:
  video_processor.py:626:                    break
  video_processor.py:627:                tmp.write(chunk)
> video_processor.py:628:        extractor.wait(timeout=30)
  video_processor.py:629:        cmd = base_cmd + ['-i', tmp_path]
  video_processor.py:630:        try:
  video_processor.py:631:            proc = subprocess.run(cmd, check=True)
  video_processor.py:643:    return False
  video_processor.py:644:
  video_processor.py:645:
> video_processor.py:646:def overlay_http_or_pipe(broll_url: str, t0: float, t1: float, render_cfg, 
base_cmd: list[str]) -> bool:
> video_processor.py:647:    if _try_overlay_http_direct(broll_url, t0, t1, render_cfg, base_cmd):
  video_processor.py:648:        return True
> video_processor.py:649:    return _overlay_via_pipe(broll_url, t0, t1, render_cfg, base_cmd)
  video_processor.py:650:
  video_processor.py:651:
  video_processor.py:652:import os
> video_processor.py:653:import json
  video_processor.py:654:import random
  video_processor.py:655:import numpy as np
  video_processor.py:656:import shutil
  video_processor.py:665:    PipelineConfigBundle = None  # type: ignore[assignment]
  video_processor.py:666:from pipeline_core.fetchers import FetcherOrchestrator
  video_processor.py:667:from pipeline_core.dedupe import compute_phash, hamming_distance
> video_processor.py:668:from pipeline_core.logging import JsonlLogger, log_broll_decision
  video_processor.py:669:try:
  video_processor.py:670:    from pipeline_core.llm_service import (
  video_processor.py:671:        DynamicCompletionError,
  video_processor.py:715:                    break
  video_processor.py:716:        return cleaned
  video_processor.py:717:
> video_processor.py:718:    _FALLBACK_SHARED_SERVICE: LLMMetadataGeneratorService | None = None
  video_processor.py:719:
  video_processor.py:720:    def get_shared_llm_service() -> LLMMetadataGeneratorService:  # type: 
ignore[override]
> video_processor.py:721:        global _FALLBACK_SHARED_SERVICE
> video_processor.py:722:        if _FALLBACK_SHARED_SERVICE is None:
> video_processor.py:723:            _FALLBACK_SHARED_SERVICE = LLMMetadataGeneratorService()
> video_processor.py:724:        return _FALLBACK_SHARED_SERVICE
  video_processor.py:725:
  video_processor.py:726:# ðŸš€ NOUVEAU: Cache global pour Ã©viter le rechargement des modÃ¨les
> video_processor.py:727:_MODEL_CACHE = {}
  video_processor.py:728:
  video_processor.py:729:# --- Dynamic LLM context toggle (default: on)
  video_processor.py:730:ENABLE_DYNAMIC_CONTEXT = os.getenv("ENABLE_DYNAMIC_CONTEXT", "true").lower() 
not in {"0","false","no"}
  video_processor.py:732:# ---- Feature flags / knobs for per-segment refinement
  video_processor.py:733:ENABLE_SEGMENT_REFINEMENT = os.getenv("ENABLE_SEGMENT_REFINEMENT", 
"true").lower() not in {"0","false","no"}
  video_processor.py:734:SEGMENT_REFINEMENT_MAX_TERMS = int(os.getenv("SEGMENT_REFINEMENT_MAX_TERMS", 
"4"))
> video_processor.py:735:SEGMENT_FETCH_TIMEOUT_S = float(os.getenv("SEGMENT_FETCH_TIMEOUT_S", "1.5"))
  video_processor.py:736:SEGMENT_PARALLEL_REQUESTS = int(os.getenv("SEGMENT_PARALLEL_REQUESTS", "2"))
  video_processor.py:737:
  video_processor.py:738:# ---- Feature flag: prefer dynamic LLM domain for selector
  video_processor.py:752:
  video_processor.py:753:    if detected_provider_names is not None:
  video_processor.py:754:        try:
> video_processor.py:755:            providers = detected_provider_names(getattr(cfg, "fetcher", None))
> video_processor.py:756:            if providers:
> video_processor.py:757:                return list(providers)
  video_processor.py:758:        except Exception:
  video_processor.py:759:            pass
  video_processor.py:760:
  video_processor.py:761:    fetcher = getattr(cfg, "fetcher", None)
> video_processor.py:762:    provider_seq = getattr(fetcher, "providers", None)
  video_processor.py:763:    if not provider_seq:
  video_processor.py:764:        return []
  video_processor.py:765:
  video_processor.py:836:    return out
  video_processor.py:837:
  video_processor.py:838:
> video_processor.py:839:def _basic_metadata_fallback(full_text: str) -> Dict[str, Any]:
  video_processor.py:840:    """Generate simple metadata using the transcript when advanced services 
fail."""
  video_processor.py:841:
  video_processor.py:842:    text = (full_text or "").strip()
  video_processor.py:870:        if tag and tag not in hashtags:
  video_processor.py:871:            hashtags.append(tag)
  video_processor.py:872:
> video_processor.py:873:    broll_keywords = dominant_terms[:]
> video_processor.py:874:    fallback_keywords = [
  video_processor.py:875:        "audience reaction",
  video_processor.py:876:        "speaker close up",
  video_processor.py:877:        "dynamic text overlay",
  video_processor.py:878:        "city skyline",
  video_processor.py:879:        "motivational crowd",
  video_processor.py:880:    ]
> video_processor.py:881:    for term in fallback_keywords:
> video_processor.py:882:        if len(broll_keywords) >= 5:
  video_processor.py:883:            break
> video_processor.py:884:        if term not in broll_keywords:
> video_processor.py:885:            broll_keywords.append(term)
  video_processor.py:886:
  video_processor.py:887:    queries = [f"{term} b-roll" for term in dominant_terms[:3]]
> video_processor.py:888:    fallback_queries = [
  video_processor.py:889:        "motivational speech b-roll",
  video_processor.py:890:        "audience clapping stock footage",
  video_processor.py:891:        "city skyline night aerial",
  video_processor.py:892:    ]
> video_processor.py:893:    for query in fallback_queries:
  video_processor.py:894:        if len(queries) >= 3:
  video_processor.py:895:            break
  video_processor.py:896:        if query not in queries:
  video_processor.py:900:        "title": title,
  video_processor.py:901:        "description": description,
  video_processor.py:902:        "hashtags": hashtags,
> video_processor.py:903:        "broll_keywords": broll_keywords,
  video_processor.py:904:        "queries": queries,
  video_processor.py:905:    }
  video_processor.py:906:
  video_processor.py:907:
  video_processor.py:908:def _segment_brief_terms(dyn: dict, seg_idx: int) -> tuple[list[str], 
list[str]]:
> video_processor.py:909:    """Return (queries, keywords) declared for a segment inside the dynamic 
briefs."""
  video_processor.py:910:
  video_processor.py:911:    if not isinstance(dyn, dict):
  video_processor.py:912:        return [], []
  video_processor.py:939:        return []
  video_processor.py:940:
  video_processor.py:941:    queries: list[str] = []
> video_processor.py:942:    keywords: list[str] = []
  video_processor.py:943:    seen_queries: set[str] = set()
> video_processor.py:944:    seen_keywords: set[str] = set()
  video_processor.py:945:
  video_processor.py:946:    for br in matching_briefs:
  video_processor.py:947:        for term in _iter_terms(br.get("queries")):
  video_processor.py:952:            seen_queries.add(cleaned)
  video_processor.py:953:
  video_processor.py:954:    for br in matching_briefs:
> video_processor.py:955:        for term in _iter_terms(br.get("keywords")):
  video_processor.py:956:            cleaned = term.replace("_", " ").strip()
> video_processor.py:957:            if not cleaned or cleaned in seen_keywords:
  video_processor.py:958:                continue
> video_processor.py:959:            keywords.append(cleaned)
> video_processor.py:960:            seen_keywords.add(cleaned)
  video_processor.py:961:
> video_processor.py:962:    return queries, keywords
  video_processor.py:963:
  video_processor.py:964:
  video_processor.py:965:def _segment_terms_from_briefs(dyn: dict, seg_idx: int, cap: int) -> 
list[str]:
> video_processor.py:966:    """Extract up to `cap` clean terms (queries then keywords) for a given 
segment index."""
  video_processor.py:967:
  video_processor.py:968:    if cap <= 0:
  video_processor.py:969:        return []
  video_processor.py:970:
> video_processor.py:971:    queries, keywords = _segment_brief_terms(dyn, seg_idx)
  video_processor.py:972:    out: list[str] = []
  video_processor.py:973:    for term in queries:
  video_processor.py:974:        if len(out) >= cap:
  video_processor.py:975:            break
  video_processor.py:976:        out.append(term)
  video_processor.py:977:    if len(out) < cap:
> video_processor.py:978:        for term in keywords:
  video_processor.py:979:            if len(out) >= cap:
  video_processor.py:980:                break
  video_processor.py:981:            if term in out:
  video_processor.py:1011:        return _SEED_QUERY_CACHE
  video_processor.py:1012:
  video_processor.py:1013:    candidates: list[str] = []
> video_processor.py:1014:    env_path = os.getenv("BROLL_SEED_QUERIES")
  video_processor.py:1015:    search_paths: list[Path] = []
  video_processor.py:1016:    if env_path:
  video_processor.py:1017:        try:
  video_processor.py:1018:            search_paths.append(Path(env_path).expanduser())
  video_processor.py:1019:        except Exception:
  video_processor.py:1020:            pass
> video_processor.py:1021:    search_paths.append(PROJECT_ROOT / "seed_queries.json")
> video_processor.py:1022:    search_paths.append(PROJECT_ROOT / "config" / "seed_queries.json")
  video_processor.py:1023:
  video_processor.py:1024:    for path in search_paths:
  video_processor.py:1025:        try:
  video_processor.py:1029:            continue
  video_processor.py:1030:        try:
  video_processor.py:1031:            with path.open("r", encoding="utf-8") as handle:
> video_processor.py:1032:                payload = json.load(handle)
  video_processor.py:1033:        except Exception:
  video_processor.py:1034:            continue
  video_processor.py:1035:        candidates = _flatten_seed_terms(payload)
  video_processor.py:1070:    return out[: max(0, limit)]
  video_processor.py:1071:
  video_processor.py:1072:
> video_processor.py:1073:def _build_transcript_fallback_terms(
  video_processor.py:1074:    segment_text: str,
> video_processor.py:1075:    segment_keywords: Sequence[str],
  video_processor.py:1076:    *,
  video_processor.py:1077:    limit: int,
  video_processor.py:1078:) -> list[str]:
  video_processor.py:1080:        return []
  video_processor.py:1081:
  video_processor.py:1082:    tokens: list[str] = []
> video_processor.py:1083:    for source in (segment_keywords or []):
  video_processor.py:1084:        if not isinstance(source, str):
  video_processor.py:1085:            continue
  video_processor.py:1086:        tokens.extend(re.findall(r"[a-zA-Z]{4,}", source.lower()))
  video_processor.py:1115:    segment_text: str,
  video_processor.py:1116:    llm_queries: Sequence[str],
  video_processor.py:1117:    brief_queries: Sequence[str],
> video_processor.py:1118:    brief_keywords: Sequence[str],
> video_processor.py:1119:    segment_keywords: Sequence[str],
> video_processor.py:1120:    selector_keywords: Sequence[str],
  video_processor.py:1121:    cap: int,
  video_processor.py:1122:) -> tuple[list[str], str]:
  video_processor.py:1123:    cap = max(1, int(cap or 0))
  video_processor.py:1159:
  video_processor.py:1160:    _consume("llm_hint", llm_queries)
  video_processor.py:1161:    brief_pool: list[str] = []
> video_processor.py:1162:    brief_pool.extend(brief_keywords or [])
  video_processor.py:1163:    brief_pool.extend(brief_queries or [])
  video_processor.py:1164:    _consume("segment_brief", brief_pool)
  video_processor.py:1165:
  video_processor.py:1166:    if not combined:
> video_processor.py:1167:        if _consume("segment_keywords", segment_keywords):
  video_processor.py:1168:            pass
  video_processor.py:1169:    if not combined:
> video_processor.py:1170:        _consume("selector_keywords", selector_keywords)
  video_processor.py:1171:    if not combined:
  video_processor.py:1172:        seed_queries = _load_seed_queries()
  video_processor.py:1173:        _consume("seed_queries", seed_queries)
  video_processor.py:1174:
  video_processor.py:1175:    if not combined:
> video_processor.py:1176:        fallback_terms = _build_transcript_fallback_terms(
  video_processor.py:1177:            segment_text,
> video_processor.py:1178:            segment_keywords,
  video_processor.py:1179:            limit=cap,
  video_processor.py:1180:        )
> video_processor.py:1181:        _consume("transcript_fallback", fallback_terms, relax=True)
  video_processor.py:1182:
  video_processor.py:1183:    if not combined:
> video_processor.py:1184:        _consume("transcript_fallback", ["stock footage"], relax=True)
  video_processor.py:1185:
  video_processor.py:1186:    if primary_source == "none" and combined:
> video_processor.py:1187:        primary_source = "transcript_fallback"
  video_processor.py:1188:
  video_processor.py:1189:    return combined[:cap], primary_source
  video_processor.py:1190:
  video_processor.py:1216:        pass
  video_processor.py:1217:    return None, None
  video_processor.py:1218:
> video_processor.py:1219:def get_sentence_transformer_model(model_name: str):
  video_processor.py:1220:    """RÃ©cupÃ¨re un modÃ¨le SentenceTransformer depuis le cache ou le 
charge"""
  video_processor.py:1221:    # ðŸš€ OPTIMISATION: Normaliser le nom du modÃ¨le pour Ã©viter les 
doublons
> video_processor.py:1222:    normalized_name = model_name.replace('sentence-transformers/', '')
  video_processor.py:1223:    
> video_processor.py:1224:    if normalized_name not in _MODEL_CACHE:
> video_processor.py:1225:        print(f"    ðŸ”„ Chargement initial du modÃ¨le: {model_name}")
  video_processor.py:1226:        try:
  video_processor.py:1227:            from sentence_transformers import SentenceTransformer
> video_processor.py:1228:            _MODEL_CACHE[normalized_name] = SentenceTransformer(model_name)
> video_processor.py:1229:            print(f"    âœ… ModÃ¨le {model_name} chargÃ© et mis en cache")
  video_processor.py:1230:        except Exception as e:
> video_processor.py:1231:            print(f"    âŒ Erreur chargement modÃ¨le {model_name}: {e}")
  video_processor.py:1232:            return None
  video_processor.py:1233:    else:
> video_processor.py:1234:        print(f"    â™»ï¸ ModÃ¨le {model_name} rÃ©cupÃ©rÃ© du cache")
  video_processor.py:1235:    
> video_processor.py:1236:    return _MODEL_CACHE[normalized_name]
  video_processor.py:1237:
  video_processor.py:1238:def safe_remove_tree(directory: Path, max_retries: int = 3, delay: float = 
1.0) -> bool:
  video_processor.py:1239:    """
> video_processor.py:1240:    Supprime un dossier de faÃ§on sÃ©curisÃ©e avec retry et gestion des 
handles Windows
  video_processor.py:1241:    
  video_processor.py:1242:    Args:
  video_processor.py:1243:        directory: Dossier Ã  supprimer
  video_processor.py:1264:                
  video_processor.py:1265:        except PermissionError as e:
  video_processor.py:1266:            if "WinError 32" in str(e) or "being used by another process" in 
str(e):
> video_processor.py:1267:                print(f"    âšï¸ Tentative {attempt + 1}/{max_retries}: 
Fichier en cours d'utilisation, retry dans {delay}s...")
  video_processor.py:1268:                time.sleep(delay)
  video_processor.py:1269:                delay *= 1.5  # Backoff exponentiel
  video_processor.py:1270:                continue
  video_processor.py:1289:        print(f"    âŒ Ã‰chec final de suppression: {e}")
  video_processor.py:1290:        return False
  video_processor.py:1291:
> video_processor.py:1292:# Gestion optionnelle de Mediapipe avec fallback
  video_processor.py:1293:try:
  video_processor.py:1294:    import mediapipe as mp
  video_processor.py:1295:    MEDIAPIPE_AVAILABLE = True
  video_processor.py:1297:except ImportError:
  video_processor.py:1298:    MEDIAPIPE_AVAILABLE = False
  video_processor.py:1299:    mp = None
> video_processor.py:1300:    _register_dependency_status("âšï¸ Mediapipe non disponible - 
Utilisation du fallback OpenCV (fonctionnalitÃ©s rÃ©duites)")
  video_processor.py:1301:
  video_processor.py:1302:# ðŸš€ NOUVEAU: Import du sÃ©lecteur B-roll gÃ©nÃ©rique
  video_processor.py:1303:try:
> video_processor.py:1304:    from broll_selector import BrollSelector, Asset, ScoringFeatures, 
BrollCandidate
> video_processor.py:1305:    BROLL_SELECTOR_AVAILABLE = True
  video_processor.py:1306:    _register_dependency_status("âœ… SÃ©lecteur B-roll gÃ©nÃ©rique 
disponible - Scoring mixte activÃ©")
  video_processor.py:1307:except ImportError as e:
> video_processor.py:1308:    BROLL_SELECTOR_AVAILABLE = False
  video_processor.py:1309:    _register_dependency_status(f"âšï¸ SÃ©lecteur B-roll gÃ©nÃ©rique non 
disponible: {e}")
  video_processor.py:1310:    _register_dependency_status("   ðŸ”„ Utilisation du systÃ¨me de scoring 
existant")
  video_processor.py:1311:
  video_processor.py:1313:from moviepy.video.fx.all import crop
  video_processor.py:1314:from tqdm import tqdm  # NEW: console progress
  video_processor.py:1315:import re # NEW: for caption/hashtag generation
> video_processor.py:1316:from hormozi_subtitles import add_hormozi_subtitles
  video_processor.py:1317:
  video_processor.py:1318:
  video_processor.py:1319:def _format_srt_timestamp(total_seconds: float) -> str:
  video_processor.py:1376:
  video_processor.py:1377:
  video_processor.py:1378:def _read_ui_settings() -> Dict:
> video_processor.py:1379:    """Read optional UI settings from config/ui_settings.json."""
  video_processor.py:1380:    try:
> video_processor.py:1381:        cfg_path = Path('config/ui_settings.json')
  video_processor.py:1382:        if cfg_path.exists():
  video_processor.py:1383:            with open(cfg_path, 'r', encoding='utf-8') as f:
> video_processor.py:1384:                return json.load(f) or {}
  video_processor.py:1385:    except Exception:
  video_processor.py:1386:        pass
  video_processor.py:1387:    return {}
  video_processor.py:1410:    return getattr(Config, "ENABLE_PIPELINE_CORE_FETCHER", False)
  video_processor.py:1411:
  video_processor.py:1412:
> video_processor.py:1413:def _legacy_pipeline_fallback_enabled() -> bool:
  video_processor.py:1414:    """Return True when the legacy src.pipeline integration may execute."""
  video_processor.py:1415:
> video_processor.py:1416:    override = os.getenv("ENABLE_LEGACY_PIPELINE_FALLBACK")
  video_processor.py:1417:    if override is not None:
  video_processor.py:1418:        return _to_bool(override)
> video_processor.py:1419:    return getattr(Config, "ENABLE_LEGACY_PIPELINE_FALLBACK", False)
  video_processor.py:1420:
  video_processor.py:1421:
  video_processor.py:1422:# Load optional UI overrides once
  video_processor.py:1445:                _register_dependency_status(f"âœ… ImageMagick configurÃ©: 
{path}")
  video_processor.py:1446:                return True
  video_processor.py:1447:
> video_processor.py:1448:        _register_dependency_status("âšï¸ ImageMagick non trouvÃ©, 
utilisation du mode fallback")
  video_processor.py:1449:        return False
  video_processor.py:1450:
  video_processor.py:1451:    except Exception as e:
  video_processor.py:1470:    TARGET_HEIGHT = 1280  # Format 9:16
  video_processor.py:1471:    
  video_processor.py:1472:    # ParamÃ¨tres Whisper
> video_processor.py:1473:    WHISPER_MODEL = "tiny"  # ou "small", "medium", "large"
  video_processor.py:1474:    
  video_processor.py:1475:    # ParamÃ¨tres sous-titres
> video_processor.py:1476:    SUBTITLE_FONT_SIZE = 85
> video_processor.py:1477:    SUBTITLE_COLOR = 'yellow'
> video_processor.py:1478:    SUBTITLE_STROKE_COLOR = 'black'
> video_processor.py:1479:    SUBTITLE_STROKE_WIDTH = 3
  video_processor.py:1480:    # Biais global (en secondes) pour corriger un lÃ©ger dÃ©calage 
systÃ©matique
  video_processor.py:1481:    # 0.0 par dÃ©faut pour Ã©viter tout dÃ©calage si non nÃ©cessaire
> video_processor.py:1482:    SUBTITLE_TIMING_BIAS_S = 0.0
  video_processor.py:1483:
  video_processor.py:1484:    # Activation B-roll: UI > ENV > dÃ©faut(off)
  video_processor.py:1485:    # Si fetchers cochÃ©s, activer automatiquement l'insertion B-roll, sauf 
si explicitement dÃ©sactivÃ© cÃ´tÃ© UI
> video_processor.py:1486:    _UI_ENABLE_BROLL = _UI_SETTINGS.get('enable_broll') if 'enable_broll' in 
_UI_SETTINGS else None
> video_processor.py:1487:    _ENV_ENABLE_BROLL = os.getenv('ENABLE_BROLL') or 
os.getenv('AI_BROLL_ENABLED')
> video_processor.py:1488:    _AUTO_ENABLE = _to_bool(_UI_SETTINGS.get('broll_fetch_enable'), 
default=True) if 'broll_fetch_enable' in _UI_SETTINGS else _to_bool(os.getenv('BROLL_FETCH_ENABLE') or 
os.getenv('AI_BROLL_ENABLE_FETCHER'), default=True)
> video_processor.py:1489:    ENABLE_BROLL = (
> video_processor.py:1490:        _to_bool(_UI_ENABLE_BROLL, default=False) if _UI_ENABLE_BROLL is not 
None
> video_processor.py:1491:        else (_to_bool(_ENV_ENABLE_BROLL, default=False) or _AUTO_ENABLE)
  video_processor.py:1492:    )
> video_processor.py:1493:    ENABLE_LEGACY_PIPELINE_FALLBACK = _to_bool(
> video_processor.py:1494:        _UI_SETTINGS.get('legacy_pipeline_fallback'),
> video_processor.py:1495:        default=getattr(_ROOT_CONFIG, 'ENABLE_LEGACY_PIPELINE_FALLBACK', 
False),
> video_processor.py:1496:    ) if 'legacy_pipeline_fallback' in _UI_SETTINGS else 
getattr(_ROOT_CONFIG, 'ENABLE_LEGACY_PIPELINE_FALLBACK', False)
  video_processor.py:1497:
  video_processor.py:1498:    # === Options fetcher B-roll (stock) ===
  video_processor.py:1499:    # Active le fetch automatique: UI > ENV > dÃ©faut(on)
> video_processor.py:1500:    BROLL_FETCH_ENABLE = _to_bool(_UI_SETTINGS.get('broll_fetch_enable'), 
default=True) if 'broll_fetch_enable' in _UI_SETTINGS else _to_bool(os.getenv('BROLL_FETCH_ENABLE') or 
os.getenv('AI_BROLL_ENABLE_FETCHER'), default=True)
  video_processor.py:1501:    # Fournisseur: UI > ENV > dÃ©faut pexels
> video_processor.py:1502:    BROLL_FETCH_PROVIDER = (_UI_SETTINGS.get('broll_fetch_provider') or 
os.getenv('AI_BROLL_FETCH_PROVIDER') or 'pexels')
  video_processor.py:1503:    # ClÃ©s API
  video_processor.py:1504:    PEXELS_API_KEY = _UI_SETTINGS.get('PEXELS_API_KEY') or 
os.getenv('PEXELS_API_KEY')
  video_processor.py:1505:    PIXABAY_API_KEY = _UI_SETTINGS.get('PIXABAY_API_KEY') or 
os.getenv('PIXABAY_API_KEY')
  video_processor.py:1506:    # ContrÃ´les de fetch
> video_processor.py:1507:    BROLL_FETCH_MAX_PER_KEYWORD = 
int(_UI_SETTINGS.get('broll_fetch_max_per_keyword') or os.getenv('BROLL_FETCH_MAX_PER_KEYWORD') or 25) 
 # CORRIGÃ‰: 12 â†’ 25
> video_processor.py:1508:    BROLL_FETCH_ALLOW_VIDEOS = 
_to_bool(_UI_SETTINGS.get('broll_fetch_allow_videos'), default=True) if 'broll_fetch_allow_videos' in 
_UI_SETTINGS else _to_bool(os.getenv('BROLL_FETCH_ALLOW_VIDEOS'), default=True)
> video_processor.py:1509:    BROLL_FETCH_ALLOW_IMAGES = 
_to_bool(_UI_SETTINGS.get('broll_fetch_allow_images'), default=False) if 'broll_fetch_allow_images' in 
_UI_SETTINGS else _to_bool(os.getenv('BROLL_FETCH_ALLOW_IMAGES'), default=False)
  video_processor.py:1510:    # Ã‰largir le pool par dÃ©faut: activer les images si non prÃ©cisÃ©
> video_processor.py:1511:    if 'broll_fetch_allow_images' not in _UI_SETTINGS and 
os.getenv('BROLL_FETCH_ALLOW_IMAGES') is None:
> video_processor.py:1512:        BROLL_FETCH_ALLOW_IMAGES = True
  video_processor.py:1513:    # Embeddings pour matching sÃ©mantique
> video_processor.py:1514:    BROLL_USE_EMBEDDINGS = 
_to_bool(_UI_SETTINGS.get('broll_use_embeddings'), default=True) if 'broll_use_embeddings' in 
_UI_SETTINGS else _to_bool(os.getenv('BROLL_USE_EMBEDDINGS'), default=True)
> video_processor.py:1515:    BROLL_EMBEDDING_MODEL = (_UI_SETTINGS.get('broll_embedding_model') or 
os.getenv('BROLL_EMBEDDING_MODEL') or 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
  video_processor.py:1516:    # Config contextuelle
> video_processor.py:1517:    CONTEXTUAL_CONFIG_PATH = Path(_UI_SETTINGS.get('contextual_broll_yml') 
or os.getenv('CONTEXTUAL_BROLL_YML') or 'config/contextual_broll.yml')
  video_processor.py:1518:
  video_processor.py:1519:    # Sortie et nettoyage
  video_processor.py:1520:    USE_HARDLINKS = _to_bool(_UI_SETTINGS.get('use_hardlinks'), 
default=True) if 'use_hardlinks' in _UI_SETTINGS else _to_bool(os.getenv('USE_HARDLINKS'), 
default=True)
> video_processor.py:1521:    BROLL_DELETE_AFTER_USE = 
_to_bool(_UI_SETTINGS.get('broll_delete_after_use'), default=True) if 'broll_delete_after_use' in 
_UI_SETTINGS else _to_bool(os.getenv('BROLL_DELETE_AFTER_USE') or 
os.getenv('AI_BROLL_PURGE_AFTER_USE'), default=True)
  video_processor.py:1522:    # ðŸš€ NOUVEAU: Forcer le nettoyage aprÃ¨s chaque vidÃ©o pour 
Ã©conomiser l'espace
> video_processor.py:1523:    BROLL_CLEANUP_PER_VIDEO = True  # Toujours activÃ© pour Ã©viter 
l'accumulation
> video_processor.py:1524:    BROLL_PURGE_AFTER_RUN = 
_to_bool(_UI_SETTINGS.get('broll_purge_after_run'), default=True) if 'broll_purge_after_run' in 
_UI_SETTINGS else _to_bool(os.getenv('BROLL_PURGE_AFTER_RUN') or 
os.getenv('AI_BROLL_PURGE_AFTER_RUN'), default=True)
  video_processor.py:1525:    # Brand kit
  video_processor.py:1526:    BRAND_KIT_ID = _UI_SETTINGS.get('brand_kit_id') or 
os.getenv('BRAND_KIT_ID') or 'default'
  video_processor.py:1527:    # Experimental FX (wipes/zoom/LUT etc.)
  video_processor.py:1528:    ENABLE_EXPERIMENTAL_FX = 
_to_bool(_UI_SETTINGS.get('enable_experimental_fx'), default=False) if 'enable_experimental_fx' in 
_UI_SETTINGS else _to_bool(os.getenv('ENABLE_EXPERIMENTAL_FX'), default=False)
  video_processor.py:1529:
  video_processor.py:1530:    # ðŸš€ NOUVEAU: Configuration du sÃ©lecteur B-roll gÃ©nÃ©rique
> video_processor.py:1531:    BROLL_SELECTOR_CONFIG_PATH = 
Path(_UI_SETTINGS.get('broll_selector_config') or os.getenv('BROLL_SELECTOR_CONFIG') or 
'config/broll_selector_config.yaml')
> video_processor.py:1532:    BROLL_SELECTOR_ENABLED = 
_to_bool(_UI_SETTINGS.get('broll_selector_enabled'), default=True) if 'broll_selector_enabled' in 
_UI_SETTINGS else _to_bool(os.getenv('BROLL_SELECTOR_ENABLED') or 
os.getenv('AI_BROLL_SELECTOR_ENABLED'), default=True)
  video_processor.py:1533:
  video_processor.py:1534:# ðŸš€ SUPPRIMÃ‰: Fonction _detect_local_llm obsolÃ¨te
  video_processor.py:1535:# RemplacÃ©e par le systÃ¨me LLM industriel qui gÃ¨re automatiquement la 
dÃ©tection
  video_processor.py:1536:
  video_processor.py:1537:# ðŸš€ SUPPRIMÃ‰: Ancien systÃ¨me LLM obsolÃ¨te remplacÃ© par le systÃ¨me 
industriel
> video_processor.py:1538:# Cette fonction utilisait l'ancien prompt complexe et causait des timeouts
  video_processor.py:1539:# Maintenant remplacÃ©e par le systÃ¨me LLM industriel dans 
generate_caption_and_hashtags
  video_processor.py:1540:# ðŸš€ SUPPRIMÃ‰: Reste de l'ancien systÃ¨me LLM obsolÃ¨te
  video_processor.py:1541:# Toute cette logique complexe est maintenant remplacÃ©e par le systÃ¨me 
industriel
  video_processor.py:1542:
  video_processor.py:1543:# === IA: Analyse mots-clÃ©s et prompts visuels pour guider le B-roll ===
  video_processor.py:1544:
> video_processor.py:1545:def extract_keywords_from_transcript_ai(transcript_segments: List[Dict]) -> 
Dict:
  video_processor.py:1546:    """Analyse simple: thÃ¨mes, occurrences et timestamps pour B-roll 
contextuel."""
> video_processor.py:1547:    keyword_categories = {
  video_processor.py:1548:        'money': ['money', 'cash', 'dollars', 'profit', 'revenue', 'income', 
'wealth'],
  video_processor.py:1549:        'business': ['business', 'company', 'startup', 'entrepreneur', 
'strategy'],
  video_processor.py:1550:        'technology': ['tech', 'software', 'app', 'digital', 'online', 'ai', 
'automation'],
  video_processor.py:1560:        'internal_dialogue': ['dialogue', 'conversation', 'thoughts', 
'thinking', 'mindset', 'beliefs', 'circuit']
  video_processor.py:1561:    }
  video_processor.py:1562:    full_text = ' '.join([(seg.get('text') or '').lower() for seg in 
transcript_segments])
> video_processor.py:1563:    detected_keywords: Dict[str, List[str]] = {}
  video_processor.py:1564:    timestamps_by_category: Dict[str, List[Dict]] = {}
> video_processor.py:1565:    for category, kws in keyword_categories.items():
> video_processor.py:1566:        detected_keywords[category] = []
  video_processor.py:1567:        timestamps_by_category[category] = []
  video_processor.py:1568:        for kw in kws:
  video_processor.py:1569:            if kw in full_text:
> video_processor.py:1570:                detected_keywords[category].append(kw)
  video_processor.py:1571:                for seg in transcript_segments:
  video_processor.py:1572:                    text = (seg.get('text') or '').lower()
  video_processor.py:1573:                    if kw in text:
  video_processor.py:1584:                        timestamps_by_category[category].append({
  video_processor.py:1585:                            'start': float(start_val),
  video_processor.py:1586:                            'end': float(end_val),
> video_processor.py:1587:                            'keyword': kw,
  video_processor.py:1588:                            'context': seg.get('text') or ''
  video_processor.py:1589:                        })
  video_processor.py:1590:    dominant_theme = 'business'
  video_processor.py:1591:    try:
> video_processor.py:1592:        dominant_theme = max(detected_keywords.items(), key=lambda x: 
len(x[1]))[0]
  video_processor.py:1593:    except Exception:
  video_processor.py:1594:        pass
  video_processor.py:1595:    # ðŸš€ CORRECTION CRITIQUE: Gestion robuste du timestamp final
  video_processor.py:1605:            total_duration = 0.0
  video_processor.py:1606:    
  video_processor.py:1607:    return {
> video_processor.py:1608:        'keywords': detected_keywords,
  video_processor.py:1609:        'timestamps': timestamps_by_category,
  video_processor.py:1610:        'dominant_theme': dominant_theme,
  video_processor.py:1611:        'total_duration': total_duration
  video_processor.py:1612:    }
  video_processor.py:1613:
  video_processor.py:1614:
> video_processor.py:1615:def generate_broll_prompts_ai(keyword_analysis: Dict) -> List[Dict]:
  video_processor.py:1616:    """Generate B-roll prompts using AI analysis."""
  video_processor.py:1617:    try:
> video_processor.py:1618:        # Extract main theme and keywords
> video_processor.py:1619:        main_theme = keyword_analysis.get('dominant_theme', 'general')  # 
ðŸš€ CORRECTION: ClÃ© correcte
> video_processor.py:1620:        keywords = keyword_analysis.get('keywords', {})
> video_processor.py:1621:        sentiment = keyword_analysis.get('sentiment', 0.0)
  video_processor.py:1622:        
  video_processor.py:1623:        # Generate context-aware prompts
  video_processor.py:1624:        prompts = []
  video_processor.py:1658:            ])
  video_processor.py:1659:        else:
  video_processor.py:1660:            # Generic prompts for other themes  
> video_processor.py:1661:            # ðŸš€ CORRECTION: keywords est un dict, pas une liste
> video_processor.py:1662:            if keywords and isinstance(keywords, dict):
  video_processor.py:1663:                # Extraire les premiers mots-clÃ©s de toutes les catÃ©gories
  video_processor.py:1664:                all_kws = []
> video_processor.py:1665:                for category_kws in keywords.values():
  video_processor.py:1666:                    if isinstance(category_kws, list):
  video_processor.py:1667:                        all_kws.extend(category_kws[:2])  # 2 par catÃ©gorie
> video_processor.py:1668:                base_keywords = all_kws[:3] if all_kws else [main_theme]
  video_processor.py:1669:            else:
> video_processor.py:1670:                base_keywords = [main_theme]
  video_processor.py:1671:                
> video_processor.py:1672:            for kw in base_keywords:
  video_processor.py:1673:                prompts.append(f"{main_theme} {kw}")
  video_processor.py:1674:        
  video_processor.py:1675:        # Add sentiment-based prompts
  video_processor.py:1685:        
  video_processor.py:1686:    except Exception as e:
  video_processor.py:1687:        print(f"âš ï¸ Erreur gÃ©nÃ©ration prompts AI: {e}")
> video_processor.py:1688:        # Fallback prompts
  video_processor.py:1689:        return ['general content', 'people working', 'modern technology']
  video_processor.py:1690:
  video_processor.py:1691:class VideoProcessor:
  video_processor.py:1696:    def __init__(self):
  video_processor.py:1697:        emit_dependency_status()
  video_processor.py:1698:        if os.getenv("FAST_TESTS") == "1":
> video_processor.py:1699:            self.whisper_model = object()
  video_processor.py:1700:        else:
> video_processor.py:1701:            self.whisper_model = whisper.load_model(Config.WHISPER_MODEL)
  video_processor.py:1702:        _setup_directories()
  video_processor.py:1703:        # Cache Ã©ventuel pour spaCy
> video_processor.py:1704:        self._spacy_model = None
  video_processor.py:1705:
  video_processor.py:1706:        if VideoProcessor._shared_llm_service is None:
  video_processor.py:1707:            try:
  video_processor.py:1710:                logger.warning("LLM service initialisation failed: %s", exc)
  video_processor.py:1711:        self._llm_service = VideoProcessor._shared_llm_service
  video_processor.py:1712:        self._core_last_run_used = False
> video_processor.py:1713:        self._last_broll_insert_count = 0
  video_processor.py:1714:        self._pipeline_config = PipelineConfigBundle()
  video_processor.py:1715:        meta_dir = Config.OUTPUT_FOLDER / 'meta'
> video_processor.py:1716:        log_file = meta_dir / 'broll_pipeline_events.jsonl'
> video_processor.py:1717:        self._broll_event_logger: Optional[JsonlLogger] = 
JsonlLogger(log_file)
> video_processor.py:1718:        self._broll_env_logged = False
  video_processor.py:1719:
> video_processor.py:1720:    def get_last_broll_insert_count(self) -> int:
  video_processor.py:1721:        """Return the number of B-roll clips inserted during the last run."""
  video_processor.py:1722:
> video_processor.py:1723:        return getattr(self, "_last_broll_insert_count", 0)
  video_processor.py:1724:
  video_processor.py:1725:    def _setup_directories(self):
  video_processor.py:1726:        """CrÃ©e les dossiers nÃ©cessaires"""
  video_processor.py:1741:            if not candidate.exists():
  video_processor.py:1742:                candidate.mkdir(parents=True, exist_ok=True)
  video_processor.py:1743:                return candidate
> video_processor.py:1744:        # Fallback timestamp
  video_processor.py:1745:        from datetime import datetime
  video_processor.py:1746:        ts = datetime.now().strftime('%Y%m%d-%H%M%S')
  video_processor.py:1747:        cand = root / f"{clip_stem}-{ts}"
  video_processor.py:1749:        return cand
  video_processor.py:1750:
  video_processor.py:1751:    def process_single_clip(self, clip_path: Path, *, enable_core: 
Optional[bool] = None, verbose: bool = False):
> video_processor.py:1752:        """Entry point used by the CLI wrapper with JSONL boot events."""
  video_processor.py:1753:        clip_path = Path(clip_path)
  video_processor.py:1754:        if not clip_path.exists():
  video_processor.py:1755:            raise FileNotFoundError(f"Source video not found: {clip_path}")
  video_processor.py:1758:        output_root = Config.OUTPUT_FOLDER
  video_processor.py:1759:        meta_dir = output_root / 'meta'
  video_processor.py:1760:        final_dir = output_root / 'final'
> video_processor.py:1761:        subtitled_dir = output_root / 'subtitled'
> video_processor.py:1762:        for folder in (meta_dir, final_dir, subtitled_dir):
  video_processor.py:1763:            folder.mkdir(parents=True, exist_ok=True)
  video_processor.py:1764:
  video_processor.py:1765:        session_id = f"{clip_path.stem}-{int(time.time() * 1000)}"
> video_processor.py:1766:        self._broll_env_logged = False
> video_processor.py:1767:        event_logger = self._get_broll_event_logger()
  video_processor.py:1768:
  video_processor.py:1769:        use_core = enable_core if enable_core is not None else 
_pipeline_core_fetcher_enabled()
  video_processor.py:1770:        logger.info('[CORE] Orchestrator %s', 'enabled' if use_core else 
'disabled (legacy path)')
  video_processor.py:1814:        in_memory_entries = getattr(event_logger, 'entries', None)
  video_processor.py:1815:        if isinstance(log_path, Path) and (not log_path.exists() or 
log_path.stat().st_size == 0):
  video_processor.py:1816:            if not in_memory_entries:
> video_processor.py:1817:                raise RuntimeError('[CORE] No JSONL events written for this 
run.')
  video_processor.py:1818:
  video_processor.py:1819:        return final_path
  video_processor.py:1820:
> video_processor.py:1821:    def _get_broll_event_logger(self):
> video_processor.py:1822:        global _GLOBAL_BROLL_EVENTS_LOGGER
  video_processor.py:1823:
> video_processor.py:1824:        logger_obj = _GLOBAL_BROLL_EVENTS_LOGGER
  video_processor.py:1825:        if logger_obj is None:
  video_processor.py:1826:            try:
  video_processor.py:1827:                base_dir = Path(getattr(Config, 'OUTPUT_FOLDER', 
Path('output')))
  video_processor.py:1828:            except Exception:
  video_processor.py:1829:                base_dir = Path('output')
> video_processor.py:1830:            events_path = base_dir / 'meta' / 'broll_pipeline_events.jsonl'
  video_processor.py:1831:            events_path.parent.mkdir(parents=True, exist_ok=True)
> video_processor.py:1832:            logger_obj = JsonlLogger(events_path)
> video_processor.py:1833:            _GLOBAL_BROLL_EVENTS_LOGGER = logger_obj
  video_processor.py:1834:
> video_processor.py:1835:        self._broll_event_logger = logger_obj
> video_processor.py:1836:        if not getattr(self, '_broll_env_logged', False):
  video_processor.py:1837:            try:
  video_processor.py:1838:                env_payload = {
> video_processor.py:1839:                    'event': 'broll_env_ready',
> video_processor.py:1840:                    'providers': _get_provider_names(self._pipeline_config),
  video_processor.py:1841:                }
  video_processor.py:1842:                logger_obj.log(env_payload)
> video_processor.py:1843:                self._broll_env_logged = True
  video_processor.py:1844:            except Exception:
  video_processor.py:1845:                pass
  video_processor.py:1846:        return logger_obj
  video_processor.py:1849:    def _maybe_use_pipeline_core(
  video_processor.py:1850:        self,
  video_processor.py:1851:        segments,
> video_processor.py:1852:        broll_keywords,
  video_processor.py:1853:        *,
> video_processor.py:1854:        subtitles,
  video_processor.py:1855:        input_path: Path,
  video_processor.py:1856:    ) -> Optional[Tuple[int, Optional[Path]]]:
  video_processor.py:1857:        """Attempt to run the pipeline_core orchestrator if configured.
  video_processor.py:1862:        pipeline can continue unchanged.
  video_processor.py:1863:        """
  video_processor.py:1864:
> video_processor.py:1865:        event_logger = self._get_broll_event_logger()
  video_processor.py:1866:        if not _pipeline_core_fetcher_enabled():
  video_processor.py:1867:            event_logger.log({'event': 'core_disabled', 'reason': 
'flag_disabled'})
  video_processor.py:1868:            self._core_last_run_used = False
  video_processor.py:1879:            self._core_last_run_used = False
  video_processor.py:1880:            return None
  video_processor.py:1881:
> video_processor.py:1882:        providers = getattr(fetcher_cfg, 'providers', None)
> video_processor.py:1883:        providers_list: List[Any]
  video_processor.py:1884:        misconfigured = False
  video_processor.py:1885:
> video_processor.py:1886:        if providers is None:
> video_processor.py:1887:            providers_list = []
  video_processor.py:1888:        else:
  video_processor.py:1889:            try:
> video_processor.py:1890:                providers_list = list(providers or ())
  video_processor.py:1891:            except TypeError:
  video_processor.py:1892:                misconfigured = True
> video_processor.py:1893:                providers_list = [providers]
  video_processor.py:1894:
  video_processor.py:1895:        if misconfigured:
  video_processor.py:1896:            logger.warning(
> video_processor.py:1897:                "pipeline_core fetcher misconfigured: expected iterable 
`fetcher.providers`, "
  video_processor.py:1898:                "got %s; falling back to legacy pipeline",
> video_processor.py:1899:                type(providers).__name__,
  video_processor.py:1900:            )
> video_processor.py:1901:            event_logger.log({'event': 'core_disabled', 'reason': 
'providers_misconfigured', 'provider_type': type(providers).__name__})
  video_processor.py:1902:            self._core_last_run_used = False
  video_processor.py:1903:            return None
  video_processor.py:1904:
> video_processor.py:1905:        if not providers_list:
  video_processor.py:1906:            logger.warning(
> video_processor.py:1907:                "pipeline_core fetcher enabled but no providers configured; 
falling back to legacy pipeline",
  video_processor.py:1908:            )
> video_processor.py:1909:            event_logger.log({'event': 'core_disabled', 'reason': 
'no_providers'})
  video_processor.py:1910:            self._core_last_run_used = False
  video_processor.py:1911:            return None
  video_processor.py:1912:
> video_processor.py:1913:        event_logger.log({'event': 'core_engaged', 'video': input_path.name, 
'providers': len(providers_list)})
  video_processor.py:1914:
> video_processor.py:1915:        inserted = self._insert_brolls_pipeline_core(
  video_processor.py:1916:            segments,
> video_processor.py:1917:            broll_keywords,
> video_processor.py:1918:            subtitles=subtitles,
  video_processor.py:1919:            input_path=input_path,
  video_processor.py:1920:        )
  video_processor.py:1921:        return inserted
  video_processor.py:1922:
> video_processor.py:1923:    def _insert_brolls_pipeline_core(
  video_processor.py:1924:        self,
  video_processor.py:1925:        segments,
> video_processor.py:1926:        broll_keywords,
  video_processor.py:1927:        *,
> video_processor.py:1928:        subtitles,
  video_processor.py:1929:        input_path: Path,
  video_processor.py:1930:    ) -> Tuple[int, Optional[Path], Dict[str, Any]]:
  video_processor.py:1931:        global SEEN_URLS, SEEN_PHASHES, SEEN_IDENTIFIERS
  video_processor.py:1933:        SEEN_PHASHES.clear()
  video_processor.py:1934:        SEEN_IDENTIFIERS.clear()
  video_processor.py:1935:        self._core_last_run_used = True
> video_processor.py:1936:        self._last_broll_insert_count = 0
  video_processor.py:1937:        self._core_last_timeline: List[CoreTimelineEntry] = []
  video_processor.py:1938:        self._core_last_render_path: Optional[Path] = None
> video_processor.py:1939:        logger.info("[BROLL] pipeline_core orchestrator engaged")
  video_processor.py:1940:        config_bundle = self._pipeline_config
  video_processor.py:1941:        orchestrator = FetcherOrchestrator(config_bundle.fetcher)
  video_processor.py:1942:        selection_cfg = config_bundle.selection
  video_processor.py:1943:        timeboxing_cfg = config_bundle.timeboxing
> video_processor.py:1944:        event_logger = self._get_broll_event_logger()
  video_processor.py:1945:        event_logger.log(
  video_processor.py:1946:            {
> video_processor.py:1947:                "event": "broll_session_start",
  video_processor.py:1948:                "segment": -1,
  video_processor.py:1949:                "total_segments": len(segments),
  video_processor.py:1950:                "llm_healthy": bool(self._llm_service),
  video_processor.py:1958:            dyn_ctx = getattr(self, '_dyn_context', None)
  video_processor.py:1959:            dom_name, dom_conf = _choose_dynamic_domain(dyn_ctx) if 
ENABLE_SELECTOR_DYNAMIC_DOMAIN else (None, None)
  video_processor.py:1960:            dom_source = 'dyn' if dom_name else 'none'
> video_processor.py:1961:            sk = list(getattr(self, '_selector_keywords', []))
> video_processor.py:1962:            fk = list(getattr(self, '_fetch_keywords', []))
  video_processor.py:1963:            report = {
  video_processor.py:1964:                'video_stem': Path(input_path).stem,
  video_processor.py:1965:                'effective_domain': dom_name,
  video_processor.py:1966:                'domain_confidence': dom_conf,
  video_processor.py:1967:                'domain_source': dom_source,
> video_processor.py:1968:                'selector_keywords': sk,
> video_processor.py:1969:                'fetch_keywords': fk,
  video_processor.py:1970:                'segments': [],
  video_processor.py:1971:            }
  video_processor.py:1972:        except Exception:
  video_processor.py:2021:        if not segments:
  video_processor.py:2022:            event_logger.log({'event': 'core_no_segments', 'reason': 
'no_valid_segments', 'video': input_path.name})
  video_processor.py:2023:            self._core_last_run_used = False
> video_processor.py:2024:            self._last_broll_insert_count = 0
  video_processor.py:2025:            return 0, None
  video_processor.py:2026:
> video_processor.py:2027:        fetch_timeout = max((timeboxing_cfg.fetch_rank_ms or 0) / 1000.0, 
0.0)
  video_processor.py:2028:
  video_processor.py:2029:        selected_assets: List[Dict[str, Any]] = []
  video_processor.py:2030:        provider_counter: Counter[str] = Counter()
  video_processor.py:2056:            dyn_ctx = getattr(self, '_dyn_context', {})
  video_processor.py:2057:            dyn_language = str(dyn_ctx.get('language') or 
'').strip().lower() if isinstance(dyn_ctx, dict) else ''
  video_processor.py:2058:
> video_processor.py:2059:            segment_keywords = self._derive_segment_keywords(segment, 
broll_keywords)
> video_processor.py:2060:            keyword_terms = _dedupe_queries(segment_keywords, 
cap=SEGMENT_REFINEMENT_MAX_TERMS)
  video_processor.py:2061:
  video_processor.py:2062:            try:
> video_processor.py:2063:                brief_queries, brief_keywords = _segment_brief_terms(dyn_ctx 
or {}, idx)
  video_processor.py:2064:            except Exception:
> video_processor.py:2065:                brief_queries, brief_keywords = [], []
  video_processor.py:2066:
  video_processor.py:2067:            metadata_payload = {}
  video_processor.py:2068:            try:
  video_processor.py:2080:                metadata_queries_raw = metadata_payload.get('queries') or []
  video_processor.py:2081:                llm_queries = _relaxed_normalise_terms(metadata_queries_raw, 
metadata_query_cap)
  video_processor.py:2082:                if not llm_queries:
> video_processor.py:2083:                    metadata_status = 'fallback'
  video_processor.py:2084:
  video_processor.py:2085:            if metadata_status != 'ok':
  video_processor.py:2086:                transcript_source = getattr(segment, 'text', '') or ''
  video_processor.py:2087:                transcript_tokens = 
_split_basic_latin_runs(transcript_source.lower(), keep={"'", "-"})
  video_processor.py:2088:                llm_queries = _normalize_queries(
> video_processor.py:2089:                    list(metadata_payload.get('broll_keywords') or []),
  video_processor.py:2090:                    transcript_tokens,
  video_processor.py:2091:                    max_queries=metadata_query_cap,
  video_processor.py:2092:                )
  video_processor.py:2096:            query_source = 'segment_brief'
  video_processor.py:2097:
  video_processor.py:2098:            llm_attempted = False
> video_processor.py:2099:            fallback_source_label: Optional[str] = None
  video_processor.py:2100:
  video_processor.py:2101:            if getattr(self, '_llm_service', None):
  video_processor.py:2102:                llm_attempted = True
  video_processor.py:2127:                    hint_source = 'llm_hint'
  video_processor.py:2128:                if hint_terms:
  video_processor.py:2129:                    logger.info(
> video_processor.py:2130:                        "[BROLL][LLM] segment=%.2f-%.2f queries=%s 
(source=%s)",
  video_processor.py:2131:                        float(getattr(segment, 'start', 0.0) or 0.0),
  video_processor.py:2132:                        float(getattr(segment, 'end', 0.0) or 0.0),
  video_processor.py:2133:                        hint_terms,
  video_processor.py:2135:                    )
  video_processor.py:2136:                elif hint_payload:
  video_processor.py:2137:                    logger.info(
> video_processor.py:2138:                        "[BROLL][LLM] segment=%.2f-%.2f queries=%s 
(source=%s)",
  video_processor.py:2139:                        float(getattr(segment, 'start', 0.0) or 0.0),
  video_processor.py:2140:                        float(getattr(segment, 'end', 0.0) or 0.0),
  video_processor.py:2141:                        hint_payload,
  video_processor.py:2145:                hint_source = llm_source_label
  video_processor.py:2146:
  video_processor.py:2147:            if llm_attempted and not hint_payload and getattr(self, 
'_llm_service', None):
> video_processor.py:2148:                fallback_terms: Sequence[str] = []
> video_processor.py:2149:                provider_fallback = getattr(self._llm_service, 
'provider_fallback_queries', None)
> video_processor.py:2150:                if callable(provider_fallback):
  video_processor.py:2151:                    try:
> video_processor.py:2152:                        fallback_terms, fallback_source_label = 
provider_fallback(
  video_processor.py:2153:                            getattr(segment, 'text', '') or '',
  video_processor.py:2154:                            max_items=metadata_query_cap,
  video_processor.py:2155:                            language=dyn_language or None,
  video_processor.py:2156:                        )
  video_processor.py:2157:                    except Exception:
> video_processor.py:2158:                        fallback_terms, fallback_source_label = [], None
  video_processor.py:2159:
> video_processor.py:2160:                normalised_fallback = 
_relaxed_normalise_terms(fallback_terms, metadata_query_cap)
> video_processor.py:2161:                if normalised_fallback:
> video_processor.py:2162:                    llm_queries = _dedupe_queries(list(llm_queries) + 
normalised_fallback, cap=metadata_query_cap)
> video_processor.py:2163:                    if fallback_source_label and fallback_source_label != 
'none':
> video_processor.py:2164:                        llm_source_label = fallback_source_label
  video_processor.py:2165:                if not hint_payload:
  video_processor.py:2166:                    llm_healthy = False
  video_processor.py:2167:
  video_processor.py:2172:                if dyn_language in ('', 'en'):
  video_processor.py:2173:                    queries = enforce_fetch_language(queries, dyn_language 
or None)
  video_processor.py:2174:            else:
> video_processor.py:2175:                selector_keywords = list(getattr(self, '_selector_keywords', 
[]))
  video_processor.py:2176:                queries, query_source = _merge_segment_query_sources(
  video_processor.py:2177:                    segment_text=getattr(segment, 'text', '') or '',
  video_processor.py:2178:                    llm_queries=llm_queries,
  video_processor.py:2179:                    brief_queries=brief_queries,
> video_processor.py:2180:                    brief_keywords=brief_keywords,
> video_processor.py:2181:                    segment_keywords=keyword_terms,
> video_processor.py:2182:                    selector_keywords=selector_keywords,
  video_processor.py:2183:                    cap=SEGMENT_REFINEMENT_MAX_TERMS,
  video_processor.py:2184:                )
  video_processor.py:2185:
  video_processor.py:2190:                    queries = enforce_fetch_language(queries, dyn_language 
or None)
  video_processor.py:2191:
  video_processor.py:2192:                if not queries:
> video_processor.py:2193:                    fallback_terms = _build_transcript_fallback_terms(
  video_processor.py:2194:                        getattr(segment, 'text', '') or '',
> video_processor.py:2195:                        keyword_terms,
  video_processor.py:2196:                        limit=max(1, SEGMENT_REFINEMENT_MAX_TERMS),
  video_processor.py:2197:                    )
> video_processor.py:2198:                    queries = _relaxed_normalise_terms(fallback_terms, 
max(1, SEGMENT_REFINEMENT_MAX_TERMS))
  video_processor.py:2199:                    if dyn_language in ('', 'en'):
  video_processor.py:2200:                        queries = enforce_fetch_language(queries, 
dyn_language or None)
  video_processor.py:2201:                    if queries:
> video_processor.py:2202:                        query_source = 'transcript_fallback'
  video_processor.py:2203:
  video_processor.py:2204:            query_source_counter[query_source] += 1
  video_processor.py:2205:
  video_processor.py:2206:            try:
  video_processor.py:2207:                event_logger.log({
> video_processor.py:2208:                    'event': 'broll_segment_queries',
  video_processor.py:2209:                    'segment': idx,
  video_processor.py:2210:                    'queries': queries,
  video_processor.py:2211:                    'source': query_source,
  video_processor.py:2221:                    pass
  video_processor.py:2222:
  video_processor.py:2223:            if not queries:
> video_processor.py:2224:                log_broll_decision(
  video_processor.py:2225:                    event_logger,
  video_processor.py:2226:                    segment_idx=idx,
  video_processor.py:2227:                    start=segment.start,
  video_processor.py:2236:                    provider=None,
  video_processor.py:2237:                    latency_ms=0,
  video_processor.py:2238:                    llm_healthy=llm_healthy,
> video_processor.py:2239:                    reject_reasons=['no_keywords'],
  video_processor.py:2240:                    queries=queries,
  video_processor.py:2241:                )
  video_processor.py:2242:                continue
  video_processor.py:2243:
  video_processor.py:2244:            segments_with_queries += 1
  video_processor.py:2245:            try:
> video_processor.py:2246:                print(f"[BROLL] segment #{idx}: queries={queries} 
(source={query_source})")
  video_processor.py:2247:            except Exception:
  video_processor.py:2248:                pass
  video_processor.py:2249:
  video_processor.py:2261:                    filters=filters,
  video_processor.py:2262:                )
  video_processor.py:2263:
> video_processor.py:2264:            # Timebox the per-segment fetch using the strictest timeout
> video_processor.py:2265:            eff_timeout = fetch_timeout
> video_processor.py:2266:            # Config timeboxing: per-request timeout if available
  video_processor.py:2267:            try:
> video_processor.py:2268:                per_request = 
float(getattr(self._pipeline_config.timeboxing, "request_timeout_s", SEGMENT_FETCH_TIMEOUT_S) or 
SEGMENT_FETCH_TIMEOUT_S)
  video_processor.py:2269:            except Exception:
> video_processor.py:2270:                per_request = SEGMENT_FETCH_TIMEOUT_S
  video_processor.py:2271:            # Environment guard
  video_processor.py:2272:            try:
> video_processor.py:2273:                guard = float(SEGMENT_FETCH_TIMEOUT_S)
  video_processor.py:2274:            except Exception:
> video_processor.py:2275:                guard = SEGMENT_FETCH_TIMEOUT_S
  video_processor.py:2276:            # Start from a positive base if none set
> video_processor.py:2277:            if not eff_timeout or eff_timeout <= 0.0:
> video_processor.py:2278:                eff_timeout = per_request
> video_processor.py:2279:            # Apply the strictest of all timeouts
> video_processor.py:2280:            eff_timeout = min(v for v in (eff_timeout, per_request, guard) 
if v and v > 0.0)
  video_processor.py:2281:
  video_processor.py:2282:            fetch_start = time.perf_counter()
> video_processor.py:2283:            candidates = run_with_timeout(_do_fetch, eff_timeout) if 
eff_timeout else _do_fetch()
  video_processor.py:2284:            fetch_latency_ms = int((time.perf_counter() - fetch_start) * 
1000)
  video_processor.py:2285:            try:
> video_processor.py:2286:                ev = self._get_broll_event_logger()
  video_processor.py:2287:                if ev:
  video_processor.py:2288:                    ev.log({
> video_processor.py:2289:                        "event": "broll_segment_fetch_latency",
  video_processor.py:2290:                        "segment": idx,
  video_processor.py:2291:                        "latency_ms": fetch_latency_ms,
  video_processor.py:2292:                        "query_count": len(queries or []),
> video_processor.py:2293:                        "timeout_s": eff_timeout,
  video_processor.py:2294:                    })
  video_processor.py:2295:            except Exception:
  video_processor.py:2296:                pass
  video_processor.py:2335:
  video_processor.py:2336:            eligible_records = [rec for rec in filter_pass_records if not 
rec['below_threshold']]
  video_processor.py:2337:            best_record = None
> video_processor.py:2338:            fallback_record = None
  video_processor.py:2339:            if eligible_records:
  video_processor.py:2340:                best_record = max(eligible_records, key=lambda rec: 
rec['score'])
  video_processor.py:2341:            elif filter_pass_records:
> video_processor.py:2342:                fallback_record = max(filter_pass_records, key=lambda rec: 
rec['score'])
  video_processor.py:2343:                if forced_keep_allowed and (forced_keep_remaining is None or 
forced_keep_remaining > 0):
> video_processor.py:2344:                    best_record = fallback_record
  video_processor.py:2345:                    forced_keep = True
  video_processor.py:2346:                else:
  video_processor.py:2347:                    reason = 'disabled' if not forced_keep_allowed else 
'exhausted'
> video_processor.py:2348:                    fallback_candidate = fallback_record.get('candidate') if 
isinstance(fallback_record, dict) else None
> video_processor.py:2349:                    fallback_provider = fallback_record.get('provider') if 
isinstance(fallback_record, dict) else None
> video_processor.py:2350:                    fallback_url = getattr(fallback_candidate, 'url', None)
  video_processor.py:2351:                    logger.info(
> video_processor.py:2352:                        "[BROLL] forced-keep fallback skipped for segment %s 
(reason=%s, provider=%s, url=%s)",
  video_processor.py:2353:                        idx,
  video_processor.py:2354:                        reason,
> video_processor.py:2355:                        fallback_provider,
> video_processor.py:2356:                        fallback_url,
  video_processor.py:2357:                    )
  video_processor.py:2358:                    try:
  video_processor.py:2359:                        if event_logger:
  video_processor.py:2362:                                    'event': 'forced_keep_skipped',
  video_processor.py:2363:                                    'segment': idx,
  video_processor.py:2364:                                    'reason': reason,
> video_processor.py:2365:                                    'provider': fallback_provider,
> video_processor.py:2366:                                    'url': fallback_url,
  video_processor.py:2367:                                }
  video_processor.py:2368:                            )
  video_processor.py:2369:                    except Exception:
  video_processor.py:2411:                            score_payload = None
  video_processor.py:2412:                        event_logger.log(
  video_processor.py:2413:                            {
> video_processor.py:2414:                                'event': 'broll_candidate_evaluated',
  video_processor.py:2415:                                'segment': idx,
  video_processor.py:2416:                                'provider': summary_entry['provider'],
  video_processor.py:2417:                                'url': candidate_url,
  video_processor.py:2439:                        pass
  video_processor.py:2440:
  video_processor.py:2441:            if forced_keep and best_candidate:
> video_processor.py:2442:                reject_reason_counts['fallback_low_score'] += 1
> video_processor.py:2443:                reject_reasons.append('fallback_low_score')
  video_processor.py:2444:                if forced_keep_remaining is not None:
  video_processor.py:2445:                    forced_keep_remaining = max(0, forced_keep_remaining - 1)
  video_processor.py:2446:                forced_keep_consumed += 1
  video_processor.py:2451:                    else 'n/a'
  video_processor.py:2452:                )
  video_processor.py:2453:                logger.info(
> video_processor.py:2454:                    "[BROLL] forced-keep fallback used for segment %s 
(provider=%s, url=%s, score=%s, remaining=%s)",
  video_processor.py:2455:                    idx,
  video_processor.py:2456:                    best_provider,
  video_processor.py:2457:                    getattr(best_candidate, 'url', None),
  video_processor.py:2507:
  video_processor.py:2508:            latency_ms = int((time.perf_counter() - start_time) * 1000)
  video_processor.py:2509:            total_latency_ms += latency_ms
> video_processor.py:2510:            log_broll_decision(
  video_processor.py:2511:                event_logger,
  video_processor.py:2512:                segment_idx=idx,
  video_processor.py:2513:                start=segment.start,
  video_processor.py:2544:        except Exception:
  video_processor.py:2545:            pass
  video_processor.py:2546:
> video_processor.py:2547:        log_broll_decision(
  video_processor.py:2548:            event_logger,
  video_processor.py:2549:            segment_idx=-1,
  video_processor.py:2550:            start=0.0,
  video_processor.py:2570:        refined_ratio = (query_source_counter.get('segment_brief', 0) / 
total_segments) if total_segments else 0.0
  video_processor.py:2571:
  video_processor.py:2572:        summary_payload: Dict[str, Any] = {
> video_processor.py:2573:            'event': 'broll_summary',
  video_processor.py:2574:            'segments': total_segments,
  video_processor.py:2575:            'inserted': 0,
  video_processor.py:2576:            'selection_rate': 0.0,
  video_processor.py:2577:            'selected_segments': [],
> video_processor.py:2578:            'avg_broll_duration': 0.0,
> video_processor.py:2579:            'broll_per_min': 0.0,
  video_processor.py:2580:            'avg_latency_ms': round(avg_latency, 1) if avg_latency else 0.0,
  video_processor.py:2581:            'refined_ratio': round(refined_ratio, 4),
  video_processor.py:2582:            'provider_mix': {},
> video_processor.py:2583:            'providers_used': [],
  video_processor.py:2584:            'query_source_counts': dict(query_source_counter),
  video_processor.py:2585:            'total_url_dedup_hits': total_url_dedup_hits,
  video_processor.py:2586:            'total_phash_dedup_hits': total_phash_dedup_hits,
  video_processor.py:2603:            timeline_entries: List[CoreTimelineEntry] = []
  video_processor.py:2604:            download_dir: Optional[Path]
  video_processor.py:2605:            try:
> video_processor.py:2606:                download_dir = Config.TEMP_FOLDER / 'with_broll_core' / 
Path(input_path).stem
  video_processor.py:2607:                download_dir.mkdir(parents=True, exist_ok=True)
  video_processor.py:2608:            except Exception as exc:
> video_processor.py:2609:                logger.warning('[BROLL] unable to prepare core download dir: 
%s', exc)
  video_processor.py:2610:                download_dir = None
  video_processor.py:2611:
  video_processor.py:2612:            for order, asset in enumerate(selected_assets):
  video_processor.py:2657:                )
  video_processor.py:2658:
  video_processor.py:2659:            if timeline_entries:
> video_processor.py:2660:                timeline_entries, pending_seen_updates = 
_apply_broll_invariants_to_core_entries(
  video_processor.py:2661:                    timeline_entries,
  video_processor.py:2662:                    seen_updates=pending_seen_updates,
  video_processor.py:2663:                )
  video_processor.py:2669:                if download_dir is not None:
  video_processor.py:2670:                    try:
  video_processor.py:2671:                        manifest_payload = [entry.to_dict() for entry in 
timeline_entries]
> video_processor.py:2672:                        manifest_path = download_dir / 'timeline.json'
  video_processor.py:2673:                        with manifest_path.open('w', encoding='utf-8') as 
handle:
> video_processor.py:2674:                            json.dump(manifest_payload, handle, 
ensure_ascii=False, indent=2)
  video_processor.py:2675:                    except Exception as exc:
> video_processor.py:2676:                        logger.debug('[BROLL] failed to persist core 
timeline manifest: %s', exc)
  video_processor.py:2677:
  video_processor.py:2678:                candidate_entries = list(timeline_entries)
> video_processor.py:2679:                render_candidate = 
self._render_core_broll_timeline(Path(input_path), timeline_entries)
  video_processor.py:2680:                if render_candidate:
  video_processor.py:2681:                    render_path = render_candidate
  video_processor.py:2682:                    self._core_last_render_path = render_path
  video_processor.py:2706:                    pass
  video_processor.py:2707:
  video_processor.py:2708:        final_inserted = len(materialized_entries)
> video_processor.py:2709:        self._last_broll_insert_count = final_inserted
  video_processor.py:2710:
  video_processor.py:2711:        if final_inserted > 0:
  video_processor.py:2712:            final_segments = [entry.segment_index for entry in 
materialized_entries]
  video_processor.py:2713:            durations = [entry.duration for entry in materialized_entries]
> video_processor.py:2714:            providers = Counter(str(entry.provider or 'unknown') for entry 
in materialized_entries)
  video_processor.py:2715:        else:
  video_processor.py:2716:            final_segments = []
  video_processor.py:2717:            durations = []
> video_processor.py:2718:            providers = Counter()
  video_processor.py:2719:
  video_processor.py:2720:        summary_event_payload: Optional[Dict[str, Any]] = None
  video_processor.py:2721:
  video_processor.py:2722:        try:
  video_processor.py:2723:            selection_rate = (final_inserted / total_segments) if 
total_segments else 0.0
  video_processor.py:2724:            avg_duration = (sum(durations) / len(durations)) if durations 
else 0.0
> video_processor.py:2725:            broll_per_min = (final_inserted / (total_duration / 60.0)) if 
total_duration > 0 else 0.0
> video_processor.py:2726:            provider_mix = {k: v for k, v in sorted(providers.items()) if v 
> 0}
  video_processor.py:2727:
  video_processor.py:2728:            overlay_paths = [entry.path for entry in materialized_entries if 
getattr(entry, 'path', None)]
  video_processor.py:2729:            overlays_exist = bool(overlay_paths) and all(Path(path).exists() 
for path in overlay_paths)
  video_processor.py:2749:                    'inserted': final_inserted,
  video_processor.py:2750:                    'selection_rate': round(selection_rate, 4),
  video_processor.py:2751:                    'selected_segments': final_segments,
> video_processor.py:2752:                    'avg_broll_duration': round(avg_duration, 3) if 
durations else 0.0,
> video_processor.py:2753:                    'broll_per_min': round(broll_per_min, 3) if 
broll_per_min else 0.0,
  video_processor.py:2754:                    'provider_mix': provider_mix,
> video_processor.py:2755:                    'providers_used': sorted(provider_mix.keys()),
  video_processor.py:2756:                    'render_ok': render_ok_flag,
  video_processor.py:2757:                }
  video_processor.py:2758:            )
  video_processor.py:2759:            summary_event_payload = {k: v for k, v in 
summary_payload.items() if v is not None}
  video_processor.py:2760:
> video_processor.py:2761:            providers_display = ", ".join(f"{k}:{v}" for k, v in 
provider_mix.items()) or "none"
  video_processor.py:2762:            render_ok_value = summary_payload.get('render_ok')
  video_processor.py:2763:            icon = "ðŸ“Š" if render_ok_value else "âš ï¸"
  video_processor.py:2764:            suffix = ""
  video_processor.py:2768:                suffix = " (rendu indisponible)"
  video_processor.py:2769:            print(
  video_processor.py:2770:                f"    {icon} B-roll sÃ©lectionnÃ©s: 
{final_inserted}/{total_segments} "
> video_processor.py:2771:                f"({selection_rate * 100:.1f}%); 
providers={providers_display}{suffix}"
  video_processor.py:2772:            )
  video_processor.py:2773:        except Exception:
  video_processor.py:2774:            pass
  video_processor.py:2779:            except Exception:
  video_processor.py:2780:                pass
  video_processor.py:2781:
> video_processor.py:2782:        # Persist compact selection report next to JSONL
  video_processor.py:2783:        try:
  video_processor.py:2784:            ENABLE_SELECTION_REPORT = os.getenv("ENABLE_SELECTION_REPORT", 
"true").lower() not in {"0","false","no"}
  video_processor.py:2785:        except Exception:
  video_processor.py:2791:                report['selection_rate'] = round((seg_sel / seg_total), 3) 
if seg_total else 0.0
  video_processor.py:2792:                meta_dir = Config.OUTPUT_FOLDER / 'meta'
  video_processor.py:2793:                meta_dir.mkdir(parents=True, exist_ok=True)
> video_processor.py:2794:                name = f"selection_report_{report.get('video_stem') or 
'clip'}.json"
  video_processor.py:2795:                out_path = meta_dir / name
  video_processor.py:2796:                with open(out_path, 'w', encoding='utf-8') as f:
> video_processor.py:2797:                    json.dump(report, f, ensure_ascii=False, indent=2)
  video_processor.py:2798:                print(f"[REPORT] wrote {out_path}")
  video_processor.py:2799:            except Exception as e:
  video_processor.py:2800:                print(f"[REPORT] failed: {e}")
  video_processor.py:2821:        destination = download_dir / filename
  video_processor.py:2822:
  video_processor.py:2823:        try:
> video_processor.py:2824:            response = requests.get(str(url), stream=True, timeout=15)
  video_processor.py:2825:            response.raise_for_status()
  video_processor.py:2826:            with open(destination, 'wb') as fh:
  video_processor.py:2827:                for chunk in response.iter_content(chunk_size=1024 * 8):
  video_processor.py:2828:                    if chunk:
  video_processor.py:2829:                        fh.write(chunk)
  video_processor.py:2830:        except Exception as exc:
> video_processor.py:2831:            logger.warning('[BROLL] failed to download %s: %s', url, exc)
  video_processor.py:2832:            try:
  video_processor.py:2833:                if destination.exists():
  video_processor.py:2834:                    destination.unlink()
  video_processor.py:2835:            except Exception:
  video_processor.py:2836:                pass
  video_processor.py:2837:            try:
> video_processor.py:2838:                event_logger = self._get_broll_event_logger()
  video_processor.py:2839:            except Exception:
  video_processor.py:2840:                event_logger = None
  video_processor.py:2841:            provider = getattr(candidate, 'provider', None)
  video_processor.py:2843:                try:
  video_processor.py:2844:                    event_logger.log(
  video_processor.py:2845:                        {
> video_processor.py:2846:                            'event': 'broll_asset_download_failed',
  video_processor.py:2847:                            'provider': provider,
  video_processor.py:2848:                            'url': url,
  video_processor.py:2849:                            'path': str(destination),
  video_processor.py:2856:            return None
  video_processor.py:2857:
  video_processor.py:2858:        try:
> video_processor.py:2859:            event_logger = self._get_broll_event_logger()
  video_processor.py:2860:        except Exception:
  video_processor.py:2861:            event_logger = None
  video_processor.py:2862:        if event_logger is not None:
  video_processor.py:2864:            try:
  video_processor.py:2865:                event_logger.log(
  video_processor.py:2866:                    {
> video_processor.py:2867:                        'event': 'broll_asset_downloaded',
  video_processor.py:2868:                        'provider': provider,
  video_processor.py:2869:                        'url': url,
  video_processor.py:2870:                        'path': str(destination),
  video_processor.py:2876:
  video_processor.py:2877:        return destination
  video_processor.py:2878:
> video_processor.py:2879:    def _render_core_broll_timeline(
  video_processor.py:2880:        self,
  video_processor.py:2881:        input_path: Path,
  video_processor.py:2882:        timeline: Sequence[Union[CoreTimelineEntry, Dict[str, Any]]],
  video_processor.py:2884:        """Render a simple composite video using the downloaded core 
assets."""
  video_processor.py:2885:
  video_processor.py:2886:        try:
> video_processor.py:2887:            event_logger = self._get_broll_event_logger()
  video_processor.py:2888:        except Exception:
  video_processor.py:2889:            event_logger = None
  video_processor.py:2890:
  video_processor.py:2893:                try:
  video_processor.py:2894:                    event_logger.log(
  video_processor.py:2895:                        {
> video_processor.py:2896:                            'event': 'broll_timeline_failed',
  video_processor.py:2897:                            'clips': 0,
  video_processor.py:2898:                            'output': None,
  video_processor.py:2899:                            'reason': 'empty_timeline',
  video_processor.py:2940:                try:
  video_processor.py:2941:                    event_logger.log(
  video_processor.py:2942:                        {
> video_processor.py:2943:                            'event': 'broll_timeline_failed',
  video_processor.py:2944:                            'clips': 0,
  video_processor.py:2945:                            'output': None,
  video_processor.py:2946:                            'reason': 'no_valid_clips',
  video_processor.py:2950:                    pass
  video_processor.py:2951:            return None
  video_processor.py:2952:
> video_processor.py:2953:        output_dir = Config.TEMP_FOLDER / 'with_broll_core'
  video_processor.py:2954:        output_dir.mkdir(parents=True, exist_ok=True)
  video_processor.py:2955:        output_path = self._unique_path(
  video_processor.py:2956:            output_dir,
> video_processor.py:2957:            f"with_broll_core_{Path(input_path).stem}",
  video_processor.py:2958:            '.mp4',
  video_processor.py:2959:        )
  video_processor.py:2960:
  video_processor.py:2995:                    try:
  video_processor.py:2996:                        event_logger.log(
  video_processor.py:2997:                            {
> video_processor.py:2998:                                'event': 'broll_timeline_rendered',
  video_processor.py:2999:                                'clips': clip_count,
  video_processor.py:3000:                                'count': clip_count,
  video_processor.py:3001:                                'duration_s': round(total_timeline_duration, 
3),
  video_processor.py:3007:                        pass
  video_processor.py:3008:                return output_path
  video_processor.py:3009:            except Exception as exc:
> video_processor.py:3010:                logger.warning('[BROLL] core renderer (pipeline renderer) 
failed: %s', exc)
  video_processor.py:3011:                try:
  video_processor.py:3012:                    if output_path.exists():
  video_processor.py:3013:                        output_path.unlink()
  video_processor.py:3061:
  video_processor.py:3062:                    if crop_meta and event_logger is not None:
  video_processor.py:3063:                        crop_payload: Dict[str, Any] = {
> video_processor.py:3064:                            'event': 'broll_crop_applied',
  video_processor.py:3065:                            'path': str(entry.path),
  video_processor.py:3066:                            'segment': entry.segment_index,
  video_processor.py:3067:                            'src_w': int(crop_meta.get('source_width') or 
ov_w),
  video_processor.py:3128:                    try:
  video_processor.py:3129:                        event_logger.log(
  video_processor.py:3130:                            {
> video_processor.py:3131:                                'event': 'broll_timeline_rendered',
  video_processor.py:3132:                                'clips': clip_count,
  video_processor.py:3133:                                'count': clip_count,
  video_processor.py:3134:                                'duration_s': round(total_timeline_duration, 
3),
  video_processor.py:3140:                        pass
  video_processor.py:3141:                return output_path
  video_processor.py:3142:        except Exception as exc:
> video_processor.py:3143:            logger.warning('[BROLL] core renderer failed: %s', exc)
  video_processor.py:3144:            last_error = str(exc)
  video_processor.py:3145:
  video_processor.py:3146:        if event_logger is not None:
  video_processor.py:3147:            try:
  video_processor.py:3148:                event_logger.log(
  video_processor.py:3149:                    {
> video_processor.py:3150:                        'event': 'broll_timeline_failed',
  video_processor.py:3151:                        'clips': clip_count,
  video_processor.py:3152:                        'output': str(output_path),
  video_processor.py:3153:                        'reason': last_error,
  video_processor.py:3207:            inserted = _coerce_int(
  video_processor.py:3208:                result.get('inserted')
  video_processor.py:3209:                or result.get('count')
> video_processor.py:3210:                or result.get('broll_inserted_count')
  video_processor.py:3211:            ) or inserted
  video_processor.py:3212:            _maybe_update_from_payload(result)
  video_processor.py:3213:        else:
> video_processor.py:3214:            if hasattr(result, 'broll_inserted_count'):
> video_processor.py:3215:                inserted = _coerce_int(getattr(result, 
'broll_inserted_count'))
  video_processor.py:3216:            if hasattr(result, 'final_export_path'):
  video_processor.py:3217:                render_path = self._coerce_core_path(getattr(result, 
'final_export_path'))
  video_processor.py:3218:            if hasattr(result, 'to_dict'):
  video_processor.py:3222:                    as_dict = None
  video_processor.py:3223:                if isinstance(as_dict, dict):
  video_processor.py:3224:                    inserted = inserted or _coerce_int(
> video_processor.py:3225:                        as_dict.get('broll_inserted_count')
  video_processor.py:3226:                        or as_dict.get('inserted')
  video_processor.py:3227:                    )
  video_processor.py:3228:                    _maybe_update_from_payload(as_dict)
  video_processor.py:3370:        try:
  video_processor.py:3371:            from urllib.request import urlopen  # type: ignore
  video_processor.py:3372:
> video_processor.py:3373:            with urlopen(str(url), timeout=20) as response:  # nosec - 
controlled provider URLs
  video_processor.py:3374:                with destination.open('wb') as handle:
  video_processor.py:3375:                    while True:
  video_processor.py:3376:                        chunk = response.read(65536)
  video_processor.py:3379:                        handle.write(chunk)
  video_processor.py:3380:            return destination
  video_processor.py:3381:        except Exception as exc:
> video_processor.py:3382:            logger.warning('[BROLL] failed to download core asset (%s): %s', 
url, exc)
  video_processor.py:3383:            try:
  video_processor.py:3384:                if destination.exists():
  video_processor.py:3385:                    destination.unlink()
  video_processor.py:3395:        if not plan:
  video_processor.py:3396:            return None
  video_processor.py:3397:
> video_processor.py:3398:        download_dir = Config.TEMP_FOLDER / 'with_broll_core' / 
Path(input_path).stem
  video_processor.py:3399:        timeline_entries: List[CoreTimelineEntry] = []
  video_processor.py:3400:        cached_urls: Dict[str, Path] = {}
  video_processor.py:3401:
  video_processor.py:3451:            )
  video_processor.py:3452:
  video_processor.py:3453:        if timeline_entries:
> video_processor.py:3454:            timeline_entries, _ = 
_apply_broll_invariants_to_core_entries(timeline_entries)
  video_processor.py:3455:
  video_processor.py:3456:        if not timeline_entries:
  video_processor.py:3457:            return None
  video_processor.py:3458:
  video_processor.py:3459:        timeline_entries.sort(key=lambda item: (item.start, 
item.segment_index))
  video_processor.py:3460:
> video_processor.py:3461:        render_path = self._render_core_broll_timeline(Path(input_path), 
timeline_entries)
  video_processor.py:3462:        if render_path is None:
  video_processor.py:3463:            return None
  video_processor.py:3464:
  video_processor.py:3465:        self._core_last_timeline = list(timeline_entries)
  video_processor.py:3466:        self._core_last_render_path = render_path
> video_processor.py:3467:        self._last_broll_insert_count = len(timeline_entries)
  video_processor.py:3468:
  video_processor.py:3469:        event_logger = None
  video_processor.py:3470:        try:
> video_processor.py:3471:            event_logger = self._get_broll_event_logger()
  video_processor.py:3472:        except Exception:
  video_processor.py:3473:            event_logger = None
  video_processor.py:3474:        if event_logger is not None:
  video_processor.py:3485:
  video_processor.py:3486:        return render_path
  video_processor.py:3487:
> video_processor.py:3488:    def _derive_segment_keywords(self, segment, global_keywords: 
Sequence[str]) -> List[str]:
> video_processor.py:3489:        keywords: List[str] = []
> video_processor.py:3490:        if global_keywords:
> video_processor.py:3491:            keywords.extend(global_keywords[:4])
  video_processor.py:3492:        segment_words = [w.strip().lower() for w in segment.text.split() if 
len(w.strip()) > 3]
  video_processor.py:3493:        unique_segment_words: List[str] = []
  video_processor.py:3494:        for word in segment_words:
  video_processor.py:3495:            if word not in unique_segment_words:
  video_processor.py:3496:                unique_segment_words.append(word)
> video_processor.py:3497:        keywords.extend(unique_segment_words[:3])
  video_processor.py:3498:        result: List[str] = []
> video_processor.py:3499:        for word in keywords:
  video_processor.py:3500:            if word and word not in result:
  video_processor.py:3501:                result.append(word)
  video_processor.py:3502:        return result
  video_processor.py:3529:                    base_score += 0.05
  video_processor.py:3530:
  video_processor.py:3531:        tags = getattr(candidate, 'tags', None) or ()
> video_processor.py:3532:        keyword_hits = sum(1 for t in tags if isinstance(t, str) and t)
> video_processor.py:3533:        if keyword_hits:
> video_processor.py:3534:            base_score += min(0.1, keyword_hits * 0.02)
  video_processor.py:3535:
  video_processor.py:3536:        score = base_score * orientation_penalty
  video_processor.py:3537:        return max(0.0, min(1.0, score))
  video_processor.py:3623:            except Exception:
  video_processor.py:3624:                pass
  video_processor.py:3625: 
> video_processor.py:3626:    def _purge_broll_caches(self) -> None:
  video_processor.py:3627:        try:
> video_processor.py:3628:            broll_lib = Path('AI-B-roll') / 'broll_library'
> video_processor.py:3629:            broll_cache = Path('AI-B-roll') / '.cache'
> video_processor.py:3630:            if broll_lib.exists():
> video_processor.py:3631:                for item in broll_lib.glob('*'):
  video_processor.py:3632:                    try:
  video_processor.py:3633:                        if item.is_dir():
  video_processor.py:3634:                            shutil.rmtree(item, ignore_errors=True)
  video_processor.py:3636:                            item.unlink(missing_ok=True)
  video_processor.py:3637:                    except Exception:
  video_processor.py:3638:                        pass
> video_processor.py:3639:            if broll_cache.exists():
> video_processor.py:3640:                shutil.rmtree(broll_cache, ignore_errors=True)
  video_processor.py:3641:        except Exception:
  video_processor.py:3642:            pass
  video_processor.py:3643:
  video_processor.py:3644:    # ðŸš¨ CORRECTION CRITIQUE: MÃ©thodes manquantes pour le sÃ©lecteur 
B-roll
> video_processor.py:3645:    def _load_broll_selector_config(self):
  video_processor.py:3646:        """Charge la configuration du sÃ©lecteur B-roll depuis le fichier 
YAML"""
  video_processor.py:3647:        try:
  video_processor.py:3648:            import yaml
> video_processor.py:3649:            if Config.BROLL_SELECTOR_CONFIG_PATH.exists():
> video_processor.py:3650:                with open(Config.BROLL_SELECTOR_CONFIG_PATH, 'r', 
encoding='utf-8') as f:
  video_processor.py:3651:                    return yaml.safe_load(f) or {}
  video_processor.py:3652:            else:
> video_processor.py:3653:                print(f"    âšï¸ Fichier de configuration introuvable: 
{Config.BROLL_SELECTOR_CONFIG_PATH}")
  video_processor.py:3654:                return {}
  video_processor.py:3655:        except Exception as e:
  video_processor.py:3656:            print(f"    âš ï¸ Erreur chargement configuration: {e}")
  video_processor.py:3668:            hash_data = f"{asset_path.name}_{stat.st_size}_{stat.st_mtime}"
  video_processor.py:3669:            return hashlib.md5(hash_data.encode()).hexdigest()
  video_processor.py:3670:        except Exception:
> video_processor.py:3671:            # Fallback sur le nom du fichier
  video_processor.py:3672:            return str(asset_path.name)
  video_processor.py:3673:
> video_processor.py:3674:    def _extract_keywords_for_segment_spacy(self, text: str) -> List[str]:
> video_processor.py:3675:        """Extraction optionnelle (spaCy) de mots-clÃ©s 
(noms/verbes/entitÃ©s). Fallback heuristique si indisponible."""
  video_processor.py:3676:        try:
  video_processor.py:3677:            import re as _re
  video_processor.py:3678:            
  video_processor.py:3687:                'know', 'want', 'need', 'like', 'love', 'hate', 'hope', 
'wish', 'try', 'help'
  video_processor.py:3688:            }
  video_processor.py:3689:            
> video_processor.py:3690:            if self._spacy_model is None:
  video_processor.py:3691:                try:
  video_processor.py:3692:                    import spacy as _spacy
> video_processor.py:3693:                    for _model in ['en_core_web_sm', 'fr_core_news_sm', 
'xx_ent_wiki_sm']:
  video_processor.py:3694:                        try:
> video_processor.py:3695:                            self._spacy_model = _spacy.load(_model, 
disable=['parser','lemmatizer'])
  video_processor.py:3696:                            break
  video_processor.py:3697:                        except Exception:
  video_processor.py:3698:                            continue
> video_processor.py:3699:                    if self._spacy_model is None:
> video_processor.py:3700:                        self._spacy_model = _spacy.blank('en')
  video_processor.py:3701:                except Exception:
> video_processor.py:3702:                    self._spacy_model = None
  video_processor.py:3703:            doc = None
> video_processor.py:3704:            if self._spacy_model is not None:
  video_processor.py:3705:                try:
> video_processor.py:3706:                    doc = self._spacy_model(text)
  video_processor.py:3707:                except Exception:
  video_processor.py:3708:                    doc = None
> video_processor.py:3709:            keywords: List[str] = []
  video_processor.py:3710:            if doc is not None and hasattr(doc, 'ents'):
  video_processor.py:3711:                for ent in doc.ents:
  video_processor.py:3712:                    val = ent.text.strip()
> video_processor.py:3713:                    if len(val) >= 3 and val.lower() not in keywords and 
val.lower() not in GENERIC_WORDS:
> video_processor.py:3714:                        keywords.append(val.lower())
  video_processor.py:3715:            # POS si dispo
  video_processor.py:3716:            if doc is not None and getattr(doc, 'has_annotation', lambda *_: 
False)('TAG'):
  video_processor.py:3717:                for tok in doc:
  video_processor.py:3718:                    if tok.pos_ in ('NOUN','PROPN','VERB') and len(tok.text) 
>= 3:
  video_processor.py:3719:                        lemma = (tok.lemma_ or tok.text).lower()
> video_processor.py:3720:                        if lemma not in keywords and lemma not in 
GENERIC_WORDS:
> video_processor.py:3721:                            keywords.append(lemma)
> video_processor.py:3722:            # Fallback heuristique simple avec filtre
> video_processor.py:3723:            if not keywords:
  video_processor.py:3724:                for w in _split_basic_latin_runs(text or "", keep={"'", 
"-"}):
  video_processor.py:3725:                    if len(w) < 4:
  video_processor.py:3726:                        continue
  video_processor.py:3727:                    lw = w.lower()
> video_processor.py:3728:                    if lw not in keywords and lw not in GENERIC_WORDS:
> video_processor.py:3729:                        keywords.append(lw)
  video_processor.py:3730:            
  video_processor.py:3731:
> video_processor.py:3732:                        # Build clean selector/fetch keyword lists from LLM 
+ transcript
> video_processor.py:3733:                        llm_keywords_input = list(broll_keywords or [])
  video_processor.py:3734:                        transcript_tokens: List[str] = []
> video_processor.py:3735:                        for _seg in subtitles:
  video_processor.py:3736:                            _t = str(_seg.get('text','') or '')
  video_processor.py:3737:                            transcript_tokens.extend(_t.split())
> video_processor.py:3738:                        selector_keywords = 
_normalize_queries(llm_keywords_input, transcript_tokens, max_queries=12)
> video_processor.py:3739:                        fetch_keywords = 
_normalize_queries(llm_keywords_input, transcript_tokens, max_queries=8)
> video_processor.py:3740:                        if not selector_keywords:
> video_processor.py:3741:                            selector_keywords = fetch_keywords[:]
> video_processor.py:3742:                        if not fetch_keywords:
> video_processor.py:3743:                            fetch_keywords = selector_keywords[:8] if 
selector_keywords else ['motivation','reward','focus','success','mindset']
  video_processor.py:3744:            # ðŸš¨ CORRECTION IMMÃ‰DIATE: Prioriser les mots contextuels 
importants
  video_processor.py:3745:            PRIORITY_WORDS = {
  video_processor.py:3746:                'neuroscience', 'brain', 'mind', 'consciousness', 
'cognitive', 'mental', 'psychology',
  video_processor.py:3750:            }
  video_processor.py:3751:            
  video_processor.py:3752:            # RÃ©organiser pour prioriser les mots importants
> video_processor.py:3753:            priority_keywords = [kw for kw in keywords if kw in 
PRIORITY_WORDS]
> video_processor.py:3754:            other_keywords = [kw for kw in keywords if kw not in 
PRIORITY_WORDS]
  video_processor.py:3755:            
  video_processor.py:3756:            # Retourner d'abord les mots prioritaires, puis les autres
> video_processor.py:3757:            final_keywords = priority_keywords + other_keywords
> video_processor.py:3758:            return final_keywords[:12]
  video_processor.py:3759:        except Exception:
  video_processor.py:3760:            return []
  video_processor.py:3761:
  video_processor.py:3814:        logger.info("ðŸŽ‰ Pipeline terminÃ© avec succÃ¨s")
  video_processor.py:3815:        # Purge B-roll (librairie + caches) si demandÃ© pour garder le 
disque lÃ©ger
  video_processor.py:3816:        try:
> video_processor.py:3817:            if getattr(Config, 'BROLL_PURGE_AFTER_RUN', False):
> video_processor.py:3818:                self._purge_broll_caches()
  video_processor.py:3819:        except Exception:
  video_processor.py:3820:            pass
> video_processor.py:3821:        # AgrÃ©ger un rapport global mÃªme sans --json-report
  video_processor.py:3822:        try:
  video_processor.py:3823:            final_dir = (Config.OUTPUT_FOLDER / 'final')
  video_processor.py:3824:            items = []
  video_processor.py:3825:            if final_dir.exists():
> video_processor.py:3826:                for jf in final_dir.glob('final_*.json'):
  video_processor.py:3827:                    try:
> video_processor.py:3828:                        
items.append(json.loads(jf.read_text(encoding='utf-8')))
  video_processor.py:3829:                    except Exception:
  video_processor.py:3830:                        pass
> video_processor.py:3831:            report_path = Config.OUTPUT_FOLDER / 'report.json'
> video_processor.py:3832:            report_path.write_text(json.dumps({'clips': items}, 
ensure_ascii=False, indent=2), encoding='utf-8')
  video_processor.py:3833:        except Exception:
  video_processor.py:3834:            pass
  video_processor.py:3835:
  video_processor.py:3889:        
  video_processor.py:3890:        print(f"  ðŸ—£ï¸ Ã‰tape 2/4: Transcription Whisper (guide 
B-roll)...")
  video_processor.py:3891:        # Transcrire tÃ´t pour guider la sÃ©lection B-roll (SRT disponible)
> video_processor.py:3892:        subtitles = self.transcribe_segments(reframed_path)
  video_processor.py:3893:        try:
  video_processor.py:3894:            # Ã‰crire un SRT Ã  cÃ´tÃ© de la vidÃ©o reframÃ©e
  video_processor.py:3895:            srt_reframed = reframed_path.with_suffix('.srt')
> video_processor.py:3896:            write_srt(subtitles, srt_reframed)
> video_processor.py:3897:            # Sauvegarder transcription segments JSON
> video_processor.py:3898:            seg_json = per_clip_dir / f"{clip_path.stem}_segments.json"
> video_processor.py:3899:            with open(seg_json, 'w', encoding='utf-8') as f:
> video_processor.py:3900:                json.dump(subtitles, f, ensure_ascii=False)
  video_processor.py:3901:        except Exception:
  video_processor.py:3902:            pass
  video_processor.py:3903:        
> video_processor.py:3904:        print(f"  ðŸŽžï¸ Ã‰tape 3/4: Insertion des B-rolls {'(activÃ©e)' if 
getattr(Config, 'ENABLE_BROLL', False) else '(dÃ©sactivÃ©e)'}...")
  video_processor.py:3905:        
  video_processor.py:3906:        # ðŸš€ CORRECTION: GÃ©nÃ©rer les mots-clÃ©s LLM AVANT l'insertion 
des B-rolls
  video_processor.py:3907:        metadata: Dict[str, Any] = {}
> video_processor.py:3908:        broll_keywords: List[str] = []
  video_processor.py:3909:        title = ""
  video_processor.py:3910:        description = ""
  video_processor.py:3911:        hashtags: List[str] = []
  video_processor.py:3912:        try:
  video_processor.py:3913:            print("    ðŸ¤– GÃ©nÃ©ration prÃ©coce des mots-clÃ©s LLM pour 
B-rolls...")
> video_processor.py:3914:            metadata = self.generate_caption_and_hashtags(subtitles) or {}
  video_processor.py:3915:            title = str(metadata.get('title') or '').strip()
  video_processor.py:3916:            description = str(metadata.get('description') or '').strip()
  video_processor.py:3917:            hashtags = [h for h in (metadata.get('hashtags') or []) if 
isinstance(h, str)]
> video_processor.py:3918:            broll_keywords = [kw for kw in (metadata.get('broll_keywords') 
or []) if isinstance(kw, str)]
> video_processor.py:3919:            print(f"    âœ… Mots-clÃ©s B-roll LLM gÃ©nÃ©rÃ©s: 
{len(broll_keywords)} termes")
> video_processor.py:3920:            print(f"    ðŸŽ¯ Exemples: {', '.join(broll_keywords[:5])}")
  video_processor.py:3921:        except Exception as e:
  video_processor.py:3922:            print(f"    âš ï¸ Erreur gÃ©nÃ©ration mots-clÃ©s LLM: {e}")
> video_processor.py:3923:            broll_keywords = []
  video_processor.py:3924:        
  video_processor.py:3925:        # Maintenant insÃ©rer les B-rolls avec les mots-clÃ©s LLM disponibles
> video_processor.py:3926:        with_broll_path = self.insert_brolls_if_enabled(reframed_path, 
subtitles, broll_keywords)
  video_processor.py:3927:        
> video_processor.py:3928:        # Copier artefact with_broll si diffÃ©rent
  video_processor.py:3929:        try:
> video_processor.py:3930:            if with_broll_path and with_broll_path != reframed_path:
> video_processor.py:3931:                self._safe_copy(with_broll_path, per_clip_dir / 
'with_broll.mp4')
  video_processor.py:3932:        except Exception:
  video_processor.py:3933:            pass
  video_processor.py:3934:        
  video_processor.py:3936:        # GÃ©nÃ©rer meta (titre/hashtags) depuis transcription (dÃ©jÃ  fait)
  video_processor.py:3937:        try:
  video_processor.py:3938:            # RÃ©utiliser les donnÃ©es dÃ©jÃ  gÃ©nÃ©rÃ©es
> video_processor.py:3939:            if not broll_keywords:  # Fallback si pas encore gÃ©nÃ©rÃ©
> video_processor.py:3940:                metadata = self.generate_caption_and_hashtags(subtitles) or 
metadata or {}
  video_processor.py:3941:                title = str(metadata.get('title') or '').strip()
  video_processor.py:3942:                description = str(metadata.get('description') or '').strip()
  video_processor.py:3943:                hashtags = [h for h in (metadata.get('hashtags') or []) if 
isinstance(h, str)]
> video_processor.py:3944:                broll_keywords = [kw for kw in 
(metadata.get('broll_keywords') or []) if isinstance(kw, str)]
  video_processor.py:3945:
  video_processor.py:3946:            print(f"  ðŸ“ Title: {title}")
  video_processor.py:3947:            print(f"  ðŸ“ Description: {description}")
  video_processor.py:3952:                    "Title: " + title + "\n\n" +
  video_processor.py:3953:                    "Description: " + description + "\n\n" +
  video_processor.py:3954:                    "Hashtags: " + ' '.join(hashtags) + "\n\n" +
> video_processor.py:3955:                    "B-roll Keywords: " + ', '.join(broll_keywords) + "\n"
  video_processor.py:3956:                )
  video_processor.py:3957:            print(f"  ðŸ“ [MÃ‰TADONNÃ‰ES] Fichier meta.txt sauvegardÃ©: 
{meta_path}")
  video_processor.py:3958:        except Exception as e:
  video_processor.py:3959:            print(f"  âš ï¸ [ERREUR MÃ‰TADONNÃ‰ES] {e}")
> video_processor.py:3960:            # Fallback: crÃ©er des mÃ©tadonnÃ©es basiques
  video_processor.py:3961:            try:
  video_processor.py:3962:                meta_path = per_clip_dir / 'meta.txt'
  video_processor.py:3963:                with open(meta_path, 'w', encoding='utf-8') as f:
> video_processor.py:3964:                    f.write("Title: VidÃ©o gÃ©nÃ©rÃ©e 
automatiquement\n\nDescription: Contenu gÃ©nÃ©rÃ© par pipeline vidÃ©o\n\nHashtags: #video 
#auto\n\nB-roll Keywords: video, content\n")
> video_processor.py:3965:                print(f"  ðŸ“ [FALLBACK] MÃ©tadonnÃ©es de base 
sauvegardÃ©es: {meta_path}")
  video_processor.py:3966:            except Exception as e2:
> video_processor.py:3967:                print(f"  âŒ [ERREUR FALLBACK] {e2}")
  video_processor.py:3968:        
  video_processor.py:3969:        # Appliquer style Hormozi sur la vidÃ©o post B-roll
> video_processor.py:3970:        subtitled_out_dir = per_clip_dir
> video_processor.py:3971:        subtitled_out_dir.mkdir(parents=True, exist_ok=True)
> video_processor.py:3972:        final_subtitled_path = subtitled_out_dir / 'final_subtitled.mp4'
  video_processor.py:3973:        try:
  video_processor.py:3974:            span_style_map = {
  video_processor.py:3975:                # Business & Croissance
> video_processor.py:3976:                "croissance": {"color": "#39FF14", "bold": True, "emoji": 
"ðŸ“ˆ"},
> video_processor.py:3977:                "growth": {"color": "#39FF14", "bold": True, "emoji": 
"ðŸ“ˆ"},
> video_processor.py:3978:                "opportunitÃ©": {"color": "#FFD700", "bold": True, "emoji": 
"ï¿½ï¿½"},
> video_processor.py:3979:                "opportunite": {"color": "#FFD700", "bold": True, "emoji": 
"ðŸ”‘"},
> video_processor.py:3980:                "innovation": {"color": "#00E5FF", "emoji": "âš¡"},
> video_processor.py:3981:                "idÃ©e": {"color": "#00E5FF", "emoji": "ðŸ’¡"},
> video_processor.py:3982:                "idee": {"color": "#00E5FF", "emoji": "ðŸ’¡"},
> video_processor.py:3983:                "stratÃ©gie": {"color": "#FF73FA", "emoji": "ðŸ§­"},
> video_processor.py:3984:                "strategie": {"color": "#FF73FA", "emoji": "ðŸ§­"},
> video_processor.py:3985:                "plan": {"color": "#FF73FA", "emoji": "ðŸ—ºï¸"},
  video_processor.py:3986:                # Argent & Finance
> video_processor.py:3987:                "argent": {"color": "#FFD700", "bold": True, "emoji": 
"ðŸ’°"},
> video_processor.py:3988:                "money": {"color": "#FFD700", "bold": True, "emoji": "ðŸ’°"},
> video_processor.py:3989:                "cash": {"color": "#FFD700", "bold": True, "emoji": "ðŸ’°"},
> video_processor.py:3990:                "investissement": {"color": "#8AFF00", "bold": True, 
"emoji": "ðŸ“Š"},
> video_processor.py:3991:                "investissements": {"color": "#8AFF00", "bold": True, 
"emoji": "ðŸ“Š"},
> video_processor.py:3992:                "revenu": {"color": "#8AFF00", "emoji": "ðŸ¦"},
> video_processor.py:3993:                "revenus": {"color": "#8AFF00", "emoji": "ðŸ¦"},
> video_processor.py:3994:                "profit": {"color": "#8AFF00", "bold": True, "emoji": 
"ðŸ’°"},
> video_processor.py:3995:                "profits": {"color": "#8AFF00", "bold": True, "emoji": 
"ðŸ’°"},
> video_processor.py:3996:                "perte": {"color": "#FF3131", "emoji": "ðŸ“‰"},
> video_processor.py:3997:                "pertes": {"color": "#FF3131", "emoji": "ðŸ“‰"},
> video_processor.py:3998:                "Ã©chec": {"color": "#FF3131", "emoji": "âŒ"},
> video_processor.py:3999:                "echec": {"color": "#FF3131", "emoji": "âŒ"},
> video_processor.py:4000:                "budget": {"color": "#FFD700", "emoji": "ðŸ§¾"},
> video_processor.py:4001:                "gestion": {"color": "#FFD700", "emoji": "ðŸª™"},
> video_processor.py:4002:                "roi": {"color": "#8AFF00", "bold": True, "emoji": "ðŸ“ˆ"},
> video_processor.py:4003:                "chiffre": {"color": "#FFD700", "emoji": "ðŸ’°"},
> video_processor.py:4004:                "ca": {"color": "#FFD700", "emoji": "ðŸ’°"},
  video_processor.py:4005:                # Relation & Client
> video_processor.py:4006:                "client": {"color": "#00E5FF", "underline": True, "emoji": 
"ðŸ¤"},
> video_processor.py:4007:                "clients": {"color": "#00E5FF", "underline": True, "emoji": 
"ðŸ¤"},
> video_processor.py:4008:                "collaboration": {"color": "#00E5FF", "emoji": 
"ðŸ«±ðŸ¼â€ðŸ«²ðŸ½"},
> video_processor.py:4009:                "collaborations": {"color": "#00E5FF", "emoji": 
"ðŸ«±ðŸ¼â€ðŸ«²ðŸ½"},
> video_processor.py:4010:                "communautÃ©": {"color": "#39FF14", "emoji": "ðŸŒ"},
> video_processor.py:4011:                "communaute": {"color": "#39FF14", "emoji": "ðŸŒ"},
> video_processor.py:4012:                "confiance": {"color": "#00E5FF", "emoji": "ðŸ”’"},
> video_processor.py:4013:                "vente": {"color": "#FF73FA", "emoji": "ðŸ›’"},
> video_processor.py:4014:                "ventes": {"color": "#FF73FA", "emoji": "ðŸ›’"},
> video_processor.py:4015:                "deal": {"color": "#FF73FA", "emoji": "ðŸ“¦"},
> video_processor.py:4016:                "deals": {"color": "#FF73FA", "emoji": "ðŸ“¦"},
> video_processor.py:4017:                "prospect": {"color": "#00E5FF", "emoji": "ðŸ¤"},
> video_processor.py:4018:                "prospects": {"color": "#00E5FF", "emoji": "ðŸ¤"},
> video_processor.py:4019:                "contrat": {"color": "#FF73FA", "emoji": "ðŸ“‹"},
  video_processor.py:4020:                # Motivation & SuccÃ¨s
> video_processor.py:4021:                "succÃ¨s": {"color": "#39FF14", "italic": True, "emoji": 
"ðŸ†"},
> video_processor.py:4022:                "succes": {"color": "#39FF14", "italic": True, "emoji": 
"ðŸ†"},
> video_processor.py:4023:                "motivation": {"color": "#FF73FA", "bold": True, "emoji": 
"ðŸ”¥"},
> video_processor.py:4024:                "Ã©nergie": {"color": "#FF73FA", "emoji": "âš¡"},
> video_processor.py:4025:                "energie": {"color": "#FF73FA", "emoji": "âš¡"},
> video_processor.py:4026:                "victoire": {"color": "#39FF14", "emoji": "ðŸŽ¯"},
> video_processor.py:4027:                "discipline": {"color": "#FFD700", "emoji": "â³"},
> video_processor.py:4028:                "viral": {"color": "#FF73FA", "bold": True, "emoji": "ðŸš€"},
> video_processor.py:4029:                "viralitÃ©": {"color": "#FF73FA", "bold": True, "emoji": 
"ðŸŒ"},
> video_processor.py:4030:                "viralite": {"color": "#FF73FA", "bold": True, "emoji": 
"ðŸŒ"},
> video_processor.py:4031:                "impact": {"color": "#FF73FA", "emoji": "ðŸ’¥"},
> video_processor.py:4032:                "explose": {"color": "#FF73FA", "emoji": "ðŸ’¥"},
> video_processor.py:4033:                "explosion": {"color": "#FF73FA", "emoji": "ðŸ’¥"},
  video_processor.py:4034:                # Risque & Erreurs
> video_processor.py:4035:                "erreur": {"color": "#FF3131", "emoji": "âš ï¸"},
> video_processor.py:4036:                "erreurs": {"color": "#FF3131", "emoji": "âš ï¸"},
> video_processor.py:4037:                "warning": {"color": "#FF3131", "emoji": "âš ï¸"},
> video_processor.py:4038:                "obstacle": {"color": "#FF3131", "emoji": "ðŸ§±"},
> video_processor.py:4039:                "obstacles": {"color": "#FF3131", "emoji": "ðŸ§±"},
> video_processor.py:4040:                "solution": {"color": "#00E5FF", "emoji": "ðŸ”§"},
> video_processor.py:4041:                "solutions": {"color": "#00E5FF", "emoji": "ðŸ”§"},
> video_processor.py:4042:                "leÃ§on": {"color": "#00E5FF", "emoji": "ðŸ“š"},
> video_processor.py:4043:                "lecon": {"color": "#00E5FF", "emoji": "ðŸ“š"},
> video_processor.py:4044:                "apprentissage": {"color": "#00E5FF", "emoji": "ðŸ§ "},
> video_processor.py:4045:                "problÃ¨me": {"color": "#FF3131", "emoji": "ðŸ›‘"},
> video_processor.py:4046:                "probleme": {"color": "#FF3131", "emoji": "ðŸ›‘"},
  video_processor.py:4047:            }
> video_processor.py:4048:            add_hormozi_subtitles(
> video_processor.py:4049:                str(with_broll_path), subtitles, str(final_subtitled_path),
  video_processor.py:4050:                brand_kit=getattr(Config, 'BRAND_KIT_ID', 'default'),
  video_processor.py:4051:                span_style_map=span_style_map
  video_processor.py:4052:            )
  video_processor.py:4054:            print(f"  âŒ Erreur ajout sous-titres Hormozi: {e}")
  video_processor.py:4055:            # Pas de retour anticipÃ©: continuer export simple
  video_processor.py:4056:        
> video_processor.py:4057:        # Export final accumulÃ© dans output/final/ et sous-titrÃ© (burn-in) 
dans output/subtitled/
  video_processor.py:4058:        final_dir = Config.OUTPUT_FOLDER / 'final'
> video_processor.py:4059:        subtitled_dir = Config.OUTPUT_FOLDER / 'subtitled'
  video_processor.py:4060:        # Noms de base sans extension
  video_processor.py:4061:        base_name = clip_path.stem
  video_processor.py:4062:        output_path = self._unique_path(final_dir, f"final_{base_name}", 
".mp4")
  video_processor.py:4063:        try:
> video_processor.py:4064:            # Choisir source finale: si sous-titrÃ©e existe sinon with_broll 
sinon reframed
  video_processor.py:4065:            source_final = None
> video_processor.py:4066:            if final_subtitled_path.exists():
> video_processor.py:4067:                source_final = final_subtitled_path
> video_processor.py:4068:            elif with_broll_path and Path(with_broll_path).exists():
> video_processor.py:4069:                source_final = with_broll_path
  video_processor.py:4070:            else:
  video_processor.py:4071:                source_final = reframed_path
  video_processor.py:4072:            if source_final and Path(source_final).exists():
  video_processor.py:4073:                self._hardlink_or_copy(source_final, output_path)
  video_processor.py:4074:                # Ecrire SRT: Ã©viter le doublon si la vidÃ©o finale a dÃ©jÃ 
les sous-titres incrustÃ©s
> video_processor.py:4075:                is_burned = (final_subtitled_path.exists() and 
Path(source_final) == Path(final_subtitled_path))
  video_processor.py:4076:                if not is_burned:
  video_processor.py:4077:                    srt_out = output_path.with_suffix('.srt')
> video_processor.py:4078:                    write_srt(subtitles, srt_out)
  video_processor.py:4079:                    self._hardlink_or_copy(srt_out, per_clip_dir / 
'final.srt')
  video_processor.py:4080:                    # WebVTT
  video_processor.py:4081:                    try:
  video_processor.py:4082:                        vtt_out = output_path.with_suffix('.vtt')
> video_processor.py:4083:                        write_vtt(subtitles, vtt_out)
  video_processor.py:4084:                    except Exception:
  video_processor.py:4085:                        pass
  video_processor.py:4086:                else:
  video_processor.py:4087:                    # Produire uniquement une SRT dans le dossier du clip, 
pas Ã cÃ´tÃ© du MP4 final
  video_processor.py:4088:                    try:
> video_processor.py:4089:                        write_srt(subtitles, per_clip_dir / 'final.srt')
  video_processor.py:4090:                    except Exception:
  video_processor.py:4091:                        pass
  video_processor.py:4092:                # Toujours produire un VTT Ã  cÃ´tÃ© du final pour compat
  video_processor.py:4093:                try:
  video_processor.py:4094:                    vtt_out = output_path.with_suffix('.vtt')
> video_processor.py:4095:                    write_vtt(subtitles, vtt_out)
  video_processor.py:4096:                except Exception:
  video_processor.py:4097:                    pass
  video_processor.py:4098:                # Copier final dans dossier clip
  video_processor.py:4099:                self._hardlink_or_copy(output_path, per_clip_dir / 
'final.mp4')
> video_processor.py:4100:                # Si une version sous-titrÃ©e burn-in existe, la dupliquer 
dans output/subtitled/
> video_processor.py:4101:                if final_subtitled_path.exists():
> video_processor.py:4102:                    subtitled_out = self._unique_path(subtitled_dir, 
f"{base_name}_subtitled", ".mp4")
> video_processor.py:4103:                    self._hardlink_or_copy(final_subtitled_path, 
subtitled_out)
  video_processor.py:4104:                # Copier meta.txt Ã  cÃ´tÃ© du final accumulÃ©
  video_processor.py:4105:                try:
  video_processor.py:4106:                    meta_src = per_clip_dir / 'meta.txt'
  video_processor.py:4108:                        self._hardlink_or_copy(meta_src, 
output_path.with_suffix('.txt'))
  video_processor.py:4109:                except Exception:
  video_processor.py:4110:                    pass
> video_processor.py:4111:                # Ecrire un JSON rÃ©cap par clip
  video_processor.py:4112:                try:
  video_processor.py:4113:                    # DurÃ©e et hash final
  video_processor.py:4114:                    final_duration = None
  video_processor.py:4132:                        'final_mp4': str(output_path.resolve()),
  video_processor.py:4133:                        'final_srt': 
str(output_path.with_suffix('.srt').resolve()) if (not is_burned) and 
output_path.with_suffix('.srt').exists() else None,
  video_processor.py:4134:                        'final_vtt': 
str(output_path.with_suffix('.vtt').resolve()) if (not is_burned) and 
output_path.with_suffix('.vtt').exists() else None,
> video_processor.py:4135:                        'subtitled_mp4': str((subtitled_out.resolve() if 
final_subtitled_path.exists() else '')) if final_subtitled_path.exists() else None,
  video_processor.py:4136:                        'meta_txt': 
str(output_path.with_suffix('.txt').resolve()) if output_path.with_suffix('.txt').exists() else None,
  video_processor.py:4137:                        'per_clip_dir': str(per_clip_dir.resolve()),
  video_processor.py:4138:                        'duration_s': final_duration,
  video_processor.py:4148:                            } for ev in (events or [])
  video_processor.py:4149:                        ]
  video_processor.py:4150:                    }
> video_processor.py:4151:                    with open(output_path.with_suffix('.json'), 'w', 
encoding='utf-8') as jf:
> video_processor.py:4152:                        json.dump(summary, jf, ensure_ascii=False, indent=2)
> video_processor.py:4153:                    # JSONL log
  video_processor.py:4154:                    try:
> video_processor.py:4155:                        jsonl = (Config.OUTPUT_FOLDER / 'pipeline.log.jsonl')
> video_processor.py:4156:                        with open(jsonl, 'a', encoding='utf-8') as lf:
> video_processor.py:4157:                            lf.write(json.dumps(summary, ensure_ascii=False) 
+ '\n')
  video_processor.py:4158:                    except Exception:
  video_processor.py:4159:                        pass
  video_processor.py:4160:                except Exception:
  video_processor.py:4162:                print(f"  ðŸ“¤ Export terminÃ©: {output_path.name}")
  video_processor.py:4163:                # Nettoyage des intermÃ©diaires pour limiter l'empreinte 
disque
  video_processor.py:4164:                self._cleanup_files([
> video_processor.py:4165:                    with_broll_path if with_broll_path and with_broll_path 
!= output_path else None,
  video_processor.py:4166:                ])
  video_processor.py:4167:                return output_path
  video_processor.py:4168:            else:
  video_processor.py:4274:                mp_pose = None
  video_processor.py:4275:                mp_face = None
  video_processor.py:4276:            if mp_pose is not None and mp_face is not None:
> video_processor.py:4277:                with mp_pose.Pose(static_image_mode=False, 
min_detection_confidence=0.7, min_tracking_confidence=0.8) as pose, 
mp_face.FaceDetection(model_selection=0, min_detection_confidence=0.7) as face_detection:
  video_processor.py:4278:                    center = self._detect_center_with_mediapipe(image_rgb, 
pose, face_detection, mp_pose)
  video_processor.py:4279:                    if center is not None:
  video_processor.py:4280:                        return center
  video_processor.py:4296:                pose_module = None
  video_processor.py:4297:                face_module = None
  video_processor.py:4298:        if pose_module is not None and face_module is not None:
> video_processor.py:4299:            with pose_module.Pose(static_image_mode=False, 
min_detection_confidence=0.7, min_tracking_confidence=0.8) as pose, 
face_module.FaceDetection(model_selection=0, min_detection_confidence=0.7) as face_detection:
  video_processor.py:4300:                for t in tqdm(sample_times, desc="ðŸ”Ž IA focus", 
leave=False):
  video_processor.py:4301:                    try:
  video_processor.py:4302:                        frame = video.get_frame(t)  # MoviePy retourne des 
frames RGB
  video_processor.py:4398:                ffmpeg_params=['-rc','vbr','-cq','19','-b:v','0','-maxrate','
0','-pix_fmt','yuv420p','-movflags','+faststart']
  video_processor.py:4399:            )
  video_processor.py:4400:        except Exception:
> video_processor.py:4401:            # Fallback to CPU x264 with stable CRF
  video_processor.py:4402:            reframed.write_videofile(
  video_processor.py:4403:                str(output_path),
  video_processor.py:4404:                fps=fps,
  video_processor.py:4418:        logger.info("ðŸ“ Transcription audio avec Whisper")
  video_processor.py:4419:        print("    ðŸ“ Transcription Whisper en cours...")
  video_processor.py:4420:        
> video_processor.py:4421:        result = self.whisper_model.transcribe(str(video_path))
  video_processor.py:4422:        print("    âœ… Transcription terminÃ©e")
  video_processor.py:4423:        return result["text"]
  video_processor.py:4424:    
  video_processor.py:4429:        """
  video_processor.py:4430:        logger.info("â±ï¸ Transcription avec timestamps")
  video_processor.py:4431:        print("    â±ï¸ GÃ©nÃ©ration des timestamps...")
> video_processor.py:4432:        result = self.whisper_model.transcribe(str(video_path), 
word_timestamps=True)
> video_processor.py:4433:        bias = getattr(Config, 'SUBTITLE_TIMING_BIAS_S', 0.0)
> video_processor.py:4434:        subtitles: List[Dict] = []
  video_processor.py:4435:        for segment in result.get("segments", []):
  video_processor.py:4436:            seg_start = max(0.0, (segment.get("start") or 0.0) + bias)
  video_processor.py:4437:            seg_end = max(seg_start, (segment.get("end") or seg_start) + 
bias)
> video_processor.py:4438:            subtitle: Dict = {
  video_processor.py:4439:                "text": (segment.get("text") or "").strip(),
  video_processor.py:4440:                "start": seg_start,
  video_processor.py:4441:                "end": seg_end
  video_processor.py:4450:                    if wt:
  video_processor.py:4451:                        precise_words.append({"text": wt, "start": ws, 
"end": we})
  video_processor.py:4452:                if precise_words:
> video_processor.py:4453:                    subtitle["words"] = precise_words
> video_processor.py:4454:            subtitles.append(subtitle)
> video_processor.py:4455:        print(f"    âœ… {len(subtitles)} segments de sous-titres gÃ©nÃ©rÃ©s")
> video_processor.py:4456:        return subtitles
  video_processor.py:4457:
> video_processor.py:4458:    def generate_caption_and_hashtags(self, subtitles: List[Dict]) -> 
Dict[str, Any]:
  video_processor.py:4459:        """GÃ©nÃ¨re un dictionnaire complet de mÃ©tadonnÃ©es via le systÃ¨me 
LLM industriel."""
> video_processor.py:4460:        full_text = ' '.join(s.get('text', '') for s in subtitles)
  video_processor.py:4461:        video_id_hint = getattr(self, '_current_video_id', None)
  video_processor.py:4462:        if not video_id_hint:
  video_processor.py:4463:            video_id_hint = f"video_{int(time.time())}"
  video_processor.py:4468:            title: str,
  video_processor.py:4469:            description: str,
  video_processor.py:4470:            hashtags: Sequence[str],
> video_processor.py:4471:            broll_keywords: Sequence[str],
  video_processor.py:4472:            queries: Sequence[str],
  video_processor.py:4473:            base: Optional[Dict[str, Any]] = None,
  video_processor.py:4474:            source_label: Optional[str] = None,
  video_processor.py:4479:            payload['title'] = title
  video_processor.py:4480:            payload['description'] = description
  video_processor.py:4481:            payload['hashtags'] = [h for h in (hashtags or []) if 
isinstance(h, str) and h]
> video_processor.py:4482:            payload['broll_keywords'] = [kw for kw in (broll_keywords or []) 
if isinstance(kw, str) and kw]
  video_processor.py:4483:            payload['queries'] = [q for q in (queries or []) if 
isinstance(q, str) and q]
  video_processor.py:4484:            payload['metadata_source'] = source_label or status
  video_processor.py:4485:            payload['llm_status'] = status
  video_processor.py:4498:            pass
  video_processor.py:4499:
  video_processor.py:4500:        # ðŸš€ NOUVEAU: Utilisation du systÃ¨me LLM industriel
> video_processor.py:4501:        def _run_fallback(reason: Optional[str] = None):
  video_processor.py:4502:            if reason:
  video_processor.py:4503:                print(reason)
> video_processor.py:4504:            fallback_meta = _basic_metadata_fallback(full_text)
> video_processor.py:4505:            if fallback_meta:
> video_processor.py:4506:                title_fb = (fallback_meta.get('title') or '').strip()
> video_processor.py:4507:                description_fb = (fallback_meta.get('description') or 
'').strip()
> video_processor.py:4508:                hashtags_fb = [h for h in (fallback_meta.get('hashtags') or 
[]) if h]
> video_processor.py:4509:                broll_keywords_fb = list(fallback_meta.get('broll_keywords') 
or [])
> video_processor.py:4510:                queries_fb = fallback_meta.get('queries') or []
  video_processor.py:4511:
> video_processor.py:4512:                print("    ðŸª« [Fallback] MÃ©tadonnÃ©es gÃ©nÃ©rÃ©es sans 
LLM avancÃ©")
> video_processor.py:4513:                print(f"    ðŸŽ¯ Titre fallback: {title_fb}")
  video_processor.py:4514:                if description_fb:
> video_processor.py:4515:                    print(f"    ðŸ“ Description fallback: 
{description_fb[:100]}...")
  video_processor.py:4516:                if hashtags_fb:
> video_processor.py:4517:                    print(f"    #ï¸âƒ£ Hashtags fallback: {', 
'.join(hashtags_fb[:5])}...")
> video_processor.py:4518:                if broll_keywords_fb:
> video_processor.py:4519:                    print(f"    ðŸŽ¬ Mots-clÃ©s B-roll fallback: {', 
'.join(broll_keywords_fb[:5])}...")
  video_processor.py:4520:                if queries_fb:
> video_processor.py:4521:                    print(f"    ðŸ”Ž RequÃªtes fallback: {', 
'.join(queries_fb[:3])}...")
  video_processor.py:4522:
  video_processor.py:4523:                if not title_fb and description_fb:
  video_processor.py:4524:                    title_fb = (description_fb[:60] + ('â€¦' if 
len(description_fb) > 60 else ''))
  video_processor.py:4525:                payload = _finalize_metadata(
> video_processor.py:4526:                    'fallback',
  video_processor.py:4527:                    title=title_fb,
  video_processor.py:4528:                    description=description_fb,
  video_processor.py:4529:                    hashtags=hashtags_fb,
> video_processor.py:4530:                    broll_keywords=broll_keywords_fb,
  video_processor.py:4531:                    queries=queries_fb,
> video_processor.py:4532:                    base=fallback_meta,
> video_processor.py:4533:                    source_label='fallback',
  video_processor.py:4534:                )
  video_processor.py:4535:                return _remember(payload)
  video_processor.py:4536:            return None
  video_processor.py:4541:            common = [w for w, _ in counts.most_common(12) if w.isalpha()]
  video_processor.py:4542:            hashtags_h = [f"#{w}" for w in common[:12]]
  video_processor.py:4543:
> video_processor.py:4544:            # ðŸš€ NOUVEAU: Mots-clÃ©s B-roll de fallback basÃ©s sur les 
mots communs
> video_processor.py:4545:            broll_keywords_h = [w for w in common if len(w) > 3][:15]
  video_processor.py:4546:            queries_h = [f"{w} b-roll" for w in common[:8]]
  video_processor.py:4547:
  video_processor.py:4548:            # Heuristic title/description
  video_processor.py:4549:            title_h = (full_text.strip()[:60] + ("â€¦" if 
len(full_text.strip()) > 60 else "")) if full_text.strip() else ""
  video_processor.py:4550:            description_h = (full_text.strip()[:180] + ("â€¦" if 
len(full_text.strip()) > 180 else "")) if full_text.strip() else ""
> video_processor.py:4551:            print("    ðŸ§© [Heuristics] Meta gÃ©nÃ©rÃ©es en fallback")
> video_processor.py:4552:            print(f"    ðŸ”‘ Mots-clÃ©s B-roll fallback: {', 
'.join(broll_keywords_h[:5])}...")
  video_processor.py:4553:            payload = _finalize_metadata(
  video_processor.py:4554:                'heuristic',
  video_processor.py:4555:                title=title_h,
  video_processor.py:4556:                description=description_h,
  video_processor.py:4557:                hashtags=hashtags_h,
> video_processor.py:4558:                broll_keywords=broll_keywords_h,
  video_processor.py:4559:                queries=queries_h,
  video_processor.py:4560:                source_label='heuristic',
  video_processor.py:4561:            )
  video_processor.py:4562:            return _remember(payload)
  video_processor.py:4563:
> video_processor.py:4564:        if generate_metadata_as_json is None:
> video_processor.py:4565:            fallback_result = _run_fallback("    âšï¸ [LLM INDUSTRIEL] 
Service indisponible, utilisation du fallback historique")
> video_processor.py:4566:            if fallback_result:
> video_processor.py:4567:                return fallback_result
  video_processor.py:4568:            return _run_heuristics()
  video_processor.py:4569:
  video_processor.py:4570:        try:
  video_processor.py:4571:            print(f"    ðŸš€ [LLM INDUSTRIEL] GÃ©nÃ©ration de mÃ©tadonnÃ©es 
pour {len(full_text)} caractÃ¨res")
  video_processor.py:4572:
> video_processor.py:4573:            meta = generate_metadata_as_json(
  video_processor.py:4574:                full_text,
  video_processor.py:4575:                video_id=video_id_hint,
  video_processor.py:4576:            )
  video_processor.py:4577:
> video_processor.py:4578:            if not meta or not any(meta.get(key) for key in ("title", 
"description", "hashtags", "broll_keywords", "queries")):
> video_processor.py:4579:                fallback_result = _run_fallback("    âšï¸ [LLM INDUSTRIEL] 
RÃ©ponse JSON vide ou non analysable, activation du fallback")
> video_processor.py:4580:                if fallback_result:
> video_processor.py:4581:                    return fallback_result
  video_processor.py:4582:                return _run_heuristics()
  video_processor.py:4583:
  video_processor.py:4584:            title = (meta.get('title') or '').strip()
  video_processor.py:4585:            description = (meta.get('description') or '').strip()
  video_processor.py:4586:            hashtags = [h for h in (meta.get('hashtags') or []) if h]
> video_processor.py:4587:            broll_keywords = meta.get('broll_keywords') or []
  video_processor.py:4588:            queries = meta.get('queries') or []
  video_processor.py:4589:            response_len = meta.get('raw_response_length')
  video_processor.py:4590:
> video_processor.py:4591:            print(f"    âœ… [LLM INDUSTRIEL] MÃ©tadonnÃ©es gÃ©nÃ©rÃ©es avec 
succÃ¨s (JSON)")
  video_processor.py:4592:            print(f"    ðŸŽ¯ Titre: {title}")
  video_processor.py:4593:            print(f"    ðŸ“ Description: {description[:100]}...")
> video_processor.py:4594:            print(f"    #ï¸âƒ£ Hashtags JSON: {len(hashtags)} gÃ©nÃ©rÃ©s")
> video_processor.py:4595:            print(f"    ðŸŽ¬ Mots-clÃ©s B-roll JSON: {len(broll_keywords)} 
termes optimisÃ©s")
> video_processor.py:4596:            print(f"    ðŸ”Ž RequÃªtes JSON: {len(queries)} gÃ©nÃ©rÃ©es")
  video_processor.py:4597:            if response_len is not None:
  video_processor.py:4598:                print(f"    ðŸ“ RÃ©ponse LLM (caractÃ¨res): {response_len}")
  video_processor.py:4599:
  video_processor.py:4602:                title=title,
  video_processor.py:4603:                description=description,
  video_processor.py:4604:                hashtags=hashtags,
> video_processor.py:4605:                broll_keywords=broll_keywords,
  video_processor.py:4606:                queries=queries,
  video_processor.py:4607:                base=meta,
  video_processor.py:4608:                source_label='llm',
  video_processor.py:4610:            return _remember(payload)
  video_processor.py:4611:
  video_processor.py:4612:        except Exception as e:
> video_processor.py:4613:            fallback_result = _run_fallback(f"    ðŸ”„ [FALLBACK] Retour 
vers ancien systÃ¨me: {e}")
> video_processor.py:4614:            if fallback_result:
> video_processor.py:4615:                return fallback_result
  video_processor.py:4616:
  video_processor.py:4617:        return _run_heuristics()
  video_processor.py:4618:
> video_processor.py:4619:    def insert_brolls_if_enabled(self, input_path: Path, subtitles: 
List[Dict], broll_keywords: List[str]) -> Path:
  video_processor.py:4620:        """Point d'extension B-roll: retourne le chemin vidÃ©o aprÃ¨s 
insertion si activÃ©e."""
> video_processor.py:4621:        if not getattr(Config, 'ENABLE_BROLL', False):
  video_processor.py:4622:            print("    â­ï¸ B-roll dÃ©sactivÃ©s: aucune insertion")
  video_processor.py:4623:            return input_path
  video_processor.py:4624:
> video_processor.py:4625:        self._last_broll_insert_count = 0
  video_processor.py:4626:        try:
  video_processor.py:4627:            # VÃ©rifier la librairie B-roll
> video_processor.py:4628:            broll_root = Path("AI-B-roll")
> video_processor.py:4629:            broll_library = broll_root / "broll_library"
> video_processor.py:4630:            if not broll_library.exists():
  video_processor.py:4631:                print("    â„¹ï¸ Librairie B-roll absente, initialisation 
automatique")
  video_processor.py:4632:            try:
> video_processor.py:4633:                broll_library.mkdir(parents=True, exist_ok=True)
  video_processor.py:4634:            except Exception as exc:
> video_processor.py:4635:                print(f"    âšï¸ Impossible de prÃ©parer 
AI-B-roll/broll_library ({exc}); utilisation du cache pipeline_core")
> video_processor.py:4636:                fallback_base = getattr(getattr(self._pipeline_config, 
"paths", None), "temp_dir", None)
> video_processor.py:4637:                if not fallback_base:
> video_processor.py:4638:                    fallback_base = getattr(Config, "TEMP_FOLDER", 
Path("temp"))
> video_processor.py:4639:                fallback_library = Path(fallback_base) / 
"pipeline_core_broll"
  video_processor.py:4640:                try:
> video_processor.py:4641:                    fallback_library.mkdir(parents=True, exist_ok=True)
> video_processor.py:4642:                except Exception as fallback_exc:
> video_processor.py:4643:                    print(f"    âšï¸ PrÃ©paration du cache pipeline_core 
Ã©chouÃ©e: {fallback_exc}")
> video_processor.py:4644:                    fallback_library = Path(fallback_base)
> video_processor.py:4645:                broll_library = fallback_library
  video_processor.py:4646:            # PrÃ©parer chemins (Ã©crire directement dans le dossier du clip 
si possible)
  video_processor.py:4647:            clip_dir = (Path(input_path).parent if (Path(input_path).name == 
'reframed.mp4') else Config.TEMP_FOLDER)
> video_processor.py:4648:            # Si input_path est dÃ©jÃ dans un dossier clip (reframed.mp4), 
sortir with_broll.mp4 Ã cÃ´tÃ©
  video_processor.py:4649:            if Path(input_path).name == 'reframed.mp4':
> video_processor.py:4650:                output_with_broll = clip_dir / 'with_broll.mp4'
  video_processor.py:4651:            else:
> video_processor.py:4652:                output_with_broll = Config.TEMP_FOLDER / 
f"with_broll_{Path(input_path).name}"
> video_processor.py:4653:            output_with_broll.parent.mkdir(parents=True, exist_ok=True)
  video_processor.py:4654:
  video_processor.py:4655:            # --- Build dynamic LLM context once per clip (no hardcoded 
domains)
> video_processor.py:4656:            selector_keywords: List[str] = []
> video_processor.py:4657:            fetch_keywords: List[str] = []
  video_processor.py:4658:            dyn_context: Dict[str, Any] = {}
  video_processor.py:4659:            try:
> video_processor.py:4660:                transcript_text_full = " ".join(str(s.get("text", "")) for s 
in (subtitles or []))
  video_processor.py:4661:            except Exception:
  video_processor.py:4662:                transcript_text_full = ""
  video_processor.py:4663:
  video_processor.py:4669:                except Exception:
  video_processor.py:4670:                    dyn_context = {}
  video_processor.py:4671:
> video_processor.py:4672:            llm_kw = list(dyn_context.get("keywords", []) or (broll_keywords 
or []))
  video_processor.py:4673:            llm_queries = list(dyn_context.get("search_queries", []) or [])
  video_processor.py:4674:            syn_map = dyn_context.get("synonyms", {}) or {}
  video_processor.py:4675:            seg_queries: List[str] = []
  video_processor.py:4682:
  video_processor.py:4683:            pool = llm_kw + llm_queries + synonyms_flat + seg_queries
  video_processor.py:4684:            if not pool:
> video_processor.py:4685:                # Fallback minimal: a few tokens from transcript
  video_processor.py:4686:                try:
> video_processor.py:4687:                    transcript_tokens = [w for s in (subtitles or []) for w 
in str(s.get("text", "")).split()]
  video_processor.py:4688:                except Exception:
  video_processor.py:4689:                    transcript_tokens = []
  video_processor.py:4690:                pool = [t for t in transcript_tokens if isinstance(t, str) 
and len(t) >= 4][:10]
  video_processor.py:4691:
  video_processor.py:4692:            language = dyn_context.get("language")
> video_processor.py:4693:            selector_keywords = enforce_fetch_language(_dedupe_queries(pool, 
cap=12), language)
> video_processor.py:4694:            fetch_keywords = enforce_fetch_language(_dedupe_queries(pool, 
cap=8), language)
  video_processor.py:4695:
  video_processor.py:4696:            # Ensure bilingual queries when original language isn't English, 
if LLM provided EN
  video_processor.py:4697:            try:
  video_processor.py:4705:                except Exception:
  video_processor.py:4706:                    return False
  video_processor.py:4707:            if lang and lang != "en":
> video_processor.py:4708:                # If no English queries made it into fetch_keywords, try to 
inject 1-2 EN ones from LLM output
> video_processor.py:4709:                has_en = any(_is_english(q) for q in fetch_keywords)
  video_processor.py:4710:                if not has_en:
  video_processor.py:4711:                    en_from_llm = [q for q in 
(dyn_context.get("search_queries") or []) if isinstance(q, str) and _is_english(q)]
  video_processor.py:4712:                    if en_from_llm:
> video_processor.py:4713:                        fetch_keywords = 
_dedupe_queries(list(fetch_keywords) + en_from_llm[:2], cap=8)
  video_processor.py:4714:
  video_processor.py:4715:            try:
  video_processor.py:4716:                dom_names = [d.get("name") for d in 
(dyn_context.get("detected_domains", []) or []) if isinstance(d, dict)]
  video_processor.py:4717:            except Exception:
  video_processor.py:4718:                dom_names = []
  video_processor.py:4719:            print(f"Detected domains (free): {dom_names}")
> video_processor.py:4720:            print(f"Selector keywords: {selector_keywords}")
> video_processor.py:4721:            print(f"Fetch keywords: {fetch_keywords}")
  video_processor.py:4722:
  video_processor.py:4723:            # Expose dynamic context to pipeline_core segment loop
  video_processor.py:4724:            try:
  video_processor.py:4726:            except Exception:
  video_processor.py:4727:                pass
  video_processor.py:4728:
> video_processor.py:4729:            # Expose keyword lists for downstream reporting
  video_processor.py:4730:            try:
> video_processor.py:4731:                self._selector_keywords = list(selector_keywords)
> video_processor.py:4732:                self._fetch_keywords = list(fetch_keywords)
  video_processor.py:4733:            except Exception:
  video_processor.py:4734:                pass
  video_processor.py:4735:
  video_processor.py:4736:            # Observability: log dynamic context summary if logger available
  video_processor.py:4737:            try:
> video_processor.py:4738:                event_logger = self._get_broll_event_logger()
  video_processor.py:4739:                if event_logger:
  video_processor.py:4740:                    event_logger.log({
> video_processor.py:4741:                        "event": "broll_dynamic_context",
  video_processor.py:4742:                        "domains": dyn_context.get("detected_domains", []),
  video_processor.py:4743:                        "kw_count": len(llm_kw),
  video_processor.py:4744:                        "synonyms_count": sum(len(v or []) for v in 
(syn_map.values() if isinstance(syn_map, dict) else [])),
> video_processor.py:4745:                        "selector_keywords": selector_keywords,
> video_processor.py:4746:                        "fetch_keywords": fetch_keywords,
  video_processor.py:4747:                    })
  video_processor.py:4748:            except Exception:
  video_processor.py:4749:                pass
  video_processor.py:4750:
  video_processor.py:4751:            legacy_segment_payloads: List[Dict[str, Any]] = []
  video_processor.py:4752:            core_segments: List[_CoreSegment] = []
> video_processor.py:4753:            for s in subtitles:
  video_processor.py:4754:                text = str(s.get('text', '')).strip()
  video_processor.py:4755:                if not text:
  video_processor.py:4756:                    continue
  video_processor.py:4769:                print("    âšï¸ Aucun segment de transcription valide, saut 
B-roll")
  video_processor.py:4770:                return input_path
  video_processor.py:4771:
> video_processor.py:4772:            if _legacy_pipeline_fallback_enabled():
  video_processor.py:4773:                # Assurer l'import du pipeline local (src/*)
> video_processor.py:4774:                if str(broll_root.resolve()) not in sys.path:
> video_processor.py:4775:                    sys.path.insert(0, str(broll_root.resolve()))
  video_processor.py:4776:            
  video_processor.py:4777:                # ðŸš€ NOUVEAUX IMPORTS INTELLIGENTS SYNCHRONES 
(DÃ‰SACTIVÃ‰S POUR PROMPT OPTIMISÃ‰)
  video_processor.py:4778:                try:
  video_processor.py:4779:                    from sync_context_analyzer import SyncContextAnalyzer
> video_processor.py:4780:                    from broll_diversity_manager import BrollDiversityManager
  video_processor.py:4781:                    # ðŸš¨ DÃ‰SACTIVATION TEMPORAIRE: Le systÃ¨me 
intelligent interfÃ¨re avec notre prompt optimisÃ© LLM
> video_processor.py:4782:                    INTELLIGENT_BROLL_AVAILABLE = False
  video_processor.py:4783:                    print("    âšï¸  SystÃ¨me intelligent DÃ‰SACTIVÃ‰ pour 
laisser le prompt optimisÃ© LLM fonctionner")
> video_processor.py:4784:                    print("    ðŸŽ¯ Utilisation exclusive du prompt 
optimisÃ©: 25-35 keywords + structure hiÃ©rarchique")
  video_processor.py:4785:                except ImportError as e:
  video_processor.py:4786:                    print(f"    âšï¸  SystÃ¨me intelligent non disponible: 
{e}")
> video_processor.py:4787:                    print("    ðŸ”„ Fallback vers ancien systÃ¨me...")
> video_processor.py:4788:                    INTELLIGENT_BROLL_AVAILABLE = False
  video_processor.py:4789:            
  video_processor.py:4790:                # Imports B-roll dans tous les cas
> video_processor.py:4791:                from src.pipeline.config import BrollConfig  # type: ignore
> video_processor.py:4792:                from src.pipeline.keyword_extraction import 
extract_keywords_for_segment  # type: ignore
> video_processor.py:4793:                from src.pipeline.timeline_legacy import 
plan_broll_insertions, normalize_timeline, enrich_keywords  # type: ignore
  video_processor.py:4794:                from src.pipeline.renderer import render_video  # type: 
ignore
  video_processor.py:4795:                from src.pipeline.transcription import TranscriptSegment  # 
type: ignore
  video_processor.py:4796:
  video_processor.py:4809:                ]
  video_processor.py:4810:            
  video_processor.py:4811:                # ðŸ§  ANALYSE INTELLIGENTE AVANCÃ‰E
> video_processor.py:4812:                if INTELLIGENT_BROLL_AVAILABLE:
  video_processor.py:4813:                    print("    ðŸ§ Utilisation du systÃ¨me B-roll 
intelligent...")
  video_processor.py:4814:                    try:
  video_processor.py:4815:                        # Initialiser l'analyseur contextuel intelligent 
SYNCHRONE
  video_processor.py:4816:                        context_analyzer = SyncContextAnalyzer()
  video_processor.py:4817:                    
  video_processor.py:4818:                        # Analyser le contexte global de la vidÃ©o
> video_processor.py:4819:                        transcript_text = " ".join([s.get('text', '') for s 
in subtitles])
  video_processor.py:4820:                        global_analysis = 
context_analyzer.analyze_context(transcript_text)
  video_processor.py:4821:                    
  video_processor.py:4822:                        print(f"    ðŸŽ¯ Contexte dÃ©tectÃ©: 
{global_analysis.main_theme}")
  video_processor.py:4823:                        print(f"    ðŸ§¬ Sujets: {', 
'.join(global_analysis.key_topics[:3])}")
  video_processor.py:4824:                        print(f"    ðŸ˜Š Sentiment: 
{global_analysis.sentiment}")
  video_processor.py:4825:                        print(f"    ðŸ“Š ComplexitÃ©: 
{global_analysis.complexity}")
> video_processor.py:4826:                        print(f"    ðŸ”‘ Mots-clÃ©s: {', 
'.join(global_analysis.keywords[:5])}")
  video_processor.py:4827:                    
  video_processor.py:4828:                        # Persister l'analyse intelligente
  video_processor.py:4829:                        try:
  video_processor.py:4830:                            meta_dir = Config.OUTPUT_FOLDER / 'meta'
  video_processor.py:4831:                            meta_dir.mkdir(parents=True, exist_ok=True)
> video_processor.py:4832:                            meta_path = meta_dir / 
f"{Path(input_path).stem}_intelligent_broll_metadata.json"
  video_processor.py:4833:                            with open(meta_path, 'w', encoding='utf-8') as f:
> video_processor.py:4834:                                json.dump({
  video_processor.py:4835:                                    'intelligent_analysis': {
  video_processor.py:4836:                                        'main_theme': 
global_analysis.main_theme,
  video_processor.py:4837:                                        'key_topics': 
global_analysis.key_topics,
  video_processor.py:4838:                                        'sentiment': 
global_analysis.sentiment,
  video_processor.py:4839:                                        'complexity': 
global_analysis.complexity,
> video_processor.py:4840:                                        'keywords': global_analysis.keywords,
  video_processor.py:4841:                                        'context_score': 
global_analysis.context_score
  video_processor.py:4842:                                    },
  video_processor.py:4843:                                    'timestamp': str(datetime.now())
  video_processor.py:4849:                            try:
  video_processor.py:4850:                                # CrÃ©er un dossier unique pour ce clip
  video_processor.py:4851:                                clip_id = input_path.stem
> video_processor.py:4852:                                unique_broll_dir = broll_library / 
f"clip_intelligent_{clip_id}_{int(time.time())}"
> video_processor.py:4853:                                unique_broll_dir.mkdir(parents=True, 
exist_ok=True)
  video_processor.py:4854:                            
  video_processor.py:4855:                                # GÃ©nÃ©rer des prompts intelligents basÃ©s 
sur l'analyse
  video_processor.py:4856:                                intelligent_prompts = []
  video_processor.py:4857:                                main_theme = global_analysis.main_theme
> video_processor.py:4858:                                kws = 
_filter_prompt_terms(global_analysis.keywords[:6]) if hasattr(global_analysis, 'keywords') else []
  video_processor.py:4859:                                if main_theme == 'technology':
  video_processor.py:4860:                                    intelligent_prompts.extend([
  video_processor.py:4861:                                        'artificial intelligence neural 
network',
  video_processor.py:4884:                                    base = _filter_prompt_terms([main_theme] 
+ kws)
  video_processor.py:4885:                                    
intelligent_prompts.extend([f"{main_theme} {kw}" for kw in base[:3]])
  video_processor.py:4886:
> video_processor.py:4887:                                # Ajouter variantes from cleaned keywords
  video_processor.py:4888:                                for kw in kws[:3]:
  video_processor.py:4889:                                    
intelligent_prompts.append(f"{main_theme} {kw}")
  video_processor.py:4890:
  video_processor.py:4900:                            
  video_processor.py:4901:                            except Exception as e:
  video_processor.py:4902:                                print(f"    âšï¸  Erreur insertion 
intelligente: {e}")
> video_processor.py:4903:                                print("    ðŸ”„ Fallback vers ancien 
systÃ¨me...")
> video_processor.py:4904:                                INTELLIGENT_BROLL_AVAILABLE = False
  video_processor.py:4905:                            
  video_processor.py:4906:                        except Exception as e:
  video_processor.py:4907:                            print(f"    âšï¸  Erreur systÃ¨me intelligent: 
{e}")
> video_processor.py:4908:                            print("    ðŸ”„ Fallback vers ancien 
systÃ¨me...")
> video_processor.py:4909:                            INTELLIGENT_BROLL_AVAILABLE = False
  video_processor.py:4910:                    except Exception as e:
  video_processor.py:4911:                        print(f"    âšï¸  Erreur systÃ¨me intelligent: {e}")
> video_processor.py:4912:                        print("    ðŸ”„ Fallback vers ancien systÃ¨me...")
> video_processor.py:4913:                        INTELLIGENT_BROLL_AVAILABLE = False
  video_processor.py:4914:                    
> video_processor.py:4915:                # Fallback: ancienne analyse si systÃ¨me intelligent 
indisponible
> video_processor.py:4916:                if not INTELLIGENT_BROLL_AVAILABLE:
  video_processor.py:4917:                    print("    ðŸ”„ Utilisation de l'ancien systÃ¨me 
B-roll...")
> video_processor.py:4918:                    analysis = extract_keywords_from_transcript_ai(subtitles)
> video_processor.py:4919:                    prompts = generate_broll_prompts_ai(analysis)
> video_processor.py:4920:                    # Filtrer les prompts fallback
  video_processor.py:4921:                    try:
  video_processor.py:4922:                        cleaned_prompts = []
  video_processor.py:4923:                        for p in prompts:
  video_processor.py:4932:                    try:
  video_processor.py:4933:                        meta_dir = Config.OUTPUT_FOLDER / 'meta'
  video_processor.py:4934:                        meta_dir.mkdir(parents=True, exist_ok=True)
> video_processor.py:4935:                        meta_path = meta_dir / 
f"{Path(input_path).stem}_broll_metadata.json"
  video_processor.py:4936:                        with open(meta_path, 'w', encoding='utf-8') as f:
> video_processor.py:4937:                            json.dump({'analysis': analysis, 'prompts': 
prompts}, f, ensure_ascii=False, indent=2)
  video_processor.py:4938:                    except Exception:
  video_processor.py:4939:                        pass
  video_processor.py:4940:                else:
  video_processor.py:4946:                            'main_theme': global_analysis.main_theme,
  video_processor.py:4947:                            'key_topics': global_analysis.key_topics,
  video_processor.py:4948:                            'sentiment': global_analysis.sentiment,
> video_processor.py:4949:                            'keywords': global_analysis.keywords
  video_processor.py:4950:                        }
  video_processor.py:4951:                    
  video_processor.py:4952:                        # Utiliser les prompts intelligents gÃ©nÃ©rÃ©s
  video_processor.py:4953:                        prompts = intelligent_prompts if 
'intelligent_prompt' in locals() else [
> video_processor.py:4954:                            f"{global_analysis.main_theme} {kw}" for kw in 
global_analysis.keywords[:3]
  video_processor.py:4955:                        ]
  video_processor.py:4956:                    
  video_processor.py:4957:                        print(f"    ðŸŽ¯ Prompts utilisÃ©s: {', 
'.join(prompts[:3])}")
  video_processor.py:4958:                    
  video_processor.py:4959:                    except Exception as e:
  video_processor.py:4960:                        print(f"    âšï¸  Erreur prompts intelligents: {e}")
> video_processor.py:4961:                        # Fallback vers prompts gÃ©nÃ©riques
> video_processor.py:4962:                        analysis = 
extract_keywords_from_transcript_ai(subtitles)
> video_processor.py:4963:                        prompts = generate_broll_prompts_ai(analysis)
  video_processor.py:4964:            
  video_processor.py:4965:                # ðŸš€ NOUVEAU: IntÃ©gration des mots-clÃ©s B-roll du LLM
  video_processor.py:4966:                # RÃ©cupÃ©rer les mots-clÃ©s B-roll gÃ©nÃ©rÃ©s par le LLM 
(si disponibles)
> video_processor.py:4967:                llm_broll_keywords = []
  video_processor.py:4968:                try:
  video_processor.py:4969:                    # Les mots-clÃ©s B-roll sont dÃ©jÃ disponibles depuis 
generate_caption_and_hashtags
> video_processor.py:4970:                    # Ils sont passÃ©s via la variable broll_keywords dans 
le scope parent
> video_processor.py:4971:                    if 'broll_keywords' in locals():
> video_processor.py:4972:                        llm_broll_keywords = broll_keywords
> video_processor.py:4973:                        print(f"    ðŸ§ Mots-clÃ©s B-roll LLM intÃ©grÃ©s: 
{len(llm_broll_keywords)} termes")
> video_processor.py:4974:                        print(f"    ðŸŽ¯ Exemples: {', 
'.join(llm_broll_keywords[:5])}")
  video_processor.py:4975:                    else:
  video_processor.py:4976:                        print("    âšï¸ Mots-clÃ©s B-roll LLM non 
disponibles")
  video_processor.py:4977:                except Exception as e:
  video_processor.py:4978:                    print(f"    âšï¸ Erreur rÃ©cupÃ©ration mots-clÃ©s 
B-roll LLM: {e}")
  video_processor.py:4979:            
  video_processor.py:4980:                # Combiner les mots-clÃ©s LLM avec les prompts existants
> video_processor.py:4981:                if llm_broll_keywords:
  video_processor.py:4982:                    # Enrichir les prompts avec les mots-clÃ©s LLM
  video_processor.py:4983:                    enhanced_prompts = []
> video_processor.py:4984:                    for kw in llm_broll_keywords[:8]:  # Limiter Ã 8 
mots-clÃ©s principaux
  video_processor.py:4985:                        enhanced_prompts.append(kw)
  video_processor.py:4986:                        # CrÃ©er des combinaisons avec le thÃ¨me principal
  video_processor.py:4987:                        if 'global_analysis' in locals() and 
hasattr(global_analysis, 'main_theme'):
  video_processor.py:5004:            
  video_processor.py:5005:            core_result = self._maybe_use_pipeline_core(
  video_processor.py:5006:                core_segments,
> video_processor.py:5007:                broll_keywords,
> video_processor.py:5008:                subtitles=subtitles,
  video_processor.py:5009:                input_path=input_path,
  video_processor.py:5010:            )
  video_processor.py:5011:            core_inserted: Optional[int] = None
  video_processor.py:5015:                core_inserted, core_path, core_plan, core_render_ok = 
self._normalize_core_result(core_result)
  video_processor.py:5016:                display_count = core_inserted or 0
  video_processor.py:5017:                render_ok_flag = core_render_ok if core_render_ok is not 
None else (display_count > 0)
> video_processor.py:5018:                success, banner = format_broll_completion_banner(
  video_processor.py:5019:                    display_count,
  video_processor.py:5020:                    origin="pipeline_core",
  video_processor.py:5021:                    render_ok=render_ok_flag,
  video_processor.py:5043:                            "pipeline_core selected %s assets but returned 
no selection plan; falling back to legacy pipeline",
  video_processor.py:5044:                            display_count,
  video_processor.py:5045:                        )
> video_processor.py:5046:                self._last_broll_insert_count = display_count
  video_processor.py:5047:
> video_processor.py:5048:            if not _legacy_pipeline_fallback_enabled():
  video_processor.py:5049:                legacy_logger = None
  video_processor.py:5050:                try:
> video_processor.py:5051:                    legacy_logger = self._get_broll_event_logger()
  video_processor.py:5052:                except Exception:
  video_processor.py:5053:                    legacy_logger = None
  video_processor.py:5054:                if legacy_logger is not None:
  video_processor.py:5063:                return input_path
  video_processor.py:5064:
  video_processor.py:5065:            # Construire la config du pipeline (fetch + embeddings activÃ©s, 
pas de limites)
> video_processor.py:5066:            cfg = BrollConfig(
  video_processor.py:5067:                input_video=str(input_path),
> video_processor.py:5068:                output_video=output_with_broll,
> video_processor.py:5069:                broll_library=broll_library,
  video_processor.py:5070:                srt_path=None,
> video_processor.py:5071:                render_subtitles=False,
> video_processor.py:5072:                            max_broll_ratio=0.65,           # CORRIGÃ‰: 90% 
â†’ 65% pour Ã©quilibre optimal
> video_processor.py:5073:            min_gap_between_broll_s=1.5,    # CORRIGÃ‰: 0.2s â†’ 1.5s pour 
respiration visuelle
> video_processor.py:5074:                            max_broll_clip_s=4.0,           # CORRIGÃ‰: 8.0s 
â†’ 4.0s pour B-rolls Ã©quilibrÃ©s
> video_processor.py:5075:            min_broll_clip_s=2.0,           # CORRIGÃ‰: 3.5s â†’ 2.0s pour 
durÃ©e optimale
  video_processor.py:5076:                use_whisper=False,
  video_processor.py:5077:                ffmpeg_preset="fast",
  video_processor.py:5078:                crf=23,
  video_processor.py:5079:                threads=0,
  video_processor.py:5080:                # Fetchers (stock)
> video_processor.py:5081:                enable_fetcher=getattr(Config, 'BROLL_FETCH_ENABLE', False),
> video_processor.py:5082:                fetch_provider=getattr(Config, 'BROLL_FETCH_PROVIDER', 
'pexels'),
  video_processor.py:5083:                pexels_api_key=getattr(Config, 'PEXELS_API_KEY', None),
  video_processor.py:5084:                pixabay_api_key=getattr(Config, 'PIXABAY_API_KEY', None),
> video_processor.py:5085:                fetch_max_per_keyword=getattr(Config, 
'BROLL_FETCH_MAX_PER_KEYWORD', 25),  # CORRIGÃ‰: 50 â†’ 25 pour qualitÃ© optimale
> video_processor.py:5086:                fetch_allow_videos=getattr(Config, 
'BROLL_FETCH_ALLOW_VIDEOS', True),
> video_processor.py:5087:                fetch_allow_images=getattr(Config, 
'BROLL_FETCH_ALLOW_IMAGES', True),  # ActivÃ©: images animÃ©es + Ken Burns
  video_processor.py:5088:                # Embeddings
> video_processor.py:5089:                use_embeddings=getattr(Config, 'BROLL_USE_EMBEDDINGS', True),
> video_processor.py:5090:                embedding_model_name=getattr(Config, 
'BROLL_EMBEDDING_MODEL', 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'),
> video_processor.py:5091:                contextual_config_path=getattr(Config, 
'CONTEXTUAL_CONFIG_PATH', Path('config/contextual_broll.yml')),
  video_processor.py:5092:                # Experimental FX toggle
  video_processor.py:5093:                enable_experimental_fx=getattr(Config, 
'ENABLE_EXPERIMENTAL_FX', False),
  video_processor.py:5094:            )
  video_processor.py:5095:            # FETCH DYNAMIQUE PAR CLIP: CrÃ©er un dossier unique et forcer 
le fetch Ã chaque fois
  video_processor.py:5096:            try:
> video_processor.py:5097:                from src.pipeline.fetchers import ensure_assets_for_keywords 
 # type: ignore
  video_processor.py:5098:                
  video_processor.py:5099:                # CrÃ©er un dossier unique pour ce clip (Ã©viter le partage 
entre clips)
  video_processor.py:5100:                clip_id = input_path.stem  # Nom du fichier sans extension
> video_processor.py:5101:                clip_broll_dir = broll_library / 
f"clip_{clip_id}_{int(time.time())}"
> video_processor.py:5102:                clip_broll_dir.mkdir(parents=True, exist_ok=True)
  video_processor.py:5103:                
  video_processor.py:5104:                # Forcer l'activation du fetcher pour chaque clip
  video_processor.py:5105:                setattr(cfg, 'enable_fetcher', True)
> video_processor.py:5106:                setattr(cfg, 'broll_library', str(clip_broll_dir))  # 
Utiliser le dossier unique
  video_processor.py:5107:                
  video_processor.py:5108:                print(f"    ðŸ”„ Fetch B-roll personnalisÃ© pour clip: 
{clip_id}")
> video_processor.py:5109:                print(f"    ðŸ“ Dossier B-roll unique: 
{clip_broll_dir.name}")
  video_processor.py:5110:                
  video_processor.py:5111:                # ðŸš€ NOUVEAU: IntÃ©gration du sÃ©lecteur B-roll gÃ©nÃ©rique
> video_processor.py:5112:                if BROLL_SELECTOR_AVAILABLE and getattr(Config, 
'BROLL_SELECTOR_ENABLED', True):
  video_processor.py:5113:                    try:
  video_processor.py:5114:                        print("    ðŸŽ¯ SÃ©lecteur B-roll gÃ©nÃ©rique 
activÃ© - Scoring mixte intelligent")
  video_processor.py:5115:                        
  video_processor.py:5116:                        # Initialiser le sÃ©lecteur avec la configuration
  video_processor.py:5117:                        selector_config = None
> video_processor.py:5118:                        if getattr(Config, 'BROLL_SELECTOR_CONFIG_PATH', 
None):
  video_processor.py:5119:                            try:
  video_processor.py:5120:                                import yaml
> video_processor.py:5121:                                with open(Config.BROLL_SELECTOR_CONFIG_PATH, 
'r', encoding='utf-8') as f:
  video_processor.py:5122:                                    selector_config = yaml.safe_load(f)
> video_processor.py:5123:                                print(f"    âš™ï¸ Configuration chargÃ©e: 
{Config.BROLL_SELECTOR_CONFIG_PATH}")
  video_processor.py:5124:                            except Exception as e:
  video_processor.py:5125:                                print(f"    âšï¸ Erreur chargement config: 
{e}")
  video_processor.py:5126:                        
  video_processor.py:5127:                        # CrÃ©er le sÃ©lecteur
> video_processor.py:5128:                        from broll_selector import BrollSelector
> video_processor.py:5129:                        broll_selector = BrollSelector(selector_config)
  video_processor.py:5130:                        
  video_processor.py:5131:                        # Analyser le contexte pour la sÃ©lection 
intelligente
> video_processor.py:5132:                        context_keywords = []
  video_processor.py:5133:                        if 'global_analysis' in locals():
> video_processor.py:5134:                            context_keywords = global_analysis.keywords[:10] 
if hasattr(global_analysis, 'keywords') else []
  video_processor.py:5135:                        else:
> video_processor.py:5136:                            # Fallback vers extraction basique
> video_processor.py:5137:                            for s in subtitles:
  video_processor.py:5138:                                text = s.get('text', '')
  video_processor.py:5139:                                if text:
  video_processor.py:5140:                                    words = text.lower().split()
> video_processor.py:5141:                                    context_keywords.extend([w for w in 
words if len(w) > 3 and w.isalpha()])
  video_processor.py:5142:                        
  video_processor.py:5143:                        # DÃ©tecter le domaine
  video_processor.py:5144:                        detected_domain = None
  video_processor.py:5146:                            detected_domain = global_analysis.main_theme
  video_processor.py:5147:                        
  video_processor.py:5148:                        print(f"    ðŸŽ¯ Contexte: {detected_domain or 
'gÃ©nÃ©ral'}")
> video_processor.py:5149:                        print(f"    ðŸ”‘ Mots-clÃ©s contextuels: {', 
'.join(context_keywords[:5])}")
  video_processor.py:5150:                        
  video_processor.py:5151:                        # Utiliser le sÃ©lecteur pour la planification
  video_processor.py:5152:                        # Domaine effectif pour le sÃ©lecteur (dyn > global)
  video_processor.py:5161:                        except Exception:
  video_processor.py:5162:                            pass
  video_processor.py:5163:                        try:
> video_processor.py:5164:                            ev = self._get_broll_event_logger()
  video_processor.py:5165:                            if ev:
> video_processor.py:5166:                                ev.log({'event': 'broll_selector_domain', 
'domain': effective_domain, 'source': source, 'confidence': dyn_dom_conf})
  video_processor.py:5167:                        except Exception:
  video_processor.py:5168:                            pass
  video_processor.py:5169:
  video_processor.py:5170:                        try:
> video_processor.py:5171:                            selection_report = broll_selector.select_brolls(
> video_processor.py:5172:                                keywords=selector_keywords,
  video_processor.py:5173:                                domain=effective_domain,
> video_processor.py:5174:                                
min_delay=self._load_broll_selector_config().get('thresholds', {}).get('min_delay_seconds', 4.0),
> video_processor.py:5175:                                
desired_count=self._load_broll_selector_config().get('desired_broll_count', 3)
  video_processor.py:5176:                            )
  video_processor.py:5177:                        except TypeError:
> video_processor.py:5178:                            selection_report = broll_selector.select_brolls(
> video_processor.py:5179:                                keywords=selector_keywords,
> video_processor.py:5180:                                
min_delay=self._load_broll_selector_config().get('thresholds', {}).get('min_delay_seconds', 4.0),
> video_processor.py:5181:                                
desired_count=self._load_broll_selector_config().get('desired_broll_count', 3)
  video_processor.py:5182:                            )
  video_processor.py:5183:                        
  video_processor.py:5184:                        # Sauvegarder le rapport de sÃ©lection
  video_processor.py:5185:                        try:
  video_processor.py:5186:                            meta_dir = Config.OUTPUT_FOLDER / 'meta'
  video_processor.py:5187:                            meta_dir.mkdir(parents=True, exist_ok=True)
> video_processor.py:5188:                            selection_report_path = meta_dir / 
f"{Path(input_path).stem}_broll_selection_report.json"
  video_processor.py:5189:                            with open(selection_report_path, 'w', 
encoding='utf-8') as f:
> video_processor.py:5190:                                json.dump(selection_report, f, 
ensure_ascii=False, indent=2)
  video_processor.py:5191:                            print(f"    ðŸ’¾ Rapport de sÃ©lection 
sauvegardÃ©: {selection_report_path}")
  video_processor.py:5192:                        except Exception as e:
  video_processor.py:5193:                            print(f"    âšï¸ Erreur sauvegarde rapport: 
{e}")
  video_processor.py:5199:                            print(f"    ðŸŽ¯ Top score: 
{diag.get('top_score', 0):.3f}")
  video_processor.py:5200:                            print(f"    ðŸ“ Seuil appliquÃ©: 
{diag.get('min_score', 0):.3f}")
  video_processor.py:5201:                        
> video_processor.py:5202:                        if selection_report.get('fallback_used'):
> video_processor.py:5203:                            print(f"    ðŸ†˜ Fallback activÃ©: Tier 
{selection_report.get('fallback_tier', '?')}")
  video_processor.py:5204:                        
  video_processor.py:5205:                    except Exception as e:
  video_processor.py:5206:                        print(f"    âšï¸ Erreur sÃ©lecteur gÃ©nÃ©rique: 
{e}")
> video_processor.py:5207:                        print("    ðŸ”„ Fallback vers systÃ¨me existant")
  video_processor.py:5208:                
  video_processor.py:5209:                # ðŸš€ CORRECTION: IntÃ©gration des mots-clÃ©s LLM pour le 
fetch
  video_processor.py:5210:                # SÃ‰LECTION INTELLIGENTE: Mots-clÃ©s contextuels + concepts 
associÃ©s
  video_processor.py:5212:                kw_pool: list[str] = []
  video_processor.py:5213:                
  video_processor.py:5214:                # ðŸ§  PRIORITÃ‰ 1: Mots-clÃ©s LLM si disponibles
> video_processor.py:5215:                if 'broll_keywords' in locals() and broll_keywords:
> video_processor.py:5216:                    print(f"    ðŸš€ Utilisation des mots-clÃ©s LLM pour le 
fetch: {len(broll_keywords)} termes")
  video_processor.py:5217:                    # Ajouter TOUS les mots-clÃ©s LLM en prioritÃ©
> video_processor.py:5218:                    for kw in broll_keywords:
  video_processor.py:5219:                        low = (kw or '').strip().lower()
  video_processor.py:5220:                        if low and len(low) >= 3:
  video_processor.py:5221:                            kw_pool.append(low)
  video_processor.py:5224:                                parts = low.split()
  video_processor.py:5225:                                kw_pool.extend(parts)
  video_processor.py:5226:                    
> video_processor.py:5227:                    print(f"    ðŸŽ¯ Mots-clÃ©s LLM ajoutÃ©s: {', 
'.join(broll_keywords[:8])}")
  video_processor.py:5228:                
  video_processor.py:5229:                # ðŸ”„ PRIORITÃ‰ 2: Extraction des mots-clÃ©s du transcript
> video_processor.py:5230:                for s in subtitles:
> video_processor.py:5231:                    base_kws = 
extract_keywords_for_segment(s.get('text','')) or []
> video_processor.py:5232:                    spacy_kws = 
self._extract_keywords_for_segment_spacy(s.get('text','')) or []
  video_processor.py:5233:                    for kw in (base_kws + spacy_kws):
  video_processor.py:5234:                        low = (kw or '').strip().lower()
  video_processor.py:5235:                        if low and len(low) >= 3:
  video_processor.py:5312:                counts = _Counter(kw_pool)
  video_processor.py:5313:                
  video_processor.py:5314:                # ðŸš¨ CORRECTION CRITIQUE: PRIORISER les mots-clÃ©s LLM sur 
les mots-clÃ©s gÃ©nÃ©riques
> video_processor.py:5315:                if 'broll_keywords' in locals() and broll_keywords:
  video_processor.py:5316:                    # Utiliser DIRECTEMENT les mots-clÃ©s LLM comme requÃªte 
principale
> video_processor.py:5317:                    llm_keywords = [kw.strip().lower() for kw in 
broll_keywords if kw and len(kw.strip()) >= 3]
> video_processor.py:5318:                    if llm_keywords:
  video_processor.py:5319:                        # Prendre les 8 premiers mots-clÃ©s LLM + 2 concepts 
associÃ©s
> video_processor.py:5320:                        top_kws = llm_keywords[:8]
  video_processor.py:5321:                        # Ajouter quelques concepts associÃ©s pour enrichir
  video_processor.py:5322:                        for kw in top_kws[:3]:  # Pour les 3 premiers 
mots-clÃ©s LLM
  video_processor.py:5323:                            for concept, related in concept_mapping.items():
  video_processor.py:5327:                        print(f"    ðŸš€ REQUÃŠTE LLM PRIORITAIRE: {' 
'.join(top_kws[:5])}")
  video_processor.py:5328:                    else:
  video_processor.py:5329:                        top_kws = [w for w,_n in counts.most_common(15)]
> video_processor.py:5330:                        print(f"    ðŸ”„ Fallback vers mots-clÃ©s 
gÃ©nÃ©riques: {' '.join(top_kws[:5])}")
  video_processor.py:5331:                else:
  video_processor.py:5332:                    top_kws = [w for w,_n in counts.most_common(15)]
  video_processor.py:5333:                    print(f"    ðŸ”„ Mots-clÃ©s gÃ©nÃ©riques: {' 
'.join(top_kws[:5])}")
  video_processor.py:5334:                
> video_processor.py:5335:                # Fallback intelligent selon le contexte
  video_processor.py:5336:                if not top_kws:
  video_processor.py:5337:                    top_kws = 
["focus","concentration","study","brain","mind","productivity","success"]
  video_processor.py:5338:                print(f"    ðŸ”Ž Fetch B-roll sur requÃªte: {' 
'.join(top_kws[:5])}")
> video_processor.py:5339:                # Provider auto-fallback si pas de clÃ©s -> archive
  video_processor.py:5340:                import os as _os
  video_processor.py:5341:                pex = getattr(Config, 'PEXELS_API_KEY', None) or 
_os.getenv('PEXELS_API_KEY')
  video_processor.py:5342:                pixa = getattr(Config, 'PIXABAY_API_KEY', None) or 
_os.getenv('PIXABAY_API_KEY')
  video_processor.py:5351:                if not any([pex, pixa, uns]):
  video_processor.py:5352:                    try:
  video_processor.py:5353:                        setattr(cfg, 'fetch_provider', 'archive')
> video_processor.py:5354:                        print("    ðŸŒ Providers: archive (aucune clÃ© API 
dÃ©tectÃ©e)")
  video_processor.py:5355:                    except Exception:
  video_processor.py:5356:                        pass
  video_processor.py:5357:                else:
> video_processor.py:5358:                    # ðŸš€ AMÃ‰LIORATION: Construire une liste de providers 
optimisÃ©e
  video_processor.py:5359:                    prov = []
  video_processor.py:5360:                    if pex:
  video_processor.py:5361:                        prov.append('pexels')
  video_processor.py:5368:                    
  video_processor.py:5369:                    # ðŸŽ¯ AJOUT SÃ‰CURISÃ‰: Archive.org comme source 
supplÃ©mentaire
  video_processor.py:5370:                    try:
> video_processor.py:5371:                        if prov:  # Si on a des providers avec clÃ©s API
  video_processor.py:5372:                            prov.append('archive')  # Ajouter Archive.org
> video_processor.py:5373:                            print(f"    ðŸŒ Providers: {','.join(prov)} 
(Archive.org + Giphy ajoutÃ©s pour variÃ©tÃ©)")
  video_processor.py:5374:                        else:
  video_processor.py:5375:                            prov = ['archive']  # Seulement Archive.org si 
pas de clÃ©s
> video_processor.py:5376:                            print(f"    ðŸŒ Providers: {','.join(prov)} 
(Archive.org uniquement)")
  video_processor.py:5377:                        
  video_processor.py:5378:                        setattr(cfg, 'fetch_provider', ",".join(prov))
  video_processor.py:5379:                    except Exception as e:
> video_processor.py:5380:                        # Fallback sÃ©curisÃ©
  video_processor.py:5381:                        try:
  video_processor.py:5382:                            if prov:
  video_processor.py:5383:                                setattr(cfg, 'fetch_provider', 
",".join(prov))
> video_processor.py:5384:                                print(f"    ðŸŒ Providers: {','.join(prov)} 
(fallback sÃ©curisÃ©)")
  video_processor.py:5385:                            else:
  video_processor.py:5386:                                setattr(cfg, 'fetch_provider', 'archive')
> video_processor.py:5387:                                print(f"    ðŸŒ Providers: archive 
(fallback ultime)")
  video_processor.py:5388:                        except Exception:
  video_processor.py:5389:                            pass
  video_processor.py:5390:                
  video_processor.py:5392:                    setattr(cfg, 'fetch_allow_images', True)
  video_processor.py:5393:                    # ðŸš€ OPTIMISATION MULTI-SOURCES: QualitÃ© optimale 
(CORRIGÃ‰)
  video_processor.py:5394:                    if uns and giphy:  # Si Unsplash ET Giphy sont 
disponibles
> video_processor.py:5395:                        setattr(cfg, 'fetch_max_per_keyword', 35)  # 
CORRIGÃ‰: 125 â†’ 35 pour qualitÃ© maximale
  video_processor.py:5396:                        print("    ðŸ“Š Configuration optimisÃ©e: 35 assets 
max + images activÃ©es (Unsplash + Giphy + Archive)")
  video_processor.py:5397:                    elif uns:  # Si seulement Unsplash est disponible
> video_processor.py:5398:                        setattr(cfg, 'fetch_max_per_keyword', 30)  # 
CORRIGÃ‰: 100 â†’ 30 pour qualitÃ© maximale
  video_processor.py:5399:                        print("    ðŸ“Š Configuration optimisÃ©e: 30 assets 
max + images activÃ©es (Unsplash + Archive)")
  video_processor.py:5400:                    elif giphy:  # Si seulement Giphy est disponible
> video_processor.py:5401:                        setattr(cfg, 'fetch_max_per_keyword', 30)  # 
CORRIGÃ‰: 100 â†’ 30 pour qualitÃ© avec GIFs
  video_processor.py:5402:                        print("    ðŸ“Š Configuration optimisÃ©e: 30 assets 
max + images activÃ©es (Giphy + Archive)")
  video_processor.py:5403:                    else:
> video_processor.py:5404:                        setattr(cfg, 'fetch_max_per_keyword', 25)  # 
CORRIGÃ‰: 75 â†’ 25 pour Archive.org
  video_processor.py:5405:                        print("    ðŸ“Š Configuration optimisÃ©e: 25 assets 
max + images activÃ©es (Archive.org)")
  video_processor.py:5406:                except Exception:
  video_processor.py:5407:                    pass
  video_processor.py:5408:                # DÃ©clencher le fetch dans le dossier unique du clip
> video_processor.py:5409:                ensure_assets_for_keywords(cfg, fetch_keywords, top_kws)
  video_processor.py:5410:                
  video_processor.py:5411:                # ðŸš¨ CORRECTION CRITIQUE: SYSTÃˆME D'UNICITÃ‰ DES B-ROLLS
  video_processor.py:5412:                # Ã‰viter la duplication des B-rolls entre vidÃ©os 
diffÃ©rentes
  video_processor.py:5413:                try:
  video_processor.py:5414:                    # CrÃ©er un fichier de traÃ§abilitÃ© des B-rolls 
utilisÃ©s
> video_processor.py:5415:                    broll_tracking_file = Config.OUTPUT_FOLDER / 'meta' / 
'broll_usage_tracking.json'
> video_processor.py:5416:                    broll_tracking_file.parent.mkdir(parents=True, 
exist_ok=True)
  video_processor.py:5417:                    
  video_processor.py:5418:                    # Charger l'historique des B-rolls utilisÃ©s
> video_processor.py:5419:                    broll_history = {}
> video_processor.py:5420:                    if broll_tracking_file.exists():
  video_processor.py:5421:                        try:
> video_processor.py:5422:                            with open(broll_tracking_file, 'r', 
encoding='utf-8') as f:
> video_processor.py:5423:                                broll_history = json.load(f)
  video_processor.py:5424:                        except Exception:
> video_processor.py:5425:                            broll_history = {}
  video_processor.py:5426:                    
  video_processor.py:5427:                    # Identifier les B-rolls disponibles pour ce clip
> video_processor.py:5428:                    available_brolls = []
> video_processor.py:5429:                    for asset_path in clip_broll_dir.rglob('*'):
  video_processor.py:5430:                        if asset_path.suffix.lower() in {'.mp4', '.mov', 
'.mkv', '.webm', '.jpg', '.jpeg', '.png'}:
  video_processor.py:5431:                            asset_hash = 
self._calculate_asset_hash(asset_path)
  video_processor.py:5432:                            asset_info = {
  video_processor.py:5438:                            }
  video_processor.py:5439:                            
  video_processor.py:5440:                            # VÃ©rifier si ce B-roll a dÃ©jÃ  Ã©tÃ© utilisÃ©
> video_processor.py:5441:                            if asset_hash in broll_history:
> video_processor.py:5442:                                asset_info['last_used'] = 
broll_history[asset_hash].get('last_used')
> video_processor.py:5443:                                asset_info['usage_count'] = 
broll_history[asset_hash].get('usage_count', 0)
  video_processor.py:5444:                            
> video_processor.py:5445:                            available_brolls.append(asset_info)
  video_processor.py:5446:                    
  video_processor.py:5447:                    # Trier par prioritÃ©: B-rolls jamais utilisÃ©s en 
premier, puis par anciennetÃ©
> video_processor.py:5448:                    available_brolls.sort(key=lambda x: (x['usage_count'], 
x['last_used'] or '1970-01-01'))
  video_processor.py:5449:                    
  video_processor.py:5450:                    # SÃ©lectionner les B-rolls uniques pour cette vidÃ©o
> video_processor.py:5451:                    selected_brolls = available_brolls[:3]  # 3 B-rolls 
uniques
  video_processor.py:5452:                    
  video_processor.py:5453:                    # Mettre Ã  jour l'historique d'utilisation
  video_processor.py:5454:                    current_time = datetime.now().isoformat()
> video_processor.py:5455:                    for broll in selected_brolls:
> video_processor.py:5456:                        broll_history[broll['hash']] = {
  video_processor.py:5457:                            'last_used': current_time,
> video_processor.py:5458:                            'usage_count': broll['usage_count'] + 1,
  video_processor.py:5459:                            'video_id': Path(input_path).stem
  video_processor.py:5460:                        }
  video_processor.py:5461:                    
  video_processor.py:5462:                    # Sauvegarder l'historique
> video_processor.py:5463:                    with open(broll_tracking_file, 'w', encoding='utf-8') as 
f:
> video_processor.py:5464:                        json.dump(broll_history, f, ensure_ascii=False, 
indent=2)
  video_processor.py:5465:                    
> video_processor.py:5466:                    print(f"    ðŸŽ¯ B-rolls uniques sÃ©lectionnÃ©s: 
{len(selected_brolls)} (Ã©vite duplication)")
  video_processor.py:5467:                    
  video_processor.py:5468:                except Exception as e:
  video_processor.py:5469:                    print(f"    âš ï¸ Erreur systÃ¨me d'unicitÃ©: {e}")
> video_processor.py:5470:                    # Fallback: utiliser tous les B-rolls disponibles
  video_processor.py:5471:                    pass
  video_processor.py:5472:                
  video_processor.py:5473:                # Comptage aprÃ¨s fetch dans le dossier du clip
  video_processor.py:5474:                try:
  video_processor.py:5475:                    _media_exts = 
{'.mp4','.mov','.mkv','.webm','.jpg','.jpeg','.png'}
> video_processor.py:5476:                    _after = [p for p in clip_broll_dir.rglob('*') if 
p.suffix.lower() in _media_exts]
  video_processor.py:5477:                    print(f"    ðŸ“¥ Fetch terminÃ©: {len(_after)} assets 
pour ce clip")
  video_processor.py:5478:                    
> video_processor.py:5479:                    # ðŸš¨ CORRECTION CRITIQUE: CrÃ©er fetched_brolls 
accessible globalement
> video_processor.py:5480:                    fetched_brolls = []
  video_processor.py:5481:                    for asset_path in _after:
  video_processor.py:5482:                        if asset_path.exists():
> video_processor.py:5483:                            fetched_brolls.append({
  video_processor.py:5484:                                'path': str(asset_path),
  video_processor.py:5485:                                'name': asset_path.name,
  video_processor.py:5486:                                'size': asset_path.stat().st_size if 
asset_path.exists() else 0
  video_processor.py:5487:                            })
  video_processor.py:5488:                    
> video_processor.py:5489:                    print(f"    ðŸŽ¯ {len(fetched_brolls)} B-rolls prÃªts 
pour l'assignation")
  video_processor.py:5490:                    
  video_processor.py:5491:                    if len(_after) == 0:
  video_processor.py:5492:                        print("    âšï¸ Aucun asset tÃ©lÃ©chargÃ©. VÃ©rifie 
les clÃ©s API et la connectivitÃ© rÃ©seau.")
  video_processor.py:5493:                except Exception:
> video_processor.py:5494:                    fetched_brolls = []
  video_processor.py:5495:                    print("    âšï¸ Erreur lors de la prÃ©paration des 
B-rolls fetchÃ©s")
  video_processor.py:5496:                
  video_processor.py:5497:                # Construire l'index FAISS pour ce clip spÃ©cifique
  video_processor.py:5498:                try:
  video_processor.py:5499:                    if 'build_index' in globals() and build_index is not 
None:  # type: ignore[name-defined]
> video_processor.py:5500:                        index_handle = build_index(str(clip_broll_dir), 
model_name='ViT-B/32')  # type: ignore[misc]
  video_processor.py:5501:                        print(f"    ðŸ§ Index FAISS construit pour 
{clip_id}: {len(_after)} assets")
  video_processor.py:5502:                except Exception:
  video_processor.py:5503:                    index_handle = None
  video_processor.py:5524:                stopwords = set()
  video_processor.py:5525:
  video_processor.py:5526:            # ðŸš€ CORRECTION: IntÃ©gration des mots-clÃ©s LLM dans la 
planification
> video_processor.py:5527:            # Planification: nouvelle API prÃ©fÃ©rÃ©e 
(plan_broll_insertions(segments, cfg, index))
  video_processor.py:5528:            
> video_processor.py:5529:            # ðŸš¨ CORRECTION CRITIQUE: fetched_brolls est dÃ©jÃ dÃ©clarÃ© 
plus haut, ne pas le redÃ©clarer !
> video_processor.py:5530:            # fetched_brolls = []  # âŒ SUPPRIMÃ‰: Cette ligne Ã©crase la 
variable fetchÃ©e !
  video_processor.py:5531:            
  video_processor.py:5532:            try:
> video_processor.py:5533:                plan = plan_broll_insertions(segments, cfg, index_handle)  # 
type: ignore[arg-type]
  video_processor.py:5534:            except Exception:
  video_processor.py:5535:                # ðŸš€ NOUVEAU: Utiliser les mots-clÃ©s LLM pour la 
planification
> video_processor.py:5536:                seg_keywords: List[List[str]] = []
  video_processor.py:5537:                
  video_processor.py:5538:                # ðŸ§  PRIORITÃ‰ 1: Mots-clÃ©s LLM si disponibles
> video_processor.py:5539:                if 'broll_keywords' in locals() and broll_keywords:
> video_processor.py:5540:                    print(f"    ðŸš€ Utilisation des mots-clÃ©s LLM pour la 
planification: {len(broll_keywords)} termes")
  video_processor.py:5541:                    # Distribuer les mots-clÃ©s LLM sur les segments
  video_processor.py:5542:                    for i, s in enumerate(segments):
  video_processor.py:5543:                        # Prendre 2-3 mots-clÃ©s LLM par segment
> video_processor.py:5544:                        start_idx = (i * 2) % len(broll_keywords)
> video_processor.py:5545:                        end_idx = min(start_idx + 2, len(broll_keywords))
> video_processor.py:5546:                        segment_llm_kws = broll_keywords[start_idx:end_idx]
  video_processor.py:5547:                        
  video_processor.py:5548:                        # Combiner avec extraction basique
> video_processor.py:5549:                        base_kws = extract_keywords_for_segment(s.text) or []
> video_processor.py:5550:                        spacy_kws = 
self._extract_keywords_for_segment_spacy(s.text) or []
  video_processor.py:5551:                        
  video_processor.py:5552:                        # ðŸŽ¯ PRIORITÃ‰ aux mots-clÃ©s LLM
  video_processor.py:5553:                        merged: List[str] = segment_llm_kws + base_kws + 
spacy_kws
  video_processor.py:5562:                                    cleaned.append(low)
  video_processor.py:5563:                                    seen.add(low)
  video_processor.py:5564:                        
> video_processor.py:5565:                        seg_keywords.append(cleaned[:15])  # AugmentÃ©: 12 
â†’ 15
  video_processor.py:5566:                        print(f"    ðŸŽ¯ Segment {i}: {len(cleaned)} 
mots-clÃ©s (LLM: {len(segment_llm_kws)})")
  video_processor.py:5567:                else:
> video_processor.py:5568:                    # ðŸ”„ Fallback: extraction basique uniquement
  video_processor.py:5569:                    print("    âšï¸ Mots-clÃ©s LLM non disponibles, 
utilisation extraction basique")
  video_processor.py:5570:                    for s in segments:
> video_processor.py:5571:                        base_kws = extract_keywords_for_segment(s.text) or []
> video_processor.py:5572:                        spacy_kws = 
self._extract_keywords_for_segment_spacy(s.text) or []
  video_processor.py:5573:                        merged: List[str] = []
  video_processor.py:5574:                        for kw in (base_kws + spacy_kws):
  video_processor.py:5575:                            if kw and kw.lower() not in merged:
  video_processor.py:5576:                                low = kw.lower()
  video_processor.py:5577:                                if not (len(low) < 5 and low in stopwords):
  video_processor.py:5578:                                    merged.append(low)
> video_processor.py:5579:                        seg_keywords.append(merged[:12])
  video_processor.py:5580:                
  video_processor.py:5581:                with _VFC(str(input_path)) as _tmp:
  video_processor.py:5582:                    duration = float(_tmp.duration)
> video_processor.py:5583:                plan = plan_broll_insertions(  # type: ignore[call-arg]
  video_processor.py:5584:                    segments,
> video_processor.py:5585:                    seg_keywords,
  video_processor.py:5586:                    total_duration=duration,
> video_processor.py:5587:                    max_broll_ratio=cfg.max_broll_ratio,
> video_processor.py:5588:                    min_gap_between_broll_s=cfg.min_gap_between_broll_s,
> video_processor.py:5589:                    max_broll_clip_s=cfg.max_broll_clip_s,
> video_processor.py:5590:                    min_broll_clip_s=cfg.min_broll_clip_s,
  video_processor.py:5591:                )
  video_processor.py:5592:                
  video_processor.py:5593:                # ðŸš¨ CORRECTION CRITIQUE: Assigner directement les B-rolls 
fetchÃ©s aux items du plan
> video_processor.py:5594:                if plan and fetched_brolls:
> video_processor.py:5595:                    print(f"    ðŸŽ¯ Assignation directe des 
{len(fetched_brolls)} B-rolls fetchÃ©s aux {len(plan)} items du plan...")
  video_processor.py:5596:                    
  video_processor.py:5597:                    # Filtrer les B-rolls valides
> video_processor.py:5598:                    valid_brolls = [broll for broll in fetched_brolls if 
broll.get('path') and Path(broll.get('path')).exists()]
  video_processor.py:5599:                    
> video_processor.py:5600:                    if valid_brolls:
  video_processor.py:5601:                        # Assigner les B-rolls aux items du plan
  video_processor.py:5602:                        for i, item in enumerate(plan):
> video_processor.py:5603:                            if i < len(valid_brolls):
> video_processor.py:5604:                                asset_path = valid_brolls[i]['path']
  video_processor.py:5605:                                
  video_processor.py:5606:                                # Assigner l'asset_path selon le type d'objet
  video_processor.py:5607:                                if hasattr(item, 'asset_path'):
  video_processor.py:5613:                            else:
  video_processor.py:5614:                                break
  video_processor.py:5615:                        
> video_processor.py:5616:                        print(f"    ðŸŽ‰ {min(len(plan), len(valid_brolls))} 
B-rolls assignÃ©s avec succÃ¨s au plan")
  video_processor.py:5617:                    else:
> video_processor.py:5618:                        print(f"    âšï¸ Aucun B-roll valide trouvÃ© dans 
fetched_brolls")
> video_processor.py:5619:                elif not fetched_brolls:
  video_processor.py:5620:                    print(f"    âšï¸ Aucun B-roll fetchÃ© disponible pour 
l'assignation")
  video_processor.py:5621:                elif not plan:
  video_processor.py:5622:                    print(f"    âš ï¸ Plan vide - aucun item Ã  traiter")
  video_processor.py:5671:                    "resultat": 0.8, "succÃ¨s": 0.9, "succes": 0.9, 
"rÃ©ussite": 0.9, "reussite": 0.9,
  video_processor.py:5672:                }
  video_processor.py:5673:                plan = score_candidates(
> video_processor.py:5674:                    plan, segments, broll_library=str(broll_library), 
clip_model='ViT-B/32',
> video_processor.py:5675:                    use_faiss=True, top_k=10, keyword_boosts=boosts
  video_processor.py:5676:                )
  video_processor.py:5677:                
  video_processor.py:5678:            except Exception:
  video_processor.py:5698:                seen: dict[str, float] = {}
  video_processor.py:5699:                new_plan = []
  video_processor.py:5700:                for it in plan:
> video_processor.py:5701:                    # ðŸ”§ CORRECTION: GÃ©rer Ã la fois BrollPlanItem et dict
  video_processor.py:5702:                    if hasattr(it, 'asset_path'):
  video_processor.py:5703:                        ap = it.asset_path
  video_processor.py:5704:                        st = float(it.start)
  video_processor.py:5706:                        ap = it.get('asset_path')
  video_processor.py:5707:                        st = float(it.get('start', 0.0))
  video_processor.py:5708:                    else:
> video_processor.py:5709:                        # Fallback pour autres types
  video_processor.py:5710:                        ap = getattr(it, 'asset_path', None)
  video_processor.py:5711:                        st = float(getattr(it, 'start', 0.0))
  video_processor.py:5712:                    
  video_processor.py:5761: 
  video_processor.py:5762:            # ðŸŽ¯ SCORING CONTEXTUEL RENFORCÃ‰: PÃ©naliser les assets non 
pertinents au domaine
  video_processor.py:5763:            try:
> video_processor.py:5764:                if plan and hasattr(global_analysis, 'main_theme') and 
hasattr(global_analysis, 'keywords'):
  video_processor.py:5765:                    domain = global_analysis.main_theme
> video_processor.py:5766:                    keywords = global_analysis.keywords[:10] if 
hasattr(global_analysis, 'keywords') else []
  video_processor.py:5767:                    
  video_processor.py:5768:                    for item in plan:
  video_processor.py:5769:                        if hasattr(item, 'asset_path') and item.asset_path:
  video_processor.py:5774:                            continue
  video_processor.py:5775:                        
  video_processor.py:5776:                        # Calculer le score contextuel
> video_processor.py:5777:                        context_score = 
_score_contextual_relevance(asset_path, domain, keywords)
  video_processor.py:5778:                        
  video_processor.py:5779:                        # Appliquer le score contextuel au score final
  video_processor.py:5780:                        if hasattr(item, 'score'):
  video_processor.py:5789:                        elif isinstance(item, dict):
  video_processor.py:5790:                            item['context_score'] = context_score
  video_processor.py:5791:                    
> video_processor.py:5792:                    print(f"    ðŸŽ¯ Scoring contextuel appliquÃ©: domaine 
'{domain}' avec {len(keywords)} mots-clÃ©s")
  video_processor.py:5793:                    
  video_processor.py:5794:                    # ðŸ” DEBUG B-ROLL SELECTION (si activÃ©)
> video_processor.py:5795:                    debug_mode = getattr(Config, 'DEBUG_BROLL', False) or 
os.getenv('DEBUG_BROLL', 'false').lower() == 'true'
> video_processor.py:5796:                    _debug_broll_selection(plan, domain, keywords, 
debug_mode)
  video_processor.py:5797:                    
> video_processor.py:5798:                    # ðŸš¨ FALLBACK PROPRE: Si aucun asset pertinent, 
utiliser des assets neutres
  video_processor.py:5799:                    # ðŸ”§ CORRECTION CRITIQUE: VÃ©rifier d'abord si les 
items ont des assets assignÃ©s
  video_processor.py:5800:                    items_without_assets = []
  video_processor.py:5801:                    items_with_assets = []
  video_processor.py:5810:                    
  video_processor.py:5811:                    print(f"    ðŸ” Analyse des assets: 
{len(items_with_assets)} avec assets, {len(items_without_assets)} sans assets")
  video_processor.py:5812:                    
> video_processor.py:5813:                    # ðŸš¨ CORRECTION: Assigner des assets aux items sans 
assets AVANT le fallback
> video_processor.py:5814:                    if items_without_assets and fetched_brolls:
  video_processor.py:5815:                        print(f"    ðŸŽ¯ Assignation d'assets aux 
{len(items_without_assets)} items sans assets...")
  video_processor.py:5816:                        
  video_processor.py:5817:                        # Utiliser les B-rolls fetchÃ©s pour assigner aux 
items
> video_processor.py:5818:                        available_assets = [broll.get('path', '') for broll 
in fetched_brolls if broll.get('path')]
  video_processor.py:5819:                        
  video_processor.py:5820:                        for i, item in enumerate(items_without_assets):
  video_processor.py:5821:                            if i < len(available_assets):
  video_processor.py:5833:                        items_with_assets = [item for item in plan if 
(hasattr(item, 'asset_path') and item.asset_path) or (isinstance(item, dict) and 
item.get('asset_path'))]
  video_processor.py:5834:                        items_without_assets = [item for item in plan if not 
((hasattr(item, 'asset_path') and item.asset_path) or (isinstance(item, dict) and 
item.get('asset_path')))]
  video_processor.py:5835:                    
> video_processor.py:5836:                    # ðŸš¨ FALLBACK UNIQUEMENT SI VRAIMENT NÃ‰CESSAIRE
  video_processor.py:5837:                    if not items_with_assets and items_without_assets:
> video_processor.py:5838:                        print(f"    âšï¸  Aucun asset disponible, 
activation du fallback neutre")
> video_processor.py:5839:                        fallback_assets = 
_get_fallback_neutral_assets(broll_library, count=3)
> video_processor.py:5840:                        if fallback_assets:
> video_processor.py:5841:                            print(f"    ðŸ†˜ Fallback neutre: 
{len(fallback_assets)} assets gÃ©nÃ©riques utilisÃ©s")
> video_processor.py:5842:                            # CrÃ©er des items de plan avec les assets de 
fallback
> video_processor.py:5843:                            for i, asset_path in enumerate(fallback_assets):
> video_processor.py:5844:                                fallback_item = {
> video_processor.py:5845:                                    'start': 3.0 + (i * 5.0),  # Espacer les 
fallbacks
  video_processor.py:5846:                                    'end': 3.0 + (i * 5.0) + 3.0,
  video_processor.py:5847:                                    'asset_path': asset_path,
  video_processor.py:5848:                                    'score': 0.5,  # Score neutre
  video_processor.py:5849:                                    'context_score': 0.3,  # Pertinence 
faible
> video_processor.py:5850:                                    'is_fallback': True
  video_processor.py:5851:                                }
> video_processor.py:5852:                                plan.append(fallback_item)
  video_processor.py:5853:                        else:
> video_processor.py:5854:                            print(f"    ðŸš¨ Aucun asset de fallback 
disponible")
  video_processor.py:5855:                    elif items_with_assets:
> video_processor.py:5856:                        print(f"    âœ… {len(items_with_assets)} items avec 
assets assignÃ©s - Pas de fallback nÃ©cessaire")
  video_processor.py:5857:                    else:
  video_processor.py:5858:                        print(f"    âšï¸  Plan vide - Aucun item Ã traiter")
  video_processor.py:5859:                    
  video_processor.py:5869:                
  video_processor.py:5870:                # ðŸš¨ NOUVEAU: Importer le systÃ¨me de scoring contextuel 
intelligent
  video_processor.py:5871:                try:
> video_processor.py:5872:                    from src.pipeline.broll_selector import 
get_contextual_broll_score
  video_processor.py:5873:                    print("    ðŸ§ SystÃ¨me de scoring contextuel 
intelligent activÃ©")
  video_processor.py:5874:                except ImportError:
  video_processor.py:5875:                    print("    âšï¸ SystÃ¨me de scoring contextuel non 
disponible")
> video_processor.py:5876:                    get_contextual_broll_score = None
  video_processor.py:5877:                
  video_processor.py:5878:                # UTILISER LE DOSSIER SPÃ‰CIFIQUE DU CLIP (pas la librairie 
globale)
> video_processor.py:5879:                clip_specific_dir = clip_broll_dir if 'clip_specific_dir' in 
locals() else broll_library
  video_processor.py:5880:                idx_bin = (clip_specific_dir / 'faiss.index')
> video_processor.py:5881:                idx_json = (clip_specific_dir / 'faiss.json')
  video_processor.py:5882:                
> video_processor.py:5883:                model_name = getattr(cfg, 'embedding_model_name', 
'clip-ViT-B/32')
> video_processor.py:5884:                st_model = SentenceTransformer('clip-ViT-B/32') if 'ViT' in 
model_name else SentenceTransformer(model_name)
  video_processor.py:5885:                def emb_text(t: str):
> video_processor.py:5886:                    v = st_model.encode([t])[0].astype('float32')
  video_processor.py:5887:                    n = _np.linalg.norm(v) + 1e-12
  video_processor.py:5888:                    return v / n
  video_processor.py:5889:                paths = []
> video_processor.py:5890:                if idx_json.exists():
> video_processor.py:5891:                    import json as _json
  video_processor.py:5892:                    try:
> video_processor.py:5893:                        paths = 
_json.loads(idx_json.read_text(encoding='utf-8')).get('paths', [])
  video_processor.py:5894:                    except Exception:
  video_processor.py:5895:                        paths = []
  video_processor.py:5896:                index = _faiss.read_index(str(idx_bin)) if idx_bin.exists() 
else None
  video_processor.py:5906:                    q = emb_text(local) if local else None
  video_processor.py:5907:                    
  video_processor.py:5908:                    # ðŸš¨ NOUVEAU: Extraction des mots-clÃ©s pour le 
scoring contextuel
> video_processor.py:5909:                    local_keywords = []
  video_processor.py:5910:                    if local:
  video_processor.py:5911:                        # Extraire les mots-clÃ©s du texte local
  video_processor.py:5912:                        words = local.lower().split()
> video_processor.py:5913:                        local_keywords = [w for w in words if len(w) > 3 and 
w.isalpha()][:10]
  video_processor.py:5914:                    
  video_processor.py:5915:                    chosen = None
  video_processor.py:5916:                    best_score = -1
  video_processor.py:5931:                                if str(cand) not in used_recent and 
cand.exists():
  video_processor.py:5932:                                    # ðŸš¨ NOUVEAU: Calcul du score 
contextuel intelligent
  video_processor.py:5933:                                    contextual_score = 0.0
> video_processor.py:5934:                                    if 'get_contextual_broll_score' in 
globals() and local_keywords:
  video_processor.py:5935:                                        try:
  video_processor.py:5936:                                            # Extraire les tokens et tags du 
fichier
  video_processor.py:5937:                                            asset_name = cand.stem.lower()
  video_processor.py:5938:                                            asset_tokens = 
asset_name.split('_')
  video_processor.py:5939:                                            asset_tags = 
asset_name.split('_')  # SimplifiÃ© pour l'exemple
> video_processor.py:5940:                                            contextual_score = 
get_contextual_broll_score(local_keywords, asset_tokens, asset_tags)
  video_processor.py:5941:                                        except Exception as e:
  video_processor.py:5942:                                            print(f"    âšï¸ Erreur scoring 
contextuel: {e}")
  video_processor.py:5943:                                            contextual_score = 0.0
  video_processor.py:5951:                                        chosen = str(cand)
  video_processor.py:5952:                        
  video_processor.py:5953:                        # ðŸš¨ NOUVEAU: Log de la sÃ©lection contextuelle
> video_processor.py:5954:                        if chosen and 'get_contextual_broll_score' in 
globals() and local_keywords:
  video_processor.py:5955:                            try:
  video_processor.py:5956:                                asset_name = Path(chosen).stem.lower()
  video_processor.py:5957:                                asset_tokens = asset_name.split('_')
  video_processor.py:5958:                                asset_tags = asset_name.split('_')
> video_processor.py:5959:                                final_contextual_score = 
get_contextual_broll_score(local_keywords, asset_tokens, asset_tags)
  video_processor.py:5960:                                print(f"    ðŸŽ¯ SÃ©lection contextuelle: 
{Path(chosen).stem} | Score: {best_score:.3f} | Contexte: {final_contextual_score:.2f}")
  video_processor.py:5961:                            except Exception:
  video_processor.py:5962:                                pass
  video_processor.py:5963:                    
  video_processor.py:5964:                    if chosen is None:
> video_processor.py:5965:                        # ðŸš¨ NOUVEAU: Fallback contextuel intelligent au 
lieu d'alÃ©atoire
> video_processor.py:5966:                        print(f"    ðŸ” Fallback contextuel pour segment: 
{local[:50]}...")
  video_processor.py:5967:                        for p in clip_specific_dir.rglob('*'):
  video_processor.py:5968:                            if p.suffix.lower() in 
{'.mp4','.mov','.mkv','.webm','.jpg','.jpeg','.png'}:
  video_processor.py:5969:                                if str(p.resolve()) not in used_recent and 
p.exists():
> video_processor.py:5970:                                    # ðŸš¨ NOUVEAU: Ã‰valuation contextuelle 
du fallback
> video_processor.py:5971:                                    if 'get_contextual_broll_score' in 
globals() and local_keywords:
  video_processor.py:5972:                                        try:
  video_processor.py:5973:                                            asset_name = p.stem.lower()
  video_processor.py:5974:                                            asset_tokens = 
asset_name.split('_')
  video_processor.py:5975:                                            asset_tags = 
asset_name.split('_')
> video_processor.py:5976:                                            fallback_score = 
get_contextual_broll_score(local_keywords, asset_tokens, asset_tags)
> video_processor.py:5977:                                            if fallback_score > 2.0:  # 
Seuil contextuel minimum
  video_processor.py:5978:                                                chosen = str(p.resolve())
> video_processor.py:5979:                                                print(f"    âœ… Fallback 
contextuel: {p.stem} | Score: {fallback_score:.2f}")
  video_processor.py:5980:                                                break
  video_processor.py:5981:                                        except Exception:
  video_processor.py:5982:                                            pass
  video_processor.py:5983:                                    else:
> video_processor.py:5984:                                        # Fallback sans scoring contextuel
  video_processor.py:5985:                                        chosen = str(p.resolve())
  video_processor.py:5986:                                        break
  video_processor.py:5987:                    
  video_processor.py:5996:            except Exception:
  video_processor.py:5997:                pass
  video_processor.py:5998:
> video_processor.py:5999:            # VÃ©rification des asset_path avant normalisation + mini 
fallback non invasif
  video_processor.py:6000:            try:
  video_processor.py:6001:                def _get_ap(x):
  video_processor.py:6002:                    return (getattr(x, 'asset_path', None) if hasattr(x, 
'asset_path') else (x.get('asset_path') if isinstance(x, dict) else None))
  video_processor.py:6003:                missing = [it for it in (plan or []) if not _get_ap(it)]
  video_processor.py:6004:                if plan and len(missing) == len(plan):
> video_processor.py:6005:                    # Aucun asset assignÃ© par FAISS â†’ mini fallback 
d'assignation sÃ©quentielle
  video_processor.py:6006:                    # UTILISER LE DOSSIER SPÃ‰CIFIQUE DU CLIP
> video_processor.py:6007:                    clip_specific_dir = clip_broll_dir if 
'clip_specific_dir' in locals() else broll_library
  video_processor.py:6008:                    lib_assets = [p for p in clip_specific_dir.rglob('*') if 
p.suffix.lower() in {'.mp4','.mov','.mkv','.webm','.jpg','.jpeg','.png'}]
  video_processor.py:6009:                    if lib_assets:
  video_processor.py:6010:                        for i, it in enumerate(plan):
  video_processor.py:6030:            except Exception:
  video_processor.py:6031:                fps_probe = 25.0
  video_processor.py:6032:            events = normalize_timeline(plan, fps=fps_probe)
> video_processor.py:6033:            events = enrich_keywords(events)
  video_processor.py:6034:            
  video_processor.py:6035:
  video_processor.py:6036:            
  video_processor.py:6037:            # Hard fail if no valid events
  video_processor.py:6038:            if not events:
> video_processor.py:6039:                raise RuntimeError('Aucun B-roll valide aprÃ¨s 
planification/scoring. VÃ©rifier l\'index FAISS et la librairie. Aucun fallback synthÃ©tique 
appliquÃ©.')
  video_processor.py:6040:            # Valider que les mÃ©dias existent
  video_processor.py:6041:            from pathlib import Path as _Path
  video_processor.py:6042:            valid_events = []
  video_processor.py:6044:                mp = getattr(ev, 'media_path', '')
  video_processor.py:6045:                pp = _Path(mp)
  video_processor.py:6046:                if not pp.exists() and mp and not pp.is_absolute():
> video_processor.py:6047:                    pp = (broll_library / mp).resolve()
  video_processor.py:6048:                    if pp.exists():
  video_processor.py:6049:                        try:
  video_processor.py:6050:                            setattr(ev, 'media_path', str(pp))
  video_processor.py:6060:            except Exception:
  video_processor.py:6061:                pass
  video_processor.py:6062:            if not valid_events:
> video_processor.py:6063:                # Fallback legacy: construire un plan simple Ã partir de la 
librairie existante
  video_processor.py:6064:                try:
  video_processor.py:6065:                    _media_exts = 
{'.mp4','.mov','.mkv','.webm','.jpg','.jpeg','.png'}
> video_processor.py:6066:                    assets = [p for p in broll_library.rglob('*') if 
p.suffix.lower() in _media_exts]
  video_processor.py:6067:                    assets.sort(key=lambda p: p.stat().st_size if p.exists() 
else 0, reverse=True)
  video_processor.py:6068:                    assets = assets[:20]
  video_processor.py:6069:                    if assets:
  video_processor.py:6100:                            except Exception:
  video_processor.py:6101:                                fps_probe = 25.0
  video_processor.py:6102:                            legacy_events = normalize_timeline(plan_simple, 
fps=fps_probe)
> video_processor.py:6103:                            legacy_events = enrich_keywords(legacy_events)
> video_processor.py:6104:                            print(f"    â™»ï¸ Fallback legacy appliquÃ©: 
{len(legacy_events)} events")
  video_processor.py:6105:                            valid_events = legacy_events
  video_processor.py:6106:                            # Continue vers le rendu unique plus bas
  video_processor.py:6107:                        else:
> video_processor.py:6108:                            raise RuntimeError('Librairie B-roll prÃ©sente 
mais aucun slot valide pour fallback legacy')
  video_processor.py:6109:                    else:
> video_processor.py:6110:                        raise RuntimeError('B-rolls planifiÃ©s, aucun 
media_path valide et aucune ressource en librairie pour fallback')
  video_processor.py:6111:                except Exception as _e:
> video_processor.py:6112:                    raise RuntimeError('B-rolls planifiÃ©s, mais aucun 
media_path valide trouvÃ©. Fallback legacy impossible: ' + str(_e))
> video_processor.py:6113:            # Rendu unique avec les events valides (incl. fallback le cas 
Ã©chÃ©ant)
  video_processor.py:6114:            render_video(cfg, segments, valid_events)
  video_processor.py:6115:
  video_processor.py:6116:            inserted_count = len(valid_events)
> video_processor.py:6117:            self._last_broll_insert_count = inserted_count
  video_processor.py:6118:
  video_processor.py:6119:            # VÃ‰RIFICATION ET NETTOYAGE INTELLIGENT DES B-ROLLS
  video_processor.py:6120:            try:
> video_processor.py:6121:                if getattr(Config, 'BROLL_DELETE_AFTER_USE', False):
  video_processor.py:6122:                    print("    ðŸ” VÃ©rification des B-rolls avant 
suppression...")
  video_processor.py:6123:                    
  video_processor.py:6124:                    # Importer le systÃ¨me de vÃ©rification
  video_processor.py:6125:                    try:
> video_processor.py:6126:                        from broll_verification_system import 
create_verification_system
  video_processor.py:6127:                        verifier = create_verification_system()
  video_processor.py:6128:                        
  video_processor.py:6129:                        # VÃ©rifier l'insertion des B-rolls
> video_processor.py:6130:                        verification_result = 
verifier.verify_broll_insertion(
  video_processor.py:6131:                            video_path=cfg.output_video,
> video_processor.py:6132:                            broll_plan=plan or [],
> video_processor.py:6133:                            broll_library_path=str(clip_broll_dir) if 
'clip_broll_dir' in locals() else "AI-B-roll/broll_library"
  video_processor.py:6134:                        )
  video_processor.py:6135:                        
  video_processor.py:6136:                        # ðŸš€ CORRECTION: VÃ©rifier le type du rÃ©sultat de 
vÃ©rification
  video_processor.py:6137:                        if not isinstance(verification_result, dict):
> video_processor.py:6138:                            print(f"    âšï¸ RÃ©sultat de vÃ©rification 
invalide (type: {type(verification_result)}) - Fallback vers vÃ©rification basique")
  video_processor.py:6139:                            verification_result = {
  video_processor.py:6140:                                "verification_passed": True,  # Par dÃ©faut, 
autoriser la suppression
  video_processor.py:6141:                                "issues": [],
  video_processor.py:6163:                                    pass
  video_processor.py:6164:                            
  video_processor.py:6165:                            # Marquer le dossier comme "utilisÃ©" mais le 
garder
> video_processor.py:6166:                            if 'clip_broll_dir' in locals() and 
clip_broll_dir.exists():
  video_processor.py:6167:                                try:
  video_processor.py:6168:                                    # CrÃ©er un fichier de statut pour 
indiquer que le clip est traitÃ©
> video_processor.py:6169:                                    status_file = clip_broll_dir / 
"STATUS_COMPLETED.txt"
  video_processor.py:6170:                                    status_file.write_text(f"Clip traitÃ© le 
{time.strftime('%Y-%m-%d %H:%M:%S')}\nB-rolls utilisÃ©s: {cleaned_count}\nVÃ©rification: PASSED\n", 
encoding='utf-8')
> video_processor.py:6171:                                    print(f"    ðŸ—‚ï¸ Dossier B-roll 
conservÃ©: {clip_broll_dir.name} (fichiers nettoyÃ©s: {cleaned_count})")
  video_processor.py:6172:                                except Exception as e:
  video_processor.py:6173:                                    print(f"    âšï¸ Erreur crÃ©ation 
statut: {e}")
  video_processor.py:6174:                        else:
  video_processor.py:6181:                                print(f"       â€¢ {rec}")
  video_processor.py:6182:                            
  video_processor.py:6183:                            # CrÃ©er un fichier de statut d'Ã©chec
> video_processor.py:6184:                            if 'clip_broll_dir' in locals() and 
clip_broll_dir.exists():
  video_processor.py:6185:                                try:
> video_processor.py:6186:                                    status_file = clip_broll_dir / 
"STATUS_FAILED.txt"
  video_processor.py:6187:                                    status_file.write_text(f"Clip traitÃ© le 
{time.strftime('%Y-%m-%d %H:%M:%S')}\nVÃ©rification: FAILED\nProblÃ¨mes: {', 
'.join(verification_result.get('issues', []))}\n", encoding='utf-8')
> video_processor.py:6188:                                    print(f"    ðŸš¨ Dossier B-roll marquÃ© 
comme Ã©chec: {clip_broll_dir.name}")
  video_processor.py:6189:                                except Exception as e:
  video_processor.py:6190:                                    print(f"    âšï¸ Erreur crÃ©ation 
statut d'Ã©chec: {e}")
  video_processor.py:6191:                    
  video_processor.py:6192:                    except ImportError:
  video_processor.py:6193:                        print("    âšï¸ SystÃ¨me de vÃ©rification non 
disponible - Suppression sans vÃ©rification")
> video_processor.py:6194:                        # Fallback vers l'ancien systÃ¨me
  video_processor.py:6195:                        used_files: List[str] = []
  video_processor.py:6196:                        for item in (plan or []):
  video_processor.py:6197:                            path = getattr(item, 'asset_path', None) if 
hasattr(item, 'asset_path') else (item.get('asset_path') if isinstance(item, dict) else None)
  video_processor.py:6206:                            except Exception:
  video_processor.py:6207:                                pass
  video_processor.py:6208:                        
> video_processor.py:6209:                        if 'clip_broll_dir' in locals() and 
clip_broll_dir.exists():
  video_processor.py:6210:                            try:
> video_processor.py:6211:                                status_file = clip_broll_dir / 
"STATUS_COMPLETED_NO_VERIFICATION.txt"
  video_processor.py:6212:                                status_file.write_text(f"Clip traitÃ© le 
{time.strftime('%Y-%m-%d %H:%M:%S')}\nB-rolls utilisÃ©s: {cleaned_count}\nVÃ©rification: NON 
DISPONIBLE\n", encoding='utf-8')
> video_processor.py:6213:                                print(f"    ðŸ—‚ï¸ Dossier B-roll 
conservÃ©: {clip_broll_dir.name} (fichiers nettoyÃ©s: {cleaned_count})")
  video_processor.py:6214:                            except Exception as e:
  video_processor.py:6215:                                print(f"    âšï¸ Erreur crÃ©ation statut: 
{e}")
  video_processor.py:6216:                    
  video_processor.py:6220:                pass
  video_processor.py:6221:
  video_processor.py:6222:            output_exists = Path(cfg.output_video).exists()
> video_processor.py:6223:            success, banner = format_broll_completion_banner(inserted_count, 
origin="legacy")
  video_processor.py:6224:
  video_processor.py:6225:            if output_exists and success:
  video_processor.py:6226:                print(banner)
  video_processor.py:6248:                "money", "handshake", "meeting", "audience", "lightbulb", 
"typing", "city", "success"
  video_processor.py:6249:            ]
  video_processor.py:6250:            # Chercher quelques mÃ©dias gÃ©nÃ©riques existants
> video_processor.py:6251:            for p in broll_library.rglob('*'):
  video_processor.py:6252:                if p.suffix.lower() in 
{'.mp4','.mov','.mkv','.webm','.jpg','.jpeg','.png'}:
  video_processor.py:6253:                    name = p.stem.lower()
  video_processor.py:6254:                    if any(k in name for k in bank):
  video_processor.py:6349:            seen.add(t)
  video_processor.py:6350:    return result[:5]
  video_processor.py:6351:
> video_processor.py:6352:def _prioritize_fresh_assets(broll_candidates, clip_id):
  video_processor.py:6353:    """Priorise les assets les plus rÃ©cents basÃ©s sur le timestamp du 
dossier."""
> video_processor.py:6354:    if not broll_candidates:
> video_processor.py:6355:        return broll_candidates
  video_processor.py:6356:    
  video_processor.py:6357:    try:
  video_processor.py:6358:        # Extraire le timestamp du dossier pour chaque candidat
> video_processor.py:6359:        for candidate in broll_candidates:
  video_processor.py:6360:            if hasattr(candidate, 'file_path') and candidate.file_path:
  video_processor.py:6361:                path = Path(candidate.file_path)
  video_processor.py:6362:                # Chercher le pattern clip_*_timestamp dans le chemin
  video_processor.py:6372:                candidate.folder_timestamp = 0
  video_processor.py:6373:        
  video_processor.py:6374:        # Trier par timestamp dÃ©croissant (plus rÃ©cent en premier)
> video_processor.py:6375:        broll_candidates.sort(key=lambda x: getattr(x, 'folder_timestamp', 
0), reverse=True)
  video_processor.py:6376:        
  video_processor.py:6377:    except Exception as e:
  video_processor.py:6378:        print(f"    âš ï¸  Erreur priorisation fraÃ®cheur: {e}")
  video_processor.py:6379:    
> video_processor.py:6380:    return broll_candidates
  video_processor.py:6381:
> video_processor.py:6382:def _score_contextual_relevance(asset_path, domain, keywords):
  video_processor.py:6383:    """Score de pertinence contextuelle basÃ© sur les tokens et le 
domaine."""
  video_processor.py:6384:    try:
> video_processor.py:6385:        if not asset_path or not domain or not keywords:
  video_processor.py:6386:            return 0.5
  video_processor.py:6387:        
  video_processor.py:6388:        # Extraire les tokens du nom de fichier
  video_processor.py:6391:        
  video_processor.py:6392:        # Tokens du domaine et mots-clÃ©s
  video_processor.py:6393:        domain_tokens = set(domain.lower().split())
> video_processor.py:6394:        keyword_tokens = set()
> video_processor.py:6395:        for kw in keywords:
  video_processor.py:6396:            if isinstance(kw, str):
> video_processor.py:6397:                keyword_tokens.update(kw.lower().split())
  video_processor.py:6398:        
  video_processor.py:6399:        # Calculer l'overlap
> video_processor.py:6400:        relevant_tokens = domain_tokens | keyword_tokens
  video_processor.py:6401:        if not relevant_tokens:
  video_processor.py:6402:            return 0.5
  video_processor.py:6403:        
  video_processor.py:6418:        print(f"    âš ï¸  Erreur scoring contextuel: {e}")
  video_processor.py:6419:        return 0.5
  video_processor.py:6420:
> video_processor.py:6421:def _get_fallback_neutral_assets(broll_library, count=3):
> video_processor.py:6422:    """RÃ©cupÃ¨re des assets neutres/gÃ©nÃ©riques comme fallback."""
  video_processor.py:6423:    try:
> video_processor.py:6424:        fallback_keywords = ['neutral', 'generic', 'background', 'abstract', 
'minimal']
> video_processor.py:6425:        fallback_assets = []
  video_processor.py:6426:        
> video_processor.py:6427:        for keyword in fallback_keywords:
  video_processor.py:6428:            # Chercher dans la librairie des assets avec ces mots-clÃ©s
  video_processor.py:6429:            for ext in ['.mp4', '.mov', '.jpg', '.png']:
> video_processor.py:6430:                for asset_path in broll_library.rglob(f"*{keyword}*{ext}"):
> video_processor.py:6431:                    if asset_path.exists() and asset_path not in 
fallback_assets:
> video_processor.py:6432:                        fallback_assets.append(str(asset_path))
> video_processor.py:6433:                        if len(fallback_assets) >= count:
  video_processor.py:6434:                            break
> video_processor.py:6435:                if len(fallback_assets) >= count:
  video_processor.py:6436:                    break
> video_processor.py:6437:            if len(fallback_assets) >= count:
  video_processor.py:6438:                break
  video_processor.py:6439:        
  video_processor.py:6440:        # Si pas assez d'assets spÃ©cifiques, prendre des assets gÃ©nÃ©riques
> video_processor.py:6441:        if len(fallback_assets) < count:
  video_processor.py:6442:            for ext in ['.mp4', '.mov', '.jpg', '.png']:
> video_processor.py:6443:                for asset_path in broll_library.rglob(f"*{ext}"):
> video_processor.py:6444:                    if asset_path.exists() and asset_path not in 
fallback_assets:
> video_processor.py:6445:                        fallback_assets.append(str(asset_path))
> video_processor.py:6446:                        if len(fallback_assets) >= count:
  video_processor.py:6447:                            break
> video_processor.py:6448:                if len(fallback_assets) >= count:
  video_processor.py:6449:                    break
  video_processor.py:6450:        
> video_processor.py:6451:        return fallback_assets[:count]
  video_processor.py:6452:        
  video_processor.py:6453:    except Exception as e:
> video_processor.py:6454:        print(f"    âš ï¸  Erreur fallback neutre: {e}")
  video_processor.py:6455:        return []
  video_processor.py:6456:
> video_processor.py:6457:def _debug_broll_selection(plan, domain, keywords, debug_mode=False):
  video_processor.py:6458:    """Log dÃ©taillÃ© de la sÃ©lection B-roll si debug activÃ©."""
  video_processor.py:6459:    if not debug_mode:
  video_processor.py:6460:        return
  video_processor.py:6461:    
  video_processor.py:6462:    print(f"    ðŸ” DEBUG B-ROLL SELECTION:")
  video_processor.py:6463:    print(f"       Domaine: {domain}")
> video_processor.py:6464:    print(f"       Mots-clÃ©s: {keywords[:5]}")
  video_processor.py:6465:    print(f"       Plan: {len(plan)} items")
  video_processor.py:6466:    
  video_processor.py:6467:    for i, item in enumerate(plan[:3]):  # Afficher les 3 premiers
  video_processor.py:6482:        print(f"         Score: {score}, Context: {context_score}, 
FraÃ®cheur: {freshness}")
  video_processor.py:6483:
  video_processor.py:6484:# ðŸš€ NOUVEAU: Fonction de scoring mixte intelligent pour B-rolls
> video_processor.py:6485:def score_broll_asset_mixed(asset_path: str, asset_tags: List[str], 
query_keywords: List[str], 
  video_processor.py:6486:                           domain: Optional[str] = None, asset_metadata: 
Optional[Dict] = None) -> float:
  video_processor.py:6487:    """
  video_processor.py:6488:    Score un asset B-roll avec le systÃ¨me mixte intelligent.
  video_processor.py:6490:    Args:
  video_processor.py:6491:        asset_path: Chemin vers l'asset
  video_processor.py:6492:        asset_tags: Tags de l'asset
> video_processor.py:6493:        query_keywords: Mots-clÃ©s de la requÃªte
  video_processor.py:6494:        domain: Domaine dÃ©tectÃ© (optionnel)
  video_processor.py:6495:        asset_metadata: MÃ©tadonnÃ©es supplÃ©mentaires (optionnel)
  video_processor.py:6496:    
  video_processor.py:6498:        Score final entre 0.0 et 1.0
  video_processor.py:6499:    """
  video_processor.py:6500:    try:
> video_processor.py:6501:        if not BROLL_SELECTOR_AVAILABLE:
> video_processor.py:6502:            # Fallback vers scoring basique
> video_processor.py:6503:            return _score_broll_asset_basic(asset_path, asset_tags, 
query_keywords)
  video_processor.py:6504:        
  video_processor.py:6505:        # Utiliser le nouveau sÃ©lecteur si disponible
> video_processor.py:6506:        from broll_selector import Asset, ScoringFeatures
  video_processor.py:6507:        
  video_processor.py:6508:        # CrÃ©er un asset simulÃ© pour le scoring
  video_processor.py:6509:        asset = Asset(
  video_processor.py:6519:        )
  video_processor.py:6520:        
  video_processor.py:6521:        # Normaliser les mots-clÃ©s de la requÃªte
> video_processor.py:6522:        normalized_keywords = set()
> video_processor.py:6523:        for kw in query_keywords:
  video_processor.py:6524:            if kw and isinstance(kw, str):
  video_processor.py:6525:                clean = kw.lower().strip()
  video_processor.py:6526:                if len(clean) > 2:
> video_processor.py:6527:                    normalized_keywords.add(clean)
  video_processor.py:6528:        
  video_processor.py:6529:        # Calculer les features de scoring
  video_processor.py:6530:        features = ScoringFeatures()
  video_processor.py:6531:        
  video_processor.py:6532:        # 1. Token overlap (Jaccard)
> video_processor.py:6533:        if asset_tags and normalized_keywords:
> video_processor.py:6534:            intersection = len(set(asset_tags) & normalized_keywords)
> video_processor.py:6535:            union = len(set(asset_tags) | normalized_keywords)
  video_processor.py:6536:            features.token_overlap = intersection / union if union > 0 else 
0.0
  video_processor.py:6537:        
  video_processor.py:6538:        # 2. Domain match
  video_processor.py:6539:        if domain and asset_tags:
> video_processor.py:6540:            domain_keywords = _get_domain_keywords(domain)
> video_processor.py:6541:            domain_overlap = len(set(asset_tags) & set(domain_keywords))
> video_processor.py:6542:            features.domain_match = min(1.0, domain_overlap / 
max(len(domain_keywords), 1))
  video_processor.py:6543:        
  video_processor.py:6544:        # 3. Freshness (basÃ© sur la date de crÃ©ation du fichier)
  video_processor.py:6545:        try:
  video_processor.py:6579:        
  video_processor.py:6580:    except Exception as e:
  video_processor.py:6581:        print(f"âš ï¸ Erreur scoring mixte: {e}")
> video_processor.py:6582:        # Fallback vers scoring basique
> video_processor.py:6583:        return _score_broll_asset_basic(asset_path, asset_tags, 
query_keywords)
  video_processor.py:6584:
> video_processor.py:6585:def _score_broll_asset_basic(asset_path: str, asset_tags: List[str], 
query_keywords: List[str]) -> float:
> video_processor.py:6586:    """Scoring basique de fallback"""
  video_processor.py:6587:    try:
  video_processor.py:6588:        # Score simple basÃ© sur l'overlap de tags
> video_processor.py:6589:        if not asset_tags or not query_keywords:
  video_processor.py:6590:            return 0.5
  video_processor.py:6591:        
  video_processor.py:6592:        asset_tag_set = set(tag.lower() for tag in asset_tags)
> video_processor.py:6593:        query_set = set(kw.lower() for kw in query_keywords if kw)
  video_processor.py:6594:        
  video_processor.py:6595:        if not query_set:
  video_processor.py:6596:            return 0.5
  video_processor.py:6604:        print(f"âš ï¸ Erreur scoring basique: {e}")
  video_processor.py:6605:        return 0.5
  video_processor.py:6606:
> video_processor.py:6607:def _get_domain_keywords(domain: str) -> List[str]:
  video_processor.py:6608:    """Retourne les mots-clÃ©s spÃ©cifiques au domaine"""
> video_processor.py:6609:    domain_keywords = {
  video_processor.py:6610:        'health': ['medical', 'healthcare', 'wellness', 'fitness', 
'medicine', 'hospital', 'doctor'],
  video_processor.py:6611:        'technology': ['tech', 'digital', 'innovation', 'computer', 'ai', 
'software', 'data'],
  video_processor.py:6612:        'business': ['business', 'entrepreneur', 'success', 'growth', 
'strategy', 'office', 'professional'],
  video_processor.py:6614:        'finance': ['money', 'finance', 'investment', 'wealth', 'business', 
'success', 'growth']
  video_processor.py:6615:    }
  video_processor.py:6616:    
> video_processor.py:6617:    return domain_keywords.get(domain.lower(), [domain])
  video_processor.py:6618:
  video_processor.py:6619:def _calculate_quality_score(asset_path: str, metadata: Optional[Dict] = 
None) -> float:
  video_processor.py:6620:    """Calcule un score de qualitÃ© basÃ© sur les mÃ©tadonnÃ©es"""
  video_processor.py:6644:    except Exception:
  video_processor.py:6645:        return 0.5
  video_processor.py:6646:
> video_processor.py:6647:    def _load_broll_selector_config(self):
  video_processor.py:6648:        """Charge la configuration du sÃ©lecteur B-roll depuis le fichier 
YAML"""
  video_processor.py:6649:        try:
  video_processor.py:6650:            import yaml
> video_processor.py:6651:            if Config.BROLL_SELECTOR_CONFIG_PATH.exists():
> video_processor.py:6652:                with open(Config.BROLL_SELECTOR_CONFIG_PATH, 'r', 
encoding='utf-8') as f:
  video_processor.py:6653:                    return yaml.safe_load(f) or {}
  video_processor.py:6654:            else:
> video_processor.py:6655:                print(f"    âšï¸ Fichier de configuration introuvable: 
{Config.BROLL_SELECTOR_CONFIG_PATH}")
  video_processor.py:6656:                return {}
  video_processor.py:6657:        except Exception as e:
  video_processor.py:6658:            print(f"    âš ï¸ Erreur chargement configuration: {e}")
  video_processor.py:6670:            hash_data = f"{asset_path.name}_{stat.st_size}_{stat.st_mtime}"
  video_processor.py:6671:            return hashlib.md5(hash_data.encode()).hexdigest()
  video_processor.py:6672:        except Exception:
> video_processor.py:6673:            # Fallback sur le nom du fichier
  video_processor.py:6674:            return str(asset_path.name)
  video_processor.py:6675:
  video_processor.py:6676:import argparse
  video_processor.py:6683:    parser = argparse.ArgumentParser(description='Run the video pipeline on 
a single clip.')
  video_processor.py:6684:    parser.add_argument('--video', required=True, help='Chemin du clip 
source (mp4, mov, etc.)')
  video_processor.py:6685:    parser.add_argument('--verbose', action='store_true', help='Affiche des 
informations supplementaires pendant le run.')
> video_processor.py:6686:    parser.add_argument('--no-report', action='store_true', help='Disable 
selection report JSON sidecar')
  video_processor.py:6687:    args = parser.parse_args(list(argv) if argv is not None else None)
  video_processor.py:6688:
  video_processor.py:6689:    print(f"[CLI] cwd={os.getcwd()}")
  video_processor.py:6690:    print(f"[CLI] video={args.video}")
  video_processor.py:6691:    print(f"[CLI] 
ENABLE_PIPELINE_CORE_FETCHER={os.getenv('ENABLE_PIPELINE_CORE_FETCHER')}")
  video_processor.py:6692:
> video_processor.py:6693:    # Map CLI switch to env flag for downstream code
  video_processor.py:6694:    if getattr(args, 'no_report', False):
  video_processor.py:6695:        os.environ['ENABLE_SELECTION_REPORT'] = 'false'
  video_processor.py:6696:        print('[CLI] selection report disabled')


